<!DOCTYPE html>
<html>
<meta  lang="en" >
<head>
  <meta charset="UTF-8">
  <meta name="viewport"
    content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color" content="#fff" id="theme-color">
  <link rel="icon" href="/img/1.jpg">
  <title>Mr xu</title>
  
  
  <meta property="og:title" content="增量式与分布式">
  
  
  <meta property="og:url" content="https://github.com/xuMr6/xumr6.github.io.git/2018/08/09/%E5%A2%9E%E9%87%8F%E5%BC%8F%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/index.html">
  
  
  <meta property="og:img" content="/img/1.jpg">
  
  
  
  <meta property="og:type" content="article">
  <meta property="og:article:published_time" content="2018-08-09">
  <meta property="og:article:modified_time" content="2020-09-04">
  <meta property="og:article:author" content="Mr xu">
  
  
  <meta property="og:article:tag" content="python">
  
  
  
  
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
    var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
      }
    };
    setDarkmode();
  </script>
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
  </script>
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_f7g5jnuftcf.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  <link rel="preload" href="//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js" as="script">
  
  
  <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
  
  
  
  
<link rel="stylesheet" href="/css/main.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_f7g5jnuftcf.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">

  
  
  
<meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="XuMr" type="application/atom+xml">
</head>


<body>
  <div class="wrapper">
    
    <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
      <img class="navbar-logo-img" src="/img/1.jpg">
      
      <span class="navbar-logo-dsc">Mr xu</span>
    </span>
  </div>
  <div class="navbar-menu">
    
    <a href="/" class="navbar-menu-item">
    
    首页
    
    </a>
    
    <a href="/archives" class="navbar-menu-item">
    
    时间
    
    </a>
    
    <a href="/tags" class="navbar-menu-item">
    
    标签
    
    </a>
    
    <a href="/categories" class="navbar-menu-item">
    
    分类
    
    </a>
    
    <a class="navbar-menu-item darknavbar" id="dark"><i class="iconfont icon-weather"></i></a>
    <a class="navbar-menu-item searchnavbar" id="search"><i class="iconfont icon-search" style="font-size: 1.2rem; font-weight: 400;"></i></a>
  </div>
</nav>
    
    <div id="local-search" style="display: none;">
      <input class="navbar-menu-item" id="search-input" placeholder="请输入搜索内容...">
      <div id="search-content"></div>
    </div>
    
    <div class="section-wrap">
      <div class="container">
        <div class="columns">
          <main class="main-column">
<article class="card card-content">
  <header>
    <h1 class="post-title">
      增量式与分布式
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2018-08-08T23:56:24.000Z" style="display: flex; align-items: center;">
      <i class="iconfont icon-calendar" style="margin-right: 2px;"></i>
      <span>2018-08-09</span>
    </time>
    
    <span class="dot"></span>
    
    <a href="/categories/爬虫/" class="post-meta-link">爬虫</a>
    
    
    
    <span class="dot"></span>
    <span>2.4k words</span>
    
  </div>
  
  <div class="post-meta post-show-meta" style="margin-top: -10px;">
    <div style="display: flex; align-items: center;">
      <i class="iconfont icon-biaoqian" style="margin-right: 2px; font-size: 1.15rem;"></i>
      
      
        <a href="/tags/python/" class="post-meta-link">python</a>
      
    </div>
  </div>
  
  </header>
  <div id="section" class="post-content">
    <h3 id="基于crawlSpider的全站数据爬取"><a href="#基于crawlSpider的全站数据爬取" class="headerlink" title="基于crawlSpider的全站数据爬取"></a>基于crawlSpider的全站数据爬取</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 项目的创建</span><br><span class="line">scrapy startproject projectname</span><br><span class="line">scrapy genspider -t crawl spidername www.baidu.com</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># crawlspider全站数据爬取:</span><br><span class="line">- CrawlSpider是一个爬虫类, 是scrapy.spider的子类, 功能比spider更强大.</span><br><span class="line">- CrawlSpider的机制:</span><br><span class="line">    - 连接提取器: 可以根据指定的规则进行连接的提取</span><br><span class="line">    - 规则解析器: 更具指定的规则对响应数据进行解析</span><br></pre></td></tr></table></figure>
<h4 id="案例-基于CrawlSpider对笑话网进行全站深度数据爬取-抓取笑话标题与内容-并存储于MongoDB中"><a href="#案例-基于CrawlSpider对笑话网进行全站深度数据爬取-抓取笑话标题与内容-并存储于MongoDB中" class="headerlink" title="案例: 基于CrawlSpider对笑话网进行全站深度数据爬取, 抓取笑话标题与内容, 并存储于MongoDB中"></a>案例: 基于CrawlSpider对笑话网进行全站深度数据爬取, 抓取笑话标题与内容, 并存储于MongoDB中</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># item编码:</span><br><span class="line">import scrapy</span><br><span class="line">class JokeItem(scrapy.Item):</span><br><span class="line">    title &#x3D; scrapy.Field()</span><br><span class="line">    content &#x3D; scrapy.Field()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># spider编码:</span><br><span class="line">import scrapy</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line">from..items import JokeItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ZSpider(CrawlSpider):</span><br><span class="line">    name &#x3D; &#39;z&#39;</span><br><span class="line">    # allowed_domains &#x3D; [&#39;www.baidu.com&#39;]</span><br><span class="line">    start_urls &#x3D; [&#39;http:&#x2F;&#x2F;xiaohua.zol.com.cn&#x2F;lengxiaohua&#x2F;&#39;]</span><br><span class="line">    link &#x3D; LinkExtractor(allow&#x3D;r&#39;&#x2F;lengxiaohua&#x2F;\d+.html&#39;)</span><br><span class="line">    link_detail &#x3D; LinkExtractor(allow&#x3D;r&#39;.*?\d+\.html&#39;)</span><br><span class="line">    rules &#x3D; (</span><br><span class="line">        Rule(link, callback&#x3D;&#39;parse_item&#39;, follow&#x3D;True),</span><br><span class="line">        Rule(link_detail, callback&#x3D;&#39;parse_detail&#39;),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    def parse_item(self, response):</span><br><span class="line">        pass</span><br><span class="line"></span><br><span class="line">    def parse_detail(self, response):</span><br><span class="line">        title &#x3D; response.xpath(&#39;&#x2F;&#x2F;h1[@class&#x3D;&quot;article-title&quot;]&#x2F;text()&#39;).extract_first()</span><br><span class="line">        content &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;article-text&quot;]&#x2F;&#x2F;text()&#39;).extract()</span><br><span class="line">        content &#x3D; &#39;&#39;.join(content)</span><br><span class="line"></span><br><span class="line">        if title and content:</span><br><span class="line">            item &#x3D; JokeItem()</span><br><span class="line">            item[&quot;title&quot;] &#x3D; title</span><br><span class="line">            item[&quot;content&quot;] &#x3D; content</span><br><span class="line">            print(dict(item))</span><br><span class="line">            yield item</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># pipeline编码:</span><br><span class="line">class JokePipeline(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, mongo_uri, mongo_db):</span><br><span class="line">        self.mongo_uri &#x3D; mongo_uri</span><br><span class="line">        self.mongo_db &#x3D; mongo_db</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        return cls(</span><br><span class="line">            mongo_uri&#x3D;crawler.settings.get(&#39;MONGO_URI&#39;),</span><br><span class="line">            mongo_db&#x3D;crawler.settings.get(&#39;MONGO_DB&#39;)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        self.client &#x3D; pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db &#x3D; self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        self.db[&quot;joke&quot;].insert(dict(item))</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line">    def close(self, spider):</span><br><span class="line">        self.client.close()</span><br></pre></td></tr></table></figure>

<h4 id="电影天堂-全站深度抓取电影名与下载链接"><a href="#电影天堂-全站深度抓取电影名与下载链接" class="headerlink" title="电影天堂: 全站深度抓取电影名与下载链接:"></a>电影天堂: 全站深度抓取电影名与下载链接:</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># item定义存储字段:</span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class BossItem(scrapy.Item):</span><br><span class="line">    title &#x3D; scrapy.Field()</span><br><span class="line">    downloadlink &#x3D; scrapy.Field()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># spider编码:</span><br><span class="line">import scrapy</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line">from ..items import MvItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class BSpider(CrawlSpider):</span><br><span class="line">    name &#x3D; &#39;mv&#39;</span><br><span class="line">    # allowed_domains &#x3D; [&#39;www.baidu.com&#39;]</span><br><span class="line">    start_urls &#x3D; [&#39;https:&#x2F;&#x2F;www.ygdy8.net&#x2F;html&#x2F;gndy&#x2F;oumei&#x2F;index.html&#39;]</span><br><span class="line">    link &#x3D; LinkExtractor(allow&#x3D;r&#39;list.*?html&#39;)</span><br><span class="line">    link_detail &#x3D; LinkExtractor(allow&#x3D;r&#39;.*?&#x2F;\d+\.html&#39;)</span><br><span class="line">    rules &#x3D; (</span><br><span class="line">        Rule(link, callback&#x3D;&#39;parse_item&#39;, follow&#x3D;True,),</span><br><span class="line">        Rule(link_detail, callback&#x3D;&#39;parse_detail&#39;, follow&#x3D;True,),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    def parse_item(self, response):</span><br><span class="line">        pass</span><br><span class="line"></span><br><span class="line">    def parse_detail(self, response):</span><br><span class="line">        title &#x3D; response.xpath(&#39;&#x2F;&#x2F;h1&#x2F;&#x2F;text()&#39;).extract_first()</span><br><span class="line">        downloadlink &#x3D; response.xpath(&#39;&#x2F;&#x2F;tbody&#x2F;tr&#x2F;td&#x2F;a&#x2F;text()&#39;).extract_first()</span><br><span class="line">        if title and downloadlink and &#39;ftp&#39; in downloadlink:</span><br><span class="line">            item &#x3D; BossItem()</span><br><span class="line">            item[&#39;title&#39;] &#x3D; title</span><br><span class="line">            item[&#39;downloadlink&#39;] &#x3D; downloadlink</span><br><span class="line">            yield item</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># piplines编码:</span><br><span class="line">class MvPipeline(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, mongo_uri, mongo_db):</span><br><span class="line">        self.mongo_uri &#x3D; mongo_uri</span><br><span class="line">        self.mongo_db &#x3D; mongo_db</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        return cls(</span><br><span class="line">            mongo_uri&#x3D;crawler.settings.get(&#39;MONGO_URI&#39;),</span><br><span class="line">            mongo_db&#x3D;crawler.settings.get(&#39;MONGO_DB&#39;)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        self.client &#x3D; pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db &#x3D; self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        self.db[&quot;mv&quot;].insert(dict(item))</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line">    def close(self, spider):</span><br><span class="line">        self.client.close()</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="1-分布式"><a href="#1-分布式" class="headerlink" title="1.分布式"></a>1.分布式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 分布式概念:</span><br><span class="line">使用多台机器组成一个分布式的机群，在机群中运行同一组程序，进行联合数据的爬取。</span><br><span class="line"></span><br><span class="line"># 原生scrapy无法实现分布式原因:</span><br><span class="line">	- 原生的scrapy中的调度器不可以被共享</span><br><span class="line">	- 原生的scrapy的管道不可以被共享</span><br><span class="line"></span><br><span class="line"># 使用scrapy实现分布式思路:</span><br><span class="line">- 为原生的scrapy框架提供共享的管道和调度器</span><br><span class="line">- pip install scrapy_redis</span><br></pre></td></tr></table></figure>

<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">- 1. 创建工程: scrapy startproject projectname</span><br><span class="line">- 2. 爬虫文件: scrapy genspider -t crawl spidername www.baidu.com</span><br><span class="line">- 3. 修改爬虫文件：</span><br><span class="line">	- 3.1 导包：from scrapy_redis.spiders import RedisCrawlSpider</span><br><span class="line">	- 3.2 将当前爬虫类的父类进行修改RedisCrawlSpider</span><br><span class="line">	- 3.3 allowed_domains，start_url注释掉，添加一个新属性redis_key&#x3D;&#39;qn&#39;(调度器队列的名称)</span><br><span class="line">	- 3.4 指定redis_key &#x3D; &#39;xxx&#39; , 即共享调度器队列名字</span><br><span class="line">	- 3.4 数据解析，将解析的数据封装到item中然后向管道提交</span><br><span class="line">- 4. 配置文件的编写：</span><br><span class="line">	- 4.1 指定管道：</span><br><span class="line">		ITEM_PIPELINES &#x3D; &#123;</span><br><span class="line">			&#39;scrapy_redis.pipelines.RedisPipeline&#39;: 400</span><br><span class="line">		&#125;</span><br><span class="line">	- 4.2 指定调度器：</span><br><span class="line">		# 增加了一个去重容器类的配置, 作用使用Redis的set集合来存储请求的指纹数据, 从而实现请求去重的持久化</span><br><span class="line">		DUPEFILTER_CLASS &#x3D; &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br><span class="line">		# 使用scrapy-redis组件自己的调度器</span><br><span class="line">		SCHEDULER &#x3D; &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line">		# 配置调度器是否要持久化, 也就是当爬虫结束了, 要不要清空Redis中请求队列和去重指纹的set。如果是True, 就表示要持久化存储, 就不清空数据, 否则清空数据</span><br><span class="line">		SCHEDULER_PERSIST &#x3D; True</span><br><span class="line">	- 4.3 指定具体的redis：</span><br><span class="line">		REDIS_HOST &#x3D; &#39;redis服务的ip地址&#39;</span><br><span class="line">		REDIS_PORT &#x3D; 6379</span><br><span class="line">- 5. 修改Redis配置并指定配置启动：</span><br><span class="line">	- #bind 127.0.0.1</span><br><span class="line">	- protected-mode no</span><br><span class="line">	- 开启redis服务(携带redis的配置文件：redis-server .&#x2F;redis.windows.conf),和客户端(redis-cli)：</span><br><span class="line"></span><br><span class="line">- 6. 启动程序：scrapy runspider xxx.py(需要进入spider文件夹)</span><br><span class="line">- 7. 向调度器队列中扔入一个起始的url（redis的客户端）：lpush xxx www.xxx.com</span><br><span class="line">	(xxx表示的就是redis_key的值)</span><br></pre></td></tr></table></figure>

<h4 id="案例-阳光热线问政平台投诉信息爬取–-gt-网址-http-wz-sun0769-com-index-php-question-questionType-type-4"><a href="#案例-阳光热线问政平台投诉信息爬取–-gt-网址-http-wz-sun0769-com-index-php-question-questionType-type-4" class="headerlink" title="案例: 阳光热线问政平台投诉信息爬取–&gt;网址: http://wz.sun0769.com/index.php/question/questionType?type=4"></a>案例: 阳光热线问政平台投诉信息爬取–&gt;网址: <a target="_blank" rel="noopener" href="http://wz.sun0769.com/index.php/question/questionType?type=4">http://wz.sun0769.com/index.php/question/questionType?type=4</a></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># items编码:</span><br><span class="line">import scrapy</span><br><span class="line">class FbsproItem(scrapy.Item):</span><br><span class="line">    # define the fields for your item here like:</span><br><span class="line">    title &#x3D; scrapy.Field()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># spider编码:</span><br><span class="line">import scrapy</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line">from scrapy_redis.spiders import RedisCrawlSpider</span><br><span class="line">from fbspro.items import FbsproItem  </span><br><span class="line">class TestSpider(RedisCrawlSpider):</span><br><span class="line">    name &#x3D; &#39;test&#39;  </span><br><span class="line">    # allowed_domains &#x3D; [&#39;ww.baidu.com&#39;]</span><br><span class="line">    # start_urls &#x3D; [&#39;http:&#x2F;&#x2F;ww.baidu.com&#x2F;&#39;]</span><br><span class="line">    redis_key &#x3D; &#39;urlscheduler&#39;</span><br><span class="line">    link &#x3D; LinkExtractor(allow&#x3D;r&#39;.*?&amp;page&#x3D;\d+&#39;)</span><br><span class="line">    rules &#x3D; (</span><br><span class="line">        Rule(link, callback&#x3D;&#39;parse_item&#39;, follow&#x3D;True),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    def parse_item(self, response):</span><br><span class="line">        a_lst &#x3D; response.xpath(&#39;&#x2F;&#x2F;a[@class&#x3D;&quot;news14&quot;]&#39;)</span><br><span class="line">        for a in a_lst:</span><br><span class="line">            title &#x3D; a.xpath(&#39;.&#x2F;text()&#39;).extract_first()</span><br><span class="line">            # print(title)</span><br><span class="line">            item &#x3D; FbsproItem()</span><br><span class="line">            item[&#39;title&#39;] &#x3D; title</span><br><span class="line">            yield item</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># settings配置编码:</span><br><span class="line">USER_AGENT &#x3D; &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;76.0.3809.100 Safari&#x2F;537.36&#39;</span><br><span class="line">ROBOTSTXT_OBEY &#x3D; False</span><br><span class="line">CONCURRENT_REQUESTS &#x3D; 3</span><br><span class="line">ITEM_PIPELINES &#x3D; &#123;</span><br><span class="line">   # &#39;fbspro.pipelines.FbsproPipeline&#39;: 300,</span><br><span class="line">    &#39;scrapy_redis.pipelines.RedisPipeline&#39;: 400</span><br><span class="line">&#125;</span><br><span class="line"># 增加了一个去重容器类的配置, 作用使用Redis的set集合来存储请求的指纹数据, 从而实现请求去重的持久化</span><br><span class="line">DUPEFILTER_CLASS &#x3D; &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br><span class="line"># 使用scrapy-redis组件自己的调度器</span><br><span class="line">SCHEDULER &#x3D; &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line"># 配置调度器是否要持久化, 也就是当爬虫结束了, 要不要清空Redis中请求队列和去重指纹的set。如果是True, 就表示要持久化存储, 就不清空数据, 否则清空数据</span><br><span class="line">SCHEDULER_PERSIST &#x3D; True</span><br><span class="line"></span><br><span class="line"># redis配置</span><br><span class="line">REDIS_HOST &#x3D; &#39;192.168.12.198&#39;</span><br><span class="line">REDIS_PORT &#x3D; 6379</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="2-增量式"><a href="#2-增量式" class="headerlink" title="2.增量式"></a>2.增量式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 概念:</span><br><span class="line">	- 检测网站数据更新, 只爬取更新的内容</span><br><span class="line">	- 核心: 去重</span><br><span class="line">        - url</span><br><span class="line">        - 数据指纹</span><br></pre></td></tr></table></figure>

<h4 id="增量式爬虫-电影名称与电影类型的爬取–-gt-url-https-www-4567tv-co-list-index1-html"><a href="#增量式爬虫-电影名称与电影类型的爬取–-gt-url-https-www-4567tv-co-list-index1-html" class="headerlink" title="增量式爬虫: 电影名称与电影类型的爬取–&gt;url: https://www.4567tv.co/list/index1.html"></a>增量式爬虫: 电影名称与电影类型的爬取–&gt;url: <a target="_blank" rel="noopener" href="https://www.4567tv.co/list/index1.html">https://www.4567tv.co/list/index1.html</a></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># items编码:</span><br><span class="line">import scrapy</span><br><span class="line">class MvproItem(scrapy.Item):</span><br><span class="line">    title &#x3D; scrapy.Field()</span><br><span class="line">    position &#x3D; scrapy.Field()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># spider编码:</span><br><span class="line">import scrapy</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line">from redis import Redis</span><br><span class="line">from mvpro.items import MvproItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MoveSpider(CrawlSpider):</span><br><span class="line">    conn &#x3D; Redis(&#39;127.0.0.1&#39;, 6379)</span><br><span class="line">    name &#x3D; &#39;move&#39;</span><br><span class="line">    # allowed_domains &#x3D; [&#39;www.baidu.com&#39;]</span><br><span class="line">    start_urls &#x3D; [&#39;https:&#x2F;&#x2F;www.4567tv.co&#x2F;list&#x2F;index1.html&#39;]</span><br><span class="line">    link &#x3D; LinkExtractor(allow&#x3D;r&#39;&#x2F;list&#x2F;index1-\d+\.html&#39;)</span><br><span class="line">    rules &#x3D; (</span><br><span class="line">        Rule(link, callback&#x3D;&#39;parse_item&#39;, follow&#x3D;True),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    def parse_item(self, response):</span><br><span class="line">        li_list &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[contains(@class, &quot;index-area&quot;)]&#x2F;ul&#x2F;li&#39;)</span><br><span class="line">        for li in li_list:</span><br><span class="line">            mv_link &#x3D; &#39;https:&#x2F;&#x2F;www.4567tv.co&#39; + li.xpath(&#39;.&#x2F;a&#x2F;@href&#39;).extract_first()</span><br><span class="line">            ex &#x3D; self.conn.sadd(&#39;mv_link&#39;, mv_link)</span><br><span class="line">            if ex:</span><br><span class="line">                print(&#39;有新数据可以爬取..........................&#39;)</span><br><span class="line">                yield scrapy.Request(url&#x3D;mv_link, callback&#x3D;self.parse_detail)</span><br><span class="line">            else:</span><br><span class="line">                print(&#39;没有新数据可以爬取!!!!!!!!!!!!!!!!!!!!!!!!!&#39;)</span><br><span class="line"></span><br><span class="line">    def parse_detail(self, response):</span><br><span class="line">        title &#x3D; response.xpath(&#39;&#x2F;&#x2F;dt[@class&#x3D;&quot;name&quot;]&#x2F;text()&#39;).extract_first()</span><br><span class="line">        pro &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;ee&quot;]&#x2F;text()&#39;).extract_first()</span><br><span class="line">        item &#x3D; MvproItem()</span><br><span class="line">        item[&#39;title&#39;] &#x3D; title</span><br><span class="line">        item[&#39;position&#39;] &#x3D; pro</span><br><span class="line">        yield item</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="需求-基于数据指纹的增量式爬虫-爬取糗百文字"><a href="#需求-基于数据指纹的增量式爬虫-爬取糗百文字" class="headerlink" title="需求: 基于数据指纹的增量式爬虫, 爬取糗百文字"></a>需求: 基于数据指纹的增量式爬虫, 爬取糗百文字</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># spider编码:</span><br><span class="line">import scrapy</span><br><span class="line">from qiubai.items import QiubaiItem</span><br><span class="line">import hashlib</span><br><span class="line">from redis import Redis</span><br><span class="line"></span><br><span class="line">class QbSpider(scrapy.Spider):</span><br><span class="line">    conn &#x3D; Redis(&#39;127.0.0.1&#39;, 6379)</span><br><span class="line">    name &#x3D; &#39;qb&#39;</span><br><span class="line">    # allowed_domains &#x3D; [&#39;www.baidu.com&#39;]</span><br><span class="line">    start_urls &#x3D; [&#39;https:&#x2F;&#x2F;www.qiushibaike.com&#x2F;text&#x2F;&#39;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        div_list &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@id&#x3D;&quot;content-left&quot;]&#x2F;div&#39;)</span><br><span class="line"></span><br><span class="line">        for div in div_list:</span><br><span class="line">            content &#x3D; div.xpath(&#39;.&#x2F;a[1]&#x2F;div[@class&#x3D;&quot;content&quot;]&#x2F;span[1]&#x2F;text()&#39;).extract_first()</span><br><span class="line">            fp &#x3D; hashlib.md5(content.encode(&#39;utf-8&#39;)).hexdigest()</span><br><span class="line">            ex &#x3D; self.conn.sadd(&#39;fp&#39;, fp)</span><br><span class="line">            if ex:</span><br><span class="line">                print(&#39;有更新数据需要爬取........................&#39;)</span><br><span class="line">                item &#x3D; QiubaiItem()</span><br><span class="line">                item[&#39;content&#39;] &#x3D; content</span><br><span class="line">                yield item</span><br><span class="line">            else:</span><br><span class="line">                print(&#39;没有数据更新~~~&#39;)</span><br></pre></td></tr></table></figure>
<h2 id="3-scrapy提高数据爬取效率"><a href="#3-scrapy提高数据爬取效率" class="headerlink" title="3.scrapy提高数据爬取效率"></a>3.scrapy提高数据爬取效率</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1.增加并发：</span><br><span class="line">默认scrapy开启的并发线程为32个，可以适当进行增加。在settings配置文件中修改CONCURRENT_REQUESTS &#x3D; 100值为100,并发设置成了为100。</span><br><span class="line"></span><br><span class="line">2.降低日志级别：</span><br><span class="line">    在运行scrapy时，会有大量日志信息的输出，为了减少CPU的使用率。可以设置log输出信息为INFO或者ERROR即可。在配置文件中编写：LOG_LEVEL &#x3D; ‘INFO’</span><br><span class="line"></span><br><span class="line">3.禁止cookie：</span><br><span class="line">    如果不是真的需要cookie，则在scrapy爬取数据时可以禁止cookie从而减少CPU的使用率，提升爬取效率。在配置文件中编写：COOKIES_ENABLED &#x3D; False</span><br><span class="line"></span><br><span class="line">4.禁止重试：</span><br><span class="line">    对失败的HTTP进行重新请求（重试）会减慢爬取速度，因此可以禁止重试。在配置文件中编写：RETRY_ENABLED &#x3D; False</span><br><span class="line"></span><br><span class="line">5.减少下载超时：</span><br><span class="line">    如果对一个非常慢的链接进行爬取，减少下载超时可以能让卡住的链接快速被放弃，从而提升效率。在配置文件中进行编写：DOWNLOAD_TIMEOUT &#x3D; 10 超时时间为10s</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="4-虚拟环境"><a href="#4-虚拟环境" class="headerlink" title="4.虚拟环境"></a>4.虚拟环境</h2><h4 id="安装"><a href="#安装" class="headerlink" title="安装:"></a>安装:</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install virtualenvwrapper-win</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># 常用命令:mkvirtualenv envname  # 创建虚拟环境并自动切换到该环境下workon envname  # 切换到某虚拟环境下pip list rmvirtualenv envname  # 删除虚拟环境deactivate  # 退出虚拟环境lsvirtualenv  # 列出所有常见的虚拟环境mkvirtualenv --python&#x3D;&#x3D;C:\...\python.exe envname  # 指定Python解释器创建虚拟环境</span><br></pre></td></tr></table></figure>
  </div>
  <div>
  
  <div class="post-note note-warning copyright" style="margin-top: 42px">
    <p><span style="font-weight: bold;">作者：</span><a target="_blank" rel="nofollow noopener noreferrer" href="https://github.com/xuMr6/xumr6.github.io.git/about">Mr xu</a></p>
    <p><span style="font-weight: bold;">文章链接：</span><a target="_blank" rel="nofollow noopener noreferrer" href="https://github.com/xuMr6/xumr6.github.io.git/2018/08/09/%E5%A2%9E%E9%87%8F%E5%BC%8F%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/">https://github.com/xuMr6/xumr6.github.io.git/2018/08/09/%E5%A2%9E%E9%87%8F%E5%BC%8F%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/</a></p>
    <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，转载请注明出处！</p>
  </div>
  
  </div>
</article>
<div class="nav">
  
  <div class="nav-item-prev">
    <a href="/2018/08/15/反序列化/" class="nav-link">
      <i class="iconfont icon-left nav-prev-icon"></i>
      <div>
        <div class="nav-label">Prev</div>
        
        <div class="nav-title">反序列化 </div>
        
      </div>
    </a>
  </div>
  
  
  <div class="nav-item-next">
    <a href="/2018/08/01/多线程爬虫/" class="nav-link">
      <div>
        <div class="nav-label">Next</div>
        
        <div class="nav-title">多线程爬虫 </div>
        
      </div>
      <i class="iconfont icon-right nav-next-icon"></i>
    </a>
  </div>
  
</div>

<div class="card card-content toc-card" id="mobiletoc">
  <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8EcrawlSpider%E7%9A%84%E5%85%A8%E7%AB%99%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96"><span class="toc-text">基于crawlSpider的全站数据爬取</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B-%E5%9F%BA%E4%BA%8ECrawlSpider%E5%AF%B9%E7%AC%91%E8%AF%9D%E7%BD%91%E8%BF%9B%E8%A1%8C%E5%85%A8%E7%AB%99%E6%B7%B1%E5%BA%A6%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96-%E6%8A%93%E5%8F%96%E7%AC%91%E8%AF%9D%E6%A0%87%E9%A2%98%E4%B8%8E%E5%86%85%E5%AE%B9-%E5%B9%B6%E5%AD%98%E5%82%A8%E4%BA%8EMongoDB%E4%B8%AD"><span class="toc-text">案例: 基于CrawlSpider对笑话网进行全站深度数据爬取, 抓取笑话标题与内容, 并存储于MongoDB中</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%B5%E5%BD%B1%E5%A4%A9%E5%A0%82-%E5%85%A8%E7%AB%99%E6%B7%B1%E5%BA%A6%E6%8A%93%E5%8F%96%E7%94%B5%E5%BD%B1%E5%90%8D%E4%B8%8E%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5"><span class="toc-text">电影天堂: 全站深度抓取电影名与下载链接:</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%88%86%E5%B8%83%E5%BC%8F"><span class="toc-text">1.分布式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B-%E9%98%B3%E5%85%89%E7%83%AD%E7%BA%BF%E9%97%AE%E6%94%BF%E5%B9%B3%E5%8F%B0%E6%8A%95%E8%AF%89%E4%BF%A1%E6%81%AF%E7%88%AC%E5%8F%96%E2%80%93-gt-%E7%BD%91%E5%9D%80-http-wz-sun0769-com-index-php-question-questionType-type-4"><span class="toc-text">案例: 阳光热线问政平台投诉信息爬取–&gt;网址: http:&#x2F;&#x2F;wz.sun0769.com&#x2F;index.php&#x2F;question&#x2F;questionType?type&#x3D;4</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A2%9E%E9%87%8F%E5%BC%8F"><span class="toc-text">2.增量式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A2%9E%E9%87%8F%E5%BC%8F%E7%88%AC%E8%99%AB-%E7%94%B5%E5%BD%B1%E5%90%8D%E7%A7%B0%E4%B8%8E%E7%94%B5%E5%BD%B1%E7%B1%BB%E5%9E%8B%E7%9A%84%E7%88%AC%E5%8F%96%E2%80%93-gt-url-https-www-4567tv-co-list-index1-html"><span class="toc-text">增量式爬虫: 电影名称与电影类型的爬取–&gt;url: https:&#x2F;&#x2F;www.4567tv.co&#x2F;list&#x2F;index1.html</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9C%80%E6%B1%82-%E5%9F%BA%E4%BA%8E%E6%95%B0%E6%8D%AE%E6%8C%87%E7%BA%B9%E7%9A%84%E5%A2%9E%E9%87%8F%E5%BC%8F%E7%88%AC%E8%99%AB-%E7%88%AC%E5%8F%96%E7%B3%97%E7%99%BE%E6%96%87%E5%AD%97"><span class="toc-text">需求: 基于数据指纹的增量式爬虫, 爬取糗百文字</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-scrapy%E6%8F%90%E9%AB%98%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96%E6%95%88%E7%8E%87"><span class="toc-text">3.scrapy提高数据爬取效率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83"><span class="toc-text">4.虚拟环境</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85"><span class="toc-text">安装:</span></a></li></ol></li></ol>
</div></main>
          <aside class="left-column">
            
            <div class="card card-author">
              
<img src="/img/1.jpg" class="author-img">

<p class="author-name">Mr xu</p>
<p class="author-description"></p>
<div class="author-message">
  <a class="author-posts-count" href="/archives">
    <span>113</span>
    <span>Posts</span>
  </a>
  <a class="author-categories-count" href="/categories">
    <span>18</span>
    <span>Categories</span>
  </a>
  <a class="author-tags-count" href="/tags">
    <span>2</span>
    <span>Tags</span>
  </a>
</div>

            </div>
            
            <div class="sticky-tablet">
  
  
  <article class="display-when-two-columns spacer">
    <div class="card card-content toc-card">
      <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8EcrawlSpider%E7%9A%84%E5%85%A8%E7%AB%99%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96"><span class="toc-text">基于crawlSpider的全站数据爬取</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B-%E5%9F%BA%E4%BA%8ECrawlSpider%E5%AF%B9%E7%AC%91%E8%AF%9D%E7%BD%91%E8%BF%9B%E8%A1%8C%E5%85%A8%E7%AB%99%E6%B7%B1%E5%BA%A6%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96-%E6%8A%93%E5%8F%96%E7%AC%91%E8%AF%9D%E6%A0%87%E9%A2%98%E4%B8%8E%E5%86%85%E5%AE%B9-%E5%B9%B6%E5%AD%98%E5%82%A8%E4%BA%8EMongoDB%E4%B8%AD"><span class="toc-text">案例: 基于CrawlSpider对笑话网进行全站深度数据爬取, 抓取笑话标题与内容, 并存储于MongoDB中</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%B5%E5%BD%B1%E5%A4%A9%E5%A0%82-%E5%85%A8%E7%AB%99%E6%B7%B1%E5%BA%A6%E6%8A%93%E5%8F%96%E7%94%B5%E5%BD%B1%E5%90%8D%E4%B8%8E%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5"><span class="toc-text">电影天堂: 全站深度抓取电影名与下载链接:</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%88%86%E5%B8%83%E5%BC%8F"><span class="toc-text">1.分布式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B-%E9%98%B3%E5%85%89%E7%83%AD%E7%BA%BF%E9%97%AE%E6%94%BF%E5%B9%B3%E5%8F%B0%E6%8A%95%E8%AF%89%E4%BF%A1%E6%81%AF%E7%88%AC%E5%8F%96%E2%80%93-gt-%E7%BD%91%E5%9D%80-http-wz-sun0769-com-index-php-question-questionType-type-4"><span class="toc-text">案例: 阳光热线问政平台投诉信息爬取–&gt;网址: http:&#x2F;&#x2F;wz.sun0769.com&#x2F;index.php&#x2F;question&#x2F;questionType?type&#x3D;4</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A2%9E%E9%87%8F%E5%BC%8F"><span class="toc-text">2.增量式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A2%9E%E9%87%8F%E5%BC%8F%E7%88%AC%E8%99%AB-%E7%94%B5%E5%BD%B1%E5%90%8D%E7%A7%B0%E4%B8%8E%E7%94%B5%E5%BD%B1%E7%B1%BB%E5%9E%8B%E7%9A%84%E7%88%AC%E5%8F%96%E2%80%93-gt-url-https-www-4567tv-co-list-index1-html"><span class="toc-text">增量式爬虫: 电影名称与电影类型的爬取–&gt;url: https:&#x2F;&#x2F;www.4567tv.co&#x2F;list&#x2F;index1.html</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9C%80%E6%B1%82-%E5%9F%BA%E4%BA%8E%E6%95%B0%E6%8D%AE%E6%8C%87%E7%BA%B9%E7%9A%84%E5%A2%9E%E9%87%8F%E5%BC%8F%E7%88%AC%E8%99%AB-%E7%88%AC%E5%8F%96%E7%B3%97%E7%99%BE%E6%96%87%E5%AD%97"><span class="toc-text">需求: 基于数据指纹的增量式爬虫, 爬取糗百文字</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-scrapy%E6%8F%90%E9%AB%98%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96%E6%95%88%E7%8E%87"><span class="toc-text">3.scrapy提高数据爬取效率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83"><span class="toc-text">4.虚拟环境</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85"><span class="toc-text">安装:</span></a></li></ol></li></ol>
    </div>
  </article>
  
  
  <article class="card card-content categories-widget">
    <div class="categories-card">
  <div class="categories-header"><i class="iconfont icon-fenlei" style="padding-right: 2px;"></i>Categories</div>
  <div class="categories-list">
    
      <a href="/categories/celery">
        <div class="categories-list-item">
          celery
          <span class="categories-list-item-badge">3</span>
        </div>
      </a>
    
      <a href="/categories/docker">
        <div class="categories-list-item">
          docker
          <span class="categories-list-item-badge">4</span>
        </div>
      </a>
    
      <a href="/categories/djangobook">
        <div class="categories-list-item">
          djangobook
          <span class="categories-list-item-badge">22</span>
        </div>
      </a>
    
      <a href="/categories/django">
        <div class="categories-list-item">
          django
          <span class="categories-list-item-badge">22</span>
        </div>
      </a>
    
      <a href="/categories/ES检索">
        <div class="categories-list-item">
          ES检索
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
      <a href="/categories/jwt">
        <div class="categories-list-item">
          jwt
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
      <a href="/categories/linux">
        <div class="categories-list-item">
          linux
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/categories/python">
        <div class="categories-list-item">
          python
          <span class="categories-list-item-badge">13</span>
        </div>
      </a>
    
      <a href="/categories/mysql">
        <div class="categories-list-item">
          mysql
          <span class="categories-list-item-badge">6</span>
        </div>
      </a>
    
      <a href="/categories/nginx">
        <div class="categories-list-item">
          nginx
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
      <a href="/categories/支付宝扫码">
        <div class="categories-list-item">
          支付宝扫码
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/categories/redis">
        <div class="categories-list-item">
          redis
          <span class="categories-list-item-badge">3</span>
        </div>
      </a>
    
      <a href="/categories/爬虫">
        <div class="categories-list-item">
          爬虫
          <span class="categories-list-item-badge">9</span>
        </div>
      </a>
    
      <a href="/categories/vue">
        <div class="categories-list-item">
          vue
          <span class="categories-list-item-badge">8</span>
        </div>
      </a>
    
      <a href="/categories/git">
        <div class="categories-list-item">
          git
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/categories/mongodb">
        <div class="categories-list-item">
          mongodb
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/categories/编程">
        <div class="categories-list-item">
          编程
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/categories/算法">
        <div class="categories-list-item">
          算法
          <span class="categories-list-item-badge">3</span>
        </div>
      </a>
    
  </div>
</div>
  </article>
  
  <article class="card card-content tags-widget">
    <div class="tags-card">
  <div class="tags-header"><i class="iconfont icon-biaoqian" style="padding-right: 2px;"></i>hot tags</div>
  <div class="tags-list">
    
    <a href="\tags\python" title="python"><div class="tags-list-item">python</div></a>
    
    <a href="\tags\Linux" title="Linux"><div class="tags-list-item">Linux</div></a>
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
          <aside class="right-column">
            <div class="sticky-widescreen">
  
  
  <article class="card card-content toc-card">
    <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8EcrawlSpider%E7%9A%84%E5%85%A8%E7%AB%99%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96"><span class="toc-text">基于crawlSpider的全站数据爬取</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B-%E5%9F%BA%E4%BA%8ECrawlSpider%E5%AF%B9%E7%AC%91%E8%AF%9D%E7%BD%91%E8%BF%9B%E8%A1%8C%E5%85%A8%E7%AB%99%E6%B7%B1%E5%BA%A6%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96-%E6%8A%93%E5%8F%96%E7%AC%91%E8%AF%9D%E6%A0%87%E9%A2%98%E4%B8%8E%E5%86%85%E5%AE%B9-%E5%B9%B6%E5%AD%98%E5%82%A8%E4%BA%8EMongoDB%E4%B8%AD"><span class="toc-text">案例: 基于CrawlSpider对笑话网进行全站深度数据爬取, 抓取笑话标题与内容, 并存储于MongoDB中</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%B5%E5%BD%B1%E5%A4%A9%E5%A0%82-%E5%85%A8%E7%AB%99%E6%B7%B1%E5%BA%A6%E6%8A%93%E5%8F%96%E7%94%B5%E5%BD%B1%E5%90%8D%E4%B8%8E%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5"><span class="toc-text">电影天堂: 全站深度抓取电影名与下载链接:</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%88%86%E5%B8%83%E5%BC%8F"><span class="toc-text">1.分布式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B-%E9%98%B3%E5%85%89%E7%83%AD%E7%BA%BF%E9%97%AE%E6%94%BF%E5%B9%B3%E5%8F%B0%E6%8A%95%E8%AF%89%E4%BF%A1%E6%81%AF%E7%88%AC%E5%8F%96%E2%80%93-gt-%E7%BD%91%E5%9D%80-http-wz-sun0769-com-index-php-question-questionType-type-4"><span class="toc-text">案例: 阳光热线问政平台投诉信息爬取–&gt;网址: http:&#x2F;&#x2F;wz.sun0769.com&#x2F;index.php&#x2F;question&#x2F;questionType?type&#x3D;4</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A2%9E%E9%87%8F%E5%BC%8F"><span class="toc-text">2.增量式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A2%9E%E9%87%8F%E5%BC%8F%E7%88%AC%E8%99%AB-%E7%94%B5%E5%BD%B1%E5%90%8D%E7%A7%B0%E4%B8%8E%E7%94%B5%E5%BD%B1%E7%B1%BB%E5%9E%8B%E7%9A%84%E7%88%AC%E5%8F%96%E2%80%93-gt-url-https-www-4567tv-co-list-index1-html"><span class="toc-text">增量式爬虫: 电影名称与电影类型的爬取–&gt;url: https:&#x2F;&#x2F;www.4567tv.co&#x2F;list&#x2F;index1.html</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9C%80%E6%B1%82-%E5%9F%BA%E4%BA%8E%E6%95%B0%E6%8D%AE%E6%8C%87%E7%BA%B9%E7%9A%84%E5%A2%9E%E9%87%8F%E5%BC%8F%E7%88%AC%E8%99%AB-%E7%88%AC%E5%8F%96%E7%B3%97%E7%99%BE%E6%96%87%E5%AD%97"><span class="toc-text">需求: 基于数据指纹的增量式爬虫, 爬取糗百文字</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-scrapy%E6%8F%90%E9%AB%98%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96%E6%95%88%E7%8E%87"><span class="toc-text">3.scrapy提高数据爬取效率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83"><span class="toc-text">4.虚拟环境</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85"><span class="toc-text">安装:</span></a></li></ol></li></ol>
  </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header"><i class="iconfont icon-wenzhang_huaban" style="padding-right: 2px;"></i>Recent Posts</div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-09-06</div>
        <a href="/2020/09/06/Python基础知识点大全/"><div class="recent-posts-item-content">Python基础知识点大全</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-08-29</div>
        <a href="/2020/08/29/Django + uWSGI + Nginx 的生产环境部署，及WSGI &amp; uwsgi &amp; uWSGI 的作用/"><div class="recent-posts-item-content">Django + uWSGI + Nginx 的生产环境部署，及WSGI &amp; uwsgi &amp; uWSGI 的作用</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-08-27</div>
        <a href="/2020/08/27/Docker 的基本常用命令/"><div class="recent-posts-item-content">Docker 的基本常用命令</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-08-25</div>
        <a href="/2020/08/25/Docker 镜像 &amp; 容器和镜像的联系 读写层/"><div class="recent-posts-item-content">Docker 镜像 &amp; 容器和镜像的联系 读写层</div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
        </div>
      </div>
    </div>
  </div>
  
  <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>Copyright ©
          
          2020 -
          
          2020
        </span>
        &nbsp;
        <a href="/" class="footer-link">Mr xu </a>
      </div>
    </div>

    
    <div class="footer-dsc">
      
      Powered by
      <a href="https://hexo.io/" class="footer-link" target="_blank" rel="nofollow noopener noreferrer">&nbsp;Hexo </a>
      
      
      <span>&nbsp;|&nbsp;</span>
      
      
      Theme -
      <a href="https://github.com/theme-kaze" class="footer-link" target="_blank"
        rel="nofollow noopener noreferrer">&nbsp;Kaze</a>
      
    </div>
    
    
    
    
</footer>
  <a role="button" id="scrollbutton" class="basebutton" >
  <i class="iconfont icon-arrowleft button-icon"></i>
</a>
<a role="button" id="menubutton" class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a role="button" id="popbutton" class="basebutton">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a role="button" id="darkbutton" class="basebutton darkwidget">
  <i class="iconfont icon-weather button-icon"></i>
</a>
<a role="button" id="searchbutton" class="basebutton searchwidget">
  <i class="iconfont icon-search button-icon"></i>
</a>

  
  
  
  <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img');
    var i;
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a');
      wrapper.setAttribute('data-fslightbox', 'gallery');
      wrapper.setAttribute('href', img[i].getAttribute('data-src'));
      wrapper.style.cssText = 'width: 100%; display: flex; justify-content: center;';
      img[i].before(wrapper);
      wrapper.append(img[i]);
    }
    refreshFsLightbox();
  }
</script>
<script>loadScript("//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js", addImgLayout)</script>
  
  
  
<script src="/js/main.js"></script>

  
  
  <script>
    var addLazyload = function () {
      var observer = lozad('.lozad', {
        load: function (el) {
          el.srcset = el.getAttribute('data-src');
        },
        loaded: function (el) {
          el.classList.add('loaded');
        }
      });
      observer.observe();
    }
  </script>
  <script>loadScript("/js/lib/lozad.min.js", addLazyload)</script>
  
  
</body>

</html>