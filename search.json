[{"title":"Python基础知识点大全","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/09/06/Python基础知识点大全/","content":"Python知识点进程进程的定义\n进程是资源分配最小单位\n一个运行起来的程序就是一个进程什么是程序（程序是我们存储在硬盘里的代码、文件）当我们双击图标，打开程序的时候，实际上就是通过I/O操作（读写）内存条里面内存条就是我们所指的资源\n进程之间内存独立，不能相互访问\n\n进程定义拓展回答内容：\n\n程序并不能单独运行，只有将程序装载到内存中，系统为它分配资源才能运行，这种执行的程序就称之为进程\n程序和进程的区别就在于：程序是指令的集合，它是进程运行的静态描述文本；进程是程序的一次执行活动，属于动态概念\n在多道编程中，我们允许多个程序同时加载到内存中，在操作系统的调度下，可以实现并发地执行。\n进程的出现让每个用户感觉到自己独享CPU，因此，进程就是为了在CPU上实现多道编程而提出的。\n进程之间有自己独立的内存，各进程之间不能相互访问\n创建一个新线程很简单，创建新进程需要对父进程进行复制\n\n进程和程序的区别\n程序只是一个普通文件，是一个机器代码指令和数据的集合，所以，程序是一个静态的实体\n而进程是程序运行在数据集上的动态过程，进程是一个动态实体，它应创建而产生，应调度执行因等待资源或事件而被处于等待状态，因完成任务而被撤消\n进程是系统进行资源分配和调度的一个独立单位\n一个程序对应多个进程，一个进程为多个程序服务（两者之间是多对多的关系）\n一个程序执行在不同的数据集上就成为不同的进程，进程可以控制 块 来唯一标识每个程序\n\n多道编程概念:\n\n多道编程： 在计算机内存中同时存放几道相互独立的程序，他们共享系统资源，相互穿插运行\n单道编程： 计算机内存中只允许一个的程序运行\n\n进程具有独立的内存空间，所以没有办法相互通信进程通信:\npython提供了多种进程通信的方式，主要Queue和Pipe这两种方式，Queue用 于多个进程间实现通信，Pipe是两个进程的通信。\n\nQueue有两个方法：\\1. Put方法：以插入数据到队列中\\2. Get方法：从队列读取并且删除一个元素\nPipe常用于两个进程，两个进程分别位于管道的两端Pipe方法返回（conn1,conn2）代表一个管道的两个端，Pipe方法有duplex参数，默认为True，即全双工模式，若为FALSE，conn1只负责接收信息，conn2负责发送，\nmanagers\nRabbitMQ、redis等\n\n进程间互相访问数据的四种方法:注：不同进程间内存是不共享的，所以互相之间不能访问对方数据\n\n利用Queues实现父进程到子进程（或子进程间）的数据传递\n使用管道pipe实现两个进程间数据传递\nManagers实现很多进程间数据共享\n借助redis中间件进行数据共享\n\n进程池:为什么需要进程池?\n\n一次性开启指定数量的进程\n如果有十个进程，有一百个任务，一次可以处理多少个（一次性只能处理十个）\n防止进程开启数量过多导致服务器压力过大\n开进程池是为了效率,进程直接的切换是属于IO调度，每个进程的内存空间都有自己的寄存器，堆栈和文件。\n\n12345678910111213141516171819202122from  multiprocessing import Process,Poolimport time,osdef foo(i):    time.sleep(2)    print(&quot;in the process&quot;,os.getpid()) #打印子进程的pidreturn i+100def call(arg):print(&#x27;--&gt;exec done:&#x27;,arg,os.getpid())if __name__ == &#x27;__main__&#x27;:    pool = Pool(3)                      #进程池最多允许5个进程放入进程池    print(&quot;主进程pid：&quot;,os.getpid())     #打印父进程的pid    for i in range(10):       #用法1 callback作用是指定只有当Foo运行结束后就执行callback调用的函数,父进程调用的callback函数        pool.apply_async(func=foo, args=(i,),callback=call)        #用法2 串行 启动进程不在用Process而是直接用pool.apply()        # pool.apply(func=foo, args=(i,))    print(&#x27;end&#x27;)    pool.close()    #关闭pool    pool.join()     #进程池中进程执行完毕后再关闭，如果注释，那么程序直接关闭。\n\n进程池优点：\n123不仅仅减少了IO而且还减少了内存。下面的例子便可以区分 其他语言的进程池还可以根据服务器的压力来增减，有着上限和下限。12\n\n建议：超过五个进程就用进程池\n有了进程为什么还要线程？\n\n进程优点：提供了多道编程，让我们感觉我们每个人都拥有自己的CPU和其他资源，可以提高计算机的利用率\n进程的两个重要缺点a. 第一点：进程只能在一个时间干一件事，如果想同时干两件事或多件事，进程就无能为力了。b. 第二点：进程在执行的过程中如果阻塞，即使进程中有些工作不依赖于输入的数据，也将无法执行（例如等待输入，整个进程就会挂起）。c. 例如，我们在使用qq聊天， qq做为一个独立进程如果同一时间只能干一件事，那他如何实现在同一时刻 即能监听键盘输入、又能监听其它人给你发的消息d. 你会说，操作系统不是有分时么？分时是指在不同进程间的分时呀e. 即操作系统处理一会你的qq任务，又切换到word文档任务上了，每个cpu时间片分给你的qq程序时，你的qq还是只能同时干一件事呀\n\n\n线程线程的定义:\n\n线程是系统调度的最小单位\n同进程下线程资源共享\n进程无法自己执行，只有通过线程操作CPU，内存\n为了保证数据安全，必须使用线程锁\n\n线程定义拓展回答内容:\n\n线程是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位\n一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务\n无论你启多少个线程，你有多少个cpu, Python在执行的时候会淡定的在同一时刻只允许一个线程运行\n进程本身是无法自己执行的，要操作cpu，必须创建一个线程，线程是一系列指令的集合\n所有在同一个进程里的线程是共享同一块内存空间的，不同进程间内存空间不同\n同一个进程中的各线程可以相互访问资源，线程可以操作同进程中的其他线程，但进程仅能操作子进程\n两个进程想通信，必须要通过一个中间代理\n对主线程的修改可能回影响其他子线程，对主进程修改不会影响其他进程因为进程间内存相互独立，但是同一进程下的线程共享内存\n\n进程和线程的区别:\n\n进程包含线程\n线程共享内存空间\n进程内存是独立的（不可互相访问）\n进程可以生成子进程，子进程之间互相不能互相访问（相当于在父级进程克隆两个子进程）\n在一个进程里面线程之间可以交流。两个进程想通信，必须通过一个中间代理来实现\n创建新线程很简单，创建新进程需要对其父进程进行克隆。\n一个线程可以控制或操作同一个进程里面的其它线程。但进程只能操作子进程。\n父进程可以修改不影响子进程，但不能修改。\n线程可以帮助应用程序同时做几件事\n\nfor循环同时启动多个线程：\n123456789import threadingimport timedef sayhi(num): #定义每个线程要运行的函数    print(&quot;running on number:%s&quot; %num)    time.sleep(3)for i in range(50):    t = threading.Thread(target=sayhi,args=(&#x27;t-%s&#x27;%i,))    t.start()\n\nt.join()： 实现所有线程都执行结束后再执行主线程：\n1234567891011121314151617181920import threadingimport timestart_time = time.time()def sayhi(num): #定义每个线程要运行的函数    print(&quot;running on number:%s&quot; %num)    time.sleep(3)t_objs = []    #将进程实例对象存储在这个列表中for i in range(50):    t = threading.Thread(target=sayhi,args=(&#x27;t-%s&#x27;%i,))    t.start()          #启动一个线程，程序不会阻塞    t_objs.append(t)print(threading.active_count())    #打印当前活跃进程数量for t in t_objs: #利用for循环等待上面50个进程全部结束    t.join()     #阻塞某个程序print(threading.current_thread())    #打印执行这个命令进程print(&quot;----------------all threads has finished.....&quot;)print(threading.active_count())print(&#x27;cost time:&#x27;,time.time() - start_time)\n\n\n\nsetDaemon(): 守护线程，主线程退出时，需要子线程随主线程退出:\n123456789101112import threadingimport timestart_time = time.time()def sayhi(num): #定义每个线程要运行的函数    print(&quot;running on number:%s&quot; %num)    time.sleep(3)for i in range(50):    t = threading.Thread(target=sayhi,args=(&#x27;t-%s&#x27;%i,))    t.setDaemon(True)  #把当前线程变成守护线程，必须在t.start()前设置    t.start()          #启动一个线程，程序不会阻塞print(&#x27;cost time:&#x27;,time.time() - start_time)\n\n\n\nGIL全局解释器锁：保证同一时间仅有一个线程对资源有操作权限:\n12作用：在一个进程内，同一时刻只能有一个线程执行&#96;&#96;说明：python多线程中GIL锁只是在CPU操作时（如：计算）才是串行的，其他都是并行的，所以比串行快很多\n\n\n为了解决不同线程同时访问同一资源时，数据保护问题，而产生了GIL\nGIL在解释器的层面限制了程序在同一时间只有一个线程被CPU实际执行，而不管你的程序里实际开了多少条线程\nCPython自己定义了一个全局解释器锁，同一时间仅仅有一个线程可以拿到这个数据\npython之所以会产生这种不好的状况是因为python启用一个线程是调用操作系统原生线程，就是C接口\n但是这仅仅是CPython这个版本的问题，在PyPy，中就没有这种缺陷\n\n线程锁:\n\n当一个线程对某个资源进行CPU计算的操作时加一个线程锁，只有当前线程计算完成主动释放锁，其他线程才能对其操作\n这样就可以防止还未计算完成，释放GIL锁后其他线程对这个资源操作导致混乱问题\n线程锁本质把线程中的数据加了一把互斥锁\n\n有了GIL全局解释器锁为什么还需要线程锁因为cpu是分时使用的\nGIL是限制同一个进程中只有一个线程进入Python解释器。。。。。而线程锁是由于在线程进行数据操作时保证数据操作的安全性(同一个进程中线程之间可以共用信息，如果同时对数据进行操作，则会出现公共数据错误)其实线程锁完全可以替代GIL，但是Python的后续功能模块都是加在GIL基础上的，所以无法更改或去掉GIL,这就是Python语言最大的bug…只能用多进程或协程改善，或者直接用其他语言写这部分\n死锁定义两个以上的进程或线程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去\n用户锁:\n12345678910import timeimport threadinglock = threading.Lock()          #1 生成全局锁def addNum():    global num                  #2 在每个线程中都获取这个全局变量    print(&#x27;--get num:&#x27;,num )    time.sleep(1)    lock.acquire()              #3 修改数据前加锁    num  -= 1                   #4 对此公共变量进行-1操作    lock.release()              #5 修改后释放\n\n\n\nSemaphore(信号量):\n\n互斥锁 同时只允许一个线程更改数据，而Semaphore是同时允许一定数量的线程更改数据\n比如厕所有3个坑，那最多只允许3个人上厕所，后面的人只能等里面有人出来了才能再进去\n作用就是同一时刻允许运行的线程数量\n\n多线程\n\nGIL锁：\n全局解释锁，每次只能一个线程获得cpu的使用权：为了线程安全，也就是为了解决多线程之间的数据完整性和状态同步而加的锁，因为我们知道线程之间的数据是共享的。\n\njoin()作用：\n在进程中可以阻塞主进程的执行, 直到等待子线程全部完成之后, 才继续运行主线程后面的代码\n\nsetDaemon()：\n将该线程标记为守护线程或用户线程\n\n\n线程池\n\n使用以下模块创建线程池：\n\n\n使用threadpool模块，这是个python的第三方模块，支持python2和python3\n使用concurrent.futures模块，这个模块是python3中自带的模块，但是，python2.7以上版本也可以安装使用\n\n\n线程池实现并发:\n\n  12345678910111213141516171819pythonimport requestsfrom concurrent.futures import ThreadPoolExecutordef fetch_request(url):    result &#x3D; requests.get(url)    print(result.text)url_list &#x3D; [    &#39;https:&#x2F;&#x2F;www.baidu.com&#39;,    &#39;https:&#x2F;&#x2F;www.google.com&#x2F;&#39;,         #google页面会卡住，知道页面超时后这个进程才结束    &#39;http:&#x2F;&#x2F;dig.chouti.com&#x2F;&#39;,          #chouti页面内容会直接返回，不会等待Google页面的返回]pool &#x3D; ThreadPoolExecutor(10)            # 创建一个线程池，最多开10个线程for url in url_list:    pool.submit(fetch_request,url)       # 去线程池中获取一个线程，线程去执行fetch_request方法pool.shutdown(True)                      # 主线程自己关闭，让子线程自己拿任务执行\n\n\n\n\n协程什么是协程（进入上一次调用的状态）\n\n协程，又称微线程，纤程，协程是一种用户态的轻量级线程。\n线程的切换会保存到CPU的栈里，协程拥有自己的寄存器上下文和栈，\n协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈\n协程能保留上一次调用时的状态（即所有局部状态的一个特定组合），每次过程重入时，就相当于进入上一次调用的状态\n协程最主要的作用是在单线程的条件下实现并发的效果，但实际上还是串行的（像yield一样）\n协程能在单线程处理高并发\n\n协程的定义：\n\n协程在单线程下实现并发效果\n协程遇IO自动切换\n协程保留上一次调用状态\n\n协程的优点:\n\n无需线程上下文切换的开销\n无需原子操作锁定及同步的开销，因为协程是串行的\n方便切换控制流，简化编程模型\n高并发，高扩展，低成本，一个cpu支持上万个协程没有问题，所以非常适合高并发处理\n\n协程的缺点:\n\n无法利用多核的优势，但是协程和进程配合就可以使协程运行在不同的cpu上，就可以利用 多核的优势，但是在现实中，大部分场景都没有这个需要\n只要一个协程阻塞（Blocking），就会阻塞整个协程，因为协程是串行的,这个问题必须要解决，才能让协程大范围应用\n\n\n解决方法：如果遇到io操作，则进行协程切换,去执行其他的协程，可以用gevent来实现，具体的实现是这样的，比如协程1通过os去读一个file，这个时候就是一个 io操作，在调用os的接口前，就会有一个列表，协议1的这个操作就会被注册到这个列表中，然后就切换到其他协程去处理；等待os拿到要读file后，也会把这个文件句柄放在这个列表中，然后等待在切换到协程1的时候，协程1就可以直接从列表中拿到数据，这样就可以实现不阻塞了\n\n协程处理并发：\n\nGevent遇IO自动切换\n\n\nGevent 是一个第三方库，可以轻松通过gevent实现并发同步或异步编程\n协程之所以快是因为遇到I/O操作就切换（最后只有CPU运算）\n其实Gevent模块仅仅是对greenlet的再封装，将I/O间的手动切换变成自动切换\n\n\nGreenlet遇IO手动切换\n\n\nGreenlet全部运行在主程序操作系统进程的内部，但它们被协作式地调度。\n\n使用协程处理并发注：Gevent只用起一个线程，当请求发出去后gevent就不管,永远就只有一个线程工作，谁先回来先处理\n1234567891011121314151617181920import geventimport requestsfrom gevent import monkeymonkey.patch_all()# 这些请求谁先回来就先处理谁def fetch_async(method, url, req_kwargs):    response = requests.request(method=method, url=url, **req_kwargs)    print(response.url, response.content)        ##### 发送请求 #####gevent.joinall([    gevent.spawn(fetch_async, method=&#x27;get&#x27;, url=&#x27;https://www.python.org/&#x27;, req_kwargs=&#123;&#125;),    gevent.spawn(fetch_async, method=&#x27;get&#x27;, url=&#x27;https://www.google.com/&#x27;, req_kwargs=&#123;&#125;),    gevent.spawn(fetch_async, method=&#x27;get&#x27;, url=&#x27;https://github.com/&#x27;, req_kwargs=&#123;&#125;),])\n\nselect、poll、epoll（重点）：I/O的实质是什么？I/O的实质是将硬盘中的数据，或收到的数据实现从内核态 copy到 用户态的过程本文讨论的背景是Linux环境下的network IO。比如微信读取本地硬盘的过程微信进程会发送一个读取硬盘的请求—-》操作系统只有内核才能够读取硬盘中的数据—》数据返回给微信程序（看上去就好像是微信直接读取）\n用户态 &amp; 内核态系统空间分为两个部分，一部分是内核态，一部分是用户态的部分内核态：内核态的空间资源只有操作系统能够访问用户态：我们写的普通程序使用的空间\n\n\nselect （能监控数量有限，不能告诉用户程序具体哪个连接有数据）\n\n单个进程就可以同时处理多个网络连接的io请求（同时阻塞多个io操作）。基本原理就是程序呼叫select，然后整个程序就阻塞状态，这时候，kernel内核就会轮询检查所有select负责的文件描述符fd，当找到其中那个的数据准备好了文件描述符，会返回给select，select通知系统调用，将数据从kernel内核复制到进程缓冲区(用户空间)。\n\npoll（和select一样，仅仅去除了最大监控数量）\n\n\npoll和select在本质上没有多大差别，但是poll没有最大文件描述符数量的限制差别如下：\n\n描述fd集合的方式不同，poll使用 pollfd 结构而不是select结构fd_set结构，所以poll是链式的，没有最大连接数的限制poll有一个特点是水平触发，也就是通知程序fd就绪后，这次没有被处理，那么下次poll的时候会再次通知同个fd已经就绪。\n\nepoll (不仅没有最大监控数量限制，还能告诉用户程序哪个连接有活跃)注：epoll被认为是linux下性能最好的多路io就绪通知方法\n\n\nepoll直到Linux2.6（centos6以后）才出现了由内核直接支持\nEpoll没有最大文件描述符数量限制\nepoll最重要的优点是他可以直接告诉用户程序哪一个，比如现在用epoll去监控10000个socket链接，交给内核去监测，现在有一个连接有数据了，在有有一个连接有数据了，epoll会直接高数用户程序哪个连接有数据了\n\nepoll是select和poll的改进方案，在 linux 上可以取代 select 和 poll，可以处理大量连接的性能问题\n\nepoll能实现高并发原理\n\n\nepoll() 中内核则维护一个链表，epoll_wait 直接检查链表是不是空就知道是否有文件描述符准备好了。\n\n在内核实现中 epoll 是根据每个 sockfd 上面的与设备驱动程序建立起来的回调函数实现的。\n\n某个 sockfd 上的事件发生时，与它对应的回调函数就会被调用，把这个 sockfd 加入链表。\n\nepoll上面链表中获取文件描述，这里使用内存映射（mmap）技术， 避免了复制大量文件描述符带来的开销内存映射（mmap）：内存映射文件，是由一个文件到一块内存的映射，将不必再对文件执行I/O操作\nepoll有4个动作：创建，注册，等待，取消注册，很显然我们用不着\n\n\n\nepoll和select，poll还有一个本质的区别的就是:\nselect 和 poll 只有在下次在循环回来，再去操作系统获取文件描述符epoll 会直接告诉程序，我们这里已经就绪了，你可以接受数据了，等下一次协程去调用 epoll_wait 的时候就可以直接拿到就绪的文件描述符\n\n\n猴子补丁\n即在运行时对方法 / 类 / 属性 / 功能进行修改，把新的代码作为解决方案代替原有的程序，也就是为其打上补丁。\n在使用gevent模块的使用会遇到猴子补丁\n1234567import gevent.monkey gevent.monkey.patch_all()注解：使用猴子补丁的方式，gevent能够修改标准库里面大部分的阻塞式系统调用，包括socket、ssl、threading和 select等模块，而变为协作式运行。也就是通过猴子补丁的monkey.patch_xxx()来将python标准库中模块或函数改成gevent中的响应的具有协程的协作式对象。这样在不改变原有代码的情况下，将应用的阻塞式方法，变成协程式的。\n\n\n\n装饰器介绍\n装饰器(Decorators)是 Python 的一个重要部分。简单地说：他们是修改其他函数的功能的函数。他们有助于让我们的代码更简短，也更Pythonic（Python范儿）。\n装饰器的概念：\n123451. 装饰器实际上就是一个函数2. 有2个特别之处，参数是一个函数。返回值是一个参数12\n\n装饰器的简单理解:\n12345678实际上就是为了给一个程序添加功能，但是该程序已经上线或者已被使用，那么就不能大批量的修改源码，这样不现实，因此就产生了装饰器。注意点：1. 不能修改被装饰的函数的源代码2. 不能修改被装饰的函数的调用方式12345\n\n装饰器组成方式：\n1234函数+实参高阶函数+返回值高阶函数+嵌套函数+语法糖 &#x3D; 装饰器1\n\n\n有关高阶函数的理解：\n\n把一个函数名当作实参传给另外一个函数（”实参高阶函数“）\n返回值中包含函数名（”返回值高阶函数“）\n\n\n嵌套函数的理解：\n嵌套函数指的是在函数内部定义一个函数，而不是调用。\n\n语法糖：\n写法：@xx ，一般写在函数的上方\n\n\n装饰器实例\n\n使用高阶函数模拟装饰器:\n123456789101112131415161718import timedef timer(func):\tstart_time = time.time()\tfunc()\tprint &#x27;函数执行时间为&#x27;, time.time() - start_timedef test():\tprint &#x27;开始执行test&#x27;\ttime.sleep(3)\tprint &#x27;test执行结束&#x27;timer(test)&#x27;&#x27;&#x27;开始执行testtest执行结束函数执行时间为 3.00332999229&#x27;&#x27;&#x27;123456789101112131415\n\n计算运行时间装饰器:\n12345678910111213141516import timedef timer(func):   #timer(test1)  func=test1    def deco(*args,**kwargs):        start_time = time.time()        func(*args,**kwargs)      #run test1        stop_time = time.time()        print(&quot;running time is %s&quot;%(stop_time-start_time))    return deco@timer     # test1=timer(test1)def test1():    time.sleep(3)    print(&quot;in the test1&quot;)test1()12345678910111213\n\n装饰无参函数，示例代码如下：\n12345678910111213141516171819202122#装饰器装饰的函数无参数def timer(func):      #func其实指的就是test    def deco():        start = time.time()        func()               #这里其实是对test的调用        stop = time.time()        print (stop-start)    return deco@timer               #test函数使用装饰器def test():    time.sleep(2)    print (&quot;test is running&quot;)test()打印结果：test is running2.00351095212345678910111213141516171819\n\n\n\n\n\n\n装饰有参函数，示例代码如下：\n12345678910111213141516171819202122#装饰器装饰的函数有参数def timer(func):    def deco(*args,**kwargs):    #添加可变参数*args和**kwargs        start = time.time()        func(*args,**kwargs)      #这里也是一样，添加可变参数*args和**kwargs        stop = time.time()        print (stop-start)    return deco@timerdef test(value):     #test函数有个参数value,正因为装饰器timer装饰的函数test有参数value,因此在timer中的有了可变参数    time.sleep(2)    print (&quot;test is running %s&quot; %value)test(&quot;22&quot;)打印结果：test is running 222.0042440891312345678910111213141516171819\n\n\n\n\n\n\n带参数的装饰器，示例代码如下：\n123456789101112131415161718192021222324252627282930313233343536#装饰器带参数def timer(parameter):    def out_wapper(func):        def wapper(*wargs,**kwargs):            if parameter == &quot;task1&quot;:                start = time.time()                func(*wargs,**kwargs)                stop = time.time()                print (&quot;the task1 is run:&quot;,stop-start)            elif parameter == &quot;task2&quot;:                func(*wargs, **kwargs)                print (&quot;the task2 is run:&quot;)        return wapper    return out_wapper@timer(parameter = &quot;task1&quot;)def task1():    time.sleep(2)    print &quot;in the task1&quot;@timer(parameter = &quot;task2&quot;)def task2():    time.sleep(2)    print &quot;in the task2&quot;task1()task2()打印结果：in the task1(&#x27;the task1 is run:&#x27;, 2.002906084060669)in the task2the task2 is run:123456789101112131415161718192021222324252627282930313233\n\n\n\n\n\n装饰器使用场景\n\n授权：装饰器能有助于检查某个人是否被授权去使用一个web应用的端点(endpoint)。它们被大量使用于Flask和Django web框架中日志：在记录日志的地方添加装饰器缓存：通过装饰器获取缓存中的值\n\n闭包\n\n定义：\n如果在一个函数的内部定义了另一个函数，外部的我们叫他外函数，内部的我们叫他内函数。那闭包就是，在一个外函数中定义了一个内函数，内函数里运用了外函数的临时变量，并且外函数的返回值是内函数的引用。这样就构成了一个闭包\n\n\n\n迭代器定义:\n\n迭代是Python最强大的功能之一，是访问集合元素的一种方式。\n迭代器是一个可以记住遍历的位置的对象。\n迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。\n迭代器有两个基本的方法：iter() 和 next()。\n字符串，列表或元组对象都可用于创建迭代器：\n\n可迭代对象:\n在Python世界里，一切皆对象。对象根据定义的维度，又可以分为各种不同的类型，比如：文件对象，字符串对象，列表对象。。。等等。一句话：“实现了__inter__方法的对象就叫做可迭代对象”，__inter__方法的作用就是返回一个迭代器对象。直观理解就是能用for循环进行迭代的对象就是可迭代对象。比如：字符串，列表，元祖，字典，集合等等，都是可迭代对象。\nnext()与iter():\n12345678910111213next()返回迭代器的下一个项目next语法:next(iterator[,dafault])iterator -- 可迭代对象default -- 可选，用于设置在没有下一个元素时返回该默认值，如果不设置，又没有下一个元素则会触发 StopIteration 异常。iter():iter()函数用来生成迭代器iter语法:12345678910\n\n迭代器实现斐波那契\n1234567891011121314151617181920class Fib():    def __init__(self, n):        self.a = 0        self.b = 1        self.n = n        self.count = 0    def __iter__(self):        return self    def next(self):        res = self.a        self.a, self.b = self.b, self.a + self.b        if self.count &gt; self.n:            raise StopIteration        self.count += 1        return resprint(list(Fib(5)))print(list(Fib(10)))\n\n\n生成器和迭代器之间的区别\n在使用生成器时，我们创建一个函数；在使用迭代器时，我们使用内置函数iter()和next()。 在生成器中，我们使用关键字‘yield’来每次生成/返回一个对象。 生成器中有多少‘yield’语句，你可以自定义。 每次‘yield’暂停循环时，生成器会保存本地变量的状态。而迭代器并不会使用局部变量，它只需要一个可迭代对象进行迭代。 使用类可以实现你自己的迭代器，但无法实现生成器。 生成器运行速度快，语法简洁，更简单。 迭代器更能节约内存。\n\n\n\n生成器\n生成器定义、简介\n在python中，生成器是根据某种算法边循环边计算的一种机制。主要就是用于操作大量数据的时候，一般我们会将操作的数据读入内存中处理，可以计算机的内存是比较宝贵的资源，我认为的当要处理的数据超过内存四分之一的大小时就应该使用生成器。\n\n生成器的作用\n\n\n\n通过列表生成式，我们可以直接创建一个列表，但是，受到内存限制，列表容量肯定是有限的。\n\n而且，创建一个包含100万个元素的列表，不仅占用很大的存储空间，如果我们仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。\n\n所以，如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？\n\n这样就不必创建完整的list，从而节省大量的空间。在Python中，这种一边循环一边计算的机制，称为生成器：generator。\n\n\n\n特点\n\n\n和传统的容器相比，生成器更节省内存。\n\n延迟计算，在我们需要结果时就调用一下生成器的next()方法即可。\n\n可迭代，你可以像遍历list一样，遍历生成器\n\n\n\n生成器工作原理\n\n\n生成器是这样一个函数，它记住上一次返回时在函数体中的位置。\n\n对生成器函数的第二次（或第 n 次）调用跳转至该函数中间，而上次调用的所有局部变量都保持不变。\n\n生成器不仅“记住”了它数据状态；生成器还“记住”了它在流控制构造中的位置。\n\n生成器是一个函数，而且函数的参数都会保留。\n\n迭代到下一次的调用时，所使用的参数都是第一次所保留下的，即是说，在整个所有函数调用的参数都是第一次所调用时保留的，而不是新创建的\n在python中有两种方式创建生成器：生成器表达式 和 生成器函数。\n\n\n\n生成器 和 普通函数的区别 ？\n\n生成式函数和普通函数只有一个区别，普通函数使用return返回结果，而生成器函 数使用yield返回结果。yield的特点在于，它并不是结束函数，而是在返回结果后将函数处于一种挂起状态，等待再次next函数的调用，然后从上次挂起的地方(yield)继续执行。\n\n可迭代的数据类型\n\n列表、元组、字典和集合都是可迭代的对象，可以从其中获得迭代器。所有这些对象都可用iter()方法获取迭代器:\n\nyield运行机制:\n在Python中，yield就是这样的一个生成器。\n\n\n\n当你问生成器要一个数时，生成器会执行，直至出现 yield 语句，生成器把yield 的参数给你，之后生成器就不会往下继续运行。\n当你问他要下一个数时，他会从上次的状态开始运行，直至出现yield语句，把参数给你，之后停下。如此反复\n在python中，当你定义一个函数，使用了yield关键字时，这个函数就是一个生成器\n它的执行会和其他普通的函数有很多不同，函数返回的是一个对象，而不是你平常所用return语句那样，能得到结果值。如果想取得值，那得调用next()函数\n每当调用一次迭代器的next函数，生成器函数运行到yield之处，返回yield后面的值且在这个地方暂停，所有的状态都会被保持住，直到下次next函数被调用，或者碰到异常循环退出。\n\n1234567891011def fib(max_num):    a,b = 1,1    while a &lt; max_num:        yield b        a,b=b,a+bg = fib(10)               #生成一个生成器：[1，2, 3, 5, 8, 13]print(g.__next__())       #第一次调用返回：1print(list(g))            #把剩下元素变成列表：[2, 3, 5, 8, 13]\n\n每次执行send()或next()只是返回了对应yield表达式的参数值，其实对应表达式并未执行，直到下次再执行send()或next()才会执行上次返回参数的yield表达式，所谓的执行yield表达式就是给其赋值，并返回下一个yield表达式的参数值！yield机制详细地址\n\n面向对象简介：\n面向对象编程(Object Oriented Programming-OOP) 是一种解决软件复用的设计和编程方法。 这种方法把软件系统中相近相似的操作逻辑和操作 应用数据、状态,以类的型式描述出来,以对象实例的形式在软件系统中复用,以达到提高软件开发效率的作用。其实面向对象也很简单，却也很难，熟能生巧。你需要了解类和对象，要学会定义类，创建对象。\n特点：\n\n类(Class): 用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。\n方法：类中定义的函数。\n类变量：类变量在整个实例化的对象中是公用的。类变量定义在类中且在函数体之外。类变量通常不作为实例变量使用。\n数据成员：类变量或者实例变量用于处理类及其实例对象的相关的数据。\n方法重写：如果从父类继承的方法不能满足子类的需求，可以对其进行改写，这个过程叫方法的覆盖（override），也称为方法的重写。\n局部变量：定义在方法中的变量，只作用于当前实例的类。\n实例变量：在类的声明中，属性是用变量来表示的，这种变量就称为实例变量，实例变量就是一个用 self 修饰的变量。\n继承：即一个派生类（derived class）继承基类（base class）的字段和方法。继承也允许把一个派生类的对象作为一个基类对象对待。\n实例化：创建一个类的实例，类的具体对象。\n对象：通过类定义的数据结构实例。对象包括两个数据成员（类变量和实例变量）和方法。\n\n123下面详细介绍:\n\n1. 方法1.1 静态方法：\n\n定义：使用装饰器@staticmethod。参数随意，没有“self”和“cls”参数，但是方法体中不能使用类或实例的任何属性和方法；\n调用：实例对象和类对象都可以调用。\n特性: 静态方法只是名义上归类管理，实际上在静态方法里访问不了类或则实例中的任何属性\n作用：静态方法可以更好的组织代码，防止代码变大后变得比较混乱。\n静态方法使用场景：\n我们要写一个只在类中运行而不在实例中运行的方法.\n经常有一些跟类有关系的功能但在运行时又不需要实例和类参与的情况下需要用到静态方法.\n比如更改环境变量或者修改其他类的属性等能用到静态方法.\n这种情况可以直接用函数解决, 但这样同样会扩散类内部的代码，造成维护困难.\n\n\n\n123456789101112class Dog(object):    def __init__(self,name):        self.name = name    @staticmethod    def eat():        print(&quot;I am a static method&quot;)d = Dog(&quot;ChenRonghua&quot;)d.eat()                     #方法1：使用实例调用Dog.eat()                   #方法2：使用类直接调用\n\n\n\n1.2 类方法：\n\n定义：使用装饰器@classmethod。第一个参数必须是当前类对象，该参数名一般约定为“cls”，通过它来传递类的属性和方法（不能传实例的属性和方法）；\n调用：实例对象和类对象都可以调用。\n作用：无需实例化直接被类调用\n类方法使用场景： 当我们还未创建实例，但是需要调用类中的方法\n\n1234567891011121314class Dog(object):    name = &#x27;类变量&#x27; #在这里如果不定义类变量仅定义实例变量依然报错    def __init__(self,name):        self.name = &#x27;实例变量&#x27;        self.name = name    @classmethod    def eat(self,food):        print(&quot;%s is eating %s&quot;%(self.name,food))Dog.eat(&#x27;baozi&#x27;)                   #方法1：使用类直接调用d = Dog(&quot;ChenRonghua&quot;)          d.eat(&quot;包子&quot;)                      #方法2：使用实例d调用\n\n\n\n1.3 实例方法：\n\n定义：第一个参数必须是实例对象，该参数名一般约定为“self”，通过它来传递实例的属性和方法（也可以传类的属性和方法）；\n调用：只能由实例对象调用。\n\n1234567891011121314class Dog(object):    def __init__(self, name):        self.name = name    @property    def eat(self):        print(&quot; %s is eating&quot; % self.name)d = Dog(&quot;ChenRonghua&quot;)d.eat()# 调用会出以下错误， 说NoneType is not callable, 因为eat此时已经变成一个静态属性了， # 不是方法了， 想调用已经不需要加()号了，直接d.eat就可以了\n\n\n\n1.4 魔法方法:我们在调用python类中的某个方法时，通常会看到某些特殊的方法，它们总被双下划线所包围，像这种格式：”方法名“，这些方法很强大，充满魔力，可以让你实现很多功能。，如果你的对象实现（重载）了这些方法中的某一个，那么这个方法就会在特殊的情况下被 Python 所调用，你可以定义自己想要的行为，而这一切都是自动发生的。因此了解这类方法的作用及用户很有必要，以下对基本魔法方法做出总结，请看：魔法方法表格\ntype生成类调用顺序:\nnew : 先于init__方法，每生成一个实例执行一次，new 类方法创建实例对象__init : init__方法每生成一个实例就会执行一次，初始化实例对象__call : 后与init__方法，C()() 使用类再加一个括号调用， C为类名称__del : 析构方法，删除无用的内存对象（当程序结束会自动自行析构方法）\n类实例化时魔法方法调用顺序\n12345678910111213141516171819202122232425262728293031323334353637class Student(object):    def __new__(cls, *args, **kwargs):        print(&#x27;__new__&#x27;)        return object.__new__(cls)   # 必须返回父类的__new__方法，否则不不执行__init__方法，无法创建实例        def __init__(self,name):        print(&#x27;__init__&#x27;)        self.name = name    def __str__(self):                # 作用：打印实例时显示指定字符串，而不是内存地址        print(&#x27;__str__&#x27;)        return self.name    def __call__(self, *args, **kwargs):        # 当执行C()(*args) 或者 s1(*args) 就会执行__call__        print(&#x27;__call__&#x27;,*args)    def __del__(self):                # 作用：清除无用的实例对内存的暂用        print(&#x27;__del__&#x27;)#1、实例化时机会执行__new__、__init__s1 = Student(&#x27;tom&#x27;)#2、执行 实例()  就会执行__call__ 方法，并将参数传递给__call__函数s1(&#x27;call01&#x27;)#3、当打印实例时就会执行 __str__ 方法下返回的字符串（默认返回的实例地址）print(s1)#4、析构方法：当删除实例时就会调用 __del__ 方法del s1# 析构方法作用：在程序结束后会自动执行析构方法删除所有实例# 但是在程序运行时有很多实例是无用的，但是python内存回收机制却不会自动删除他们，这样就浪费内存# 我们可以执行 del s1 ，那么在程序运行时，python内存回收机制会检测到这些实例时无用的，才会删除# 其实我们执行del s1，并没有回收内存，只不过是摘除门牌号，python内存回收机制发现没有门牌号后会自动回收内存\n\n\n\nnew &amp; __init__详解:\n\nnew 至少要有一个参数cls，代表要实例化的类，此参数在实例化时由Python解释器自动 提供\nnew 必须要有返回值，返回实例化出来的实例，这点在自己实现 new 时要特别注 意，可以return父类 new 出来的实例，或者直接是object的 new 出来的实例\ninit 有一个参数self，就是这个 new 返回的实例， init 在 \\new 的基础上 可以完成一些其它初始化的动作， init 不需要返回值 我们可以将类比作制造商， new 方法就是前期的原材料购买环节， init 方法就是在 有原材料的基础上，加工，初始化商品环节。\n\n1.5 单例模式：__new__方法书写：\n123456789101112131415161718192021222324class A(object):\tdef __init__(self):\tprint(self)\tprint(&quot;这是 init 方法&quot;)\tdef __new__(cls):\tprint(id(cls))\tprint(&quot;这是 __new__ 方法&quot;)\tret = object.__new__(cls)\tprint(res)\treturn retprint(id(A))--&gt;: 12345678987654321a = A()--&gt;: 12345678987654321这是 new 方法--&gt;: &lt;__main__.A object at 0x105b96ac8&gt;--&gt;: &lt;__main__.A object at 0x105b96ac8&gt;\n\n\n\n线程安全的单例:123456789101112131415161718192021222324252627282930import threading&quot;&quot;&quot;线程安全的单利模式紧跟with后面的语句被求值后，返回对象的 __enter__() 方法被调用，这个方法的返回值将被赋值给as后面的变量。当with后面的代码块全部被执行完之后，将调用前面返回对象的 __exit__()方法&quot;&quot;&quot;def synchronized(func):    func.__lock__ = threading.Lock()    def lock_func(*args, **kwargs):        with func.__lock__:            return func(*args, **kwargs)    return lock_funcclass Singleton(object):    instance = None    @synchronized    def __new__(cls):        # 关键在于这，每一次实例化的时候，我们都只会返回这同一个instance对象        if not cls.instance:            cls.instance = super(Singleton, cls).__new__(cls)        return cls.instance\n\n先看类，可以看出这里我们先定义了一个类属性instance，接着我们重写了父类的__new__方法，这个方法就是我们在实例化一个对象时最先调用的一个方法。和其他静态语言不一样，其他静态语言，直接调用了构造方法，一般情况下初始化的程序也写在构造方法之中。而python实例化一个对象和初始化是分开的。__new__是类方法，__init__是实例方法，也就是说，__init__是在对象已经创建完成之后，才执行。\n在python3中，调用父类的方法是用super()来调用。所以我们这里的思路就是，还是用父类的方法去创造，但是我们要加一个判断，就是说，当这个对象也就是类属性并不为空的时候，我们就不在实例化，而是返回一个已经实例化的类属性。\n线程不安全的单例123456789101112131415161718192021class Singleton(object):\t__instance = None\tdef __new__(cls, name, age):\t\t# 如果类属性__instance的值为None，那么就创建一个对象\t\tif not cls.__instance:\t\t\tcls.__instance = object.__new__(cls)\t\t# 如果已经有实例存在，直接返回\t\treturn cls.__instancea = Singleton(&quot;Zhangsan&quot;, 18)b = Singleton(&quot;lisi&quot;, 20)print(id(a))print(id(b))a.age = 30   # 给a指向的对象添加一个属性print(b.age)  # 获取b指向的对象的age属性\n\ndel\nPython 通过调用 init() 方法构造当前类的实例化对象，而 del() 方法，功能正好和 init() 相反，其用来销毁实例化对象。\n事实上在编写程序时，如果之前创建的类实例化对象后续不再使用，最好在适当位置手动将其销毁，释放其占用的内存空间（整个过程称为垃圾回收（简称GC））。大多数情况下，Python 开发者不需要手动进行垃圾回收，因为 Python 有自动的垃圾回收机制，能自动将不需要使用的实例对象进行销毁。\n无论是手动销毁，还是 Python 自动帮我们销毁，都会调用 del() 方法。\n2. 特性面向对象三大特性: 封装，继承，多态2.1 封装:\n在类中对数据的赋值、内部调用对外部用户是透明的\n这使类变成了一个胶囊或容器，里面包含着类的数据和方法\n作用：\n防止数据被随意修改\n使外部程序不需要关注对象内部的构造，只需要通过对外提供的接口进行直接访问\n\n\n\n\n\n继承的种类\n\n单继承：一个类继承单个基类\n多继承:一个类继承多个基类\n多级继承：一个类继承自单个基类，后者继承自另一个基类\n分层继承：多个类继承自单个基类\n混合继承：两种或多种类型继承的混合\n\n封装的好处：\n\n将变化隔离\n便于使用\n提高复用性\n提高安全性\n\n123封装：将数据进行封装到对象中，以供其他函数进行调用\n\n\n\n2.2 Inheritance 继承（代码重用:\n一个类可以派生出子类，在这个父类里定义的属性、方法自动被子类继承\n比如CS中的警察和恐怖分子，可以将两个角色的相同点写到一个父类中，然后同时去继承它\n使用经典类： Person.init(self, name, age) 并重写写父类Person的构造方法，实现，先覆盖，再继承，再重构\n\n继承的优点：\n\n节省代码,减少代码的重复性\n增强耦合性(也就是增强代码可读性)\n使代码更加规范化\n子类可以调用父类的所有属性\n\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class D:    def talk(self):        print(&#x27;D&#x27;)class B(D):    pass    # def talk(self):    #     print(&#x27;B&#x27;)class C(D):    pass    def talk(self):        print(&#x27;C&#x27;)class A(B,C):    pass    # def talk(self):    #     print(&#x27;A&#x27;)a = A()a.talk()# 黑人，白人都继承父类Person就可以都有父类的属性和方法了class Person(object):    def __init__(self,name,age):  #执行Person.__init__(self,name,age)时就会将传入的参数执行一遍        self.name = name          #所以在BlackPerson中不仅有name,age而且还有sex        self.age = age        self.sex = &quot;normal&quot;    def talk(self):        print(&quot;person is talking....&quot;)class WhitePerson(Person):    passclass BlackPerson(Person):    def __init__(self,name,age,strength):     #先覆盖，再继承，再重构        #先覆盖父类的__init__方法，再继承父类__init__，再加自己的参数        Person.__init__(self,name,age)        #先继承父类Person，这里self就是BlackPerson本身        #先将name,age传给子类BlackPerson,然后调用Person.__init__构造方法将参数出入父类（）        self.strength = strength              #然后再重构自己的方法,即写自己的参数        print(self.name,self.age,self.sex)        print(self.strength)    def talk(self):        print(&quot;black balabla&quot;)    def walk(self):        print(&quot;is walking....&quot;)b = BlackPerson(&quot;wei er smith&quot;,22,&quot;Strong&quot;)b.talk()b.walk()# 运行结果：# wei er smith 22 normal# Strong# black balabla# is walking....# person is talking....\n\n新式类经典类区别：\nPython 2.x中默认都是经典类，只有显式继承了object才是新式类Python 3.x中默认都是新式类，不必显式的继承object当类是经典类时，多继承情况下，会按照深度优先方式查找当类是新式类时，多继承情况下，会按照广度优先方式查找\n2.3 Polymorphism 多态（接口重用）\n多态是面向对象的重要特性,简单点说:“一个接口，多种实现”\n指一个基类中派生出了不同的子类，且每个子类在继承同样的方法名的同时又对父类的方法做了不同的实现\n这就是同一种事物表现出的多种形态\n比如黄种人继承了人talk这个功能，但是他说的是中文，而美国人的talk是英文，但是他们是同样的talk\n\n123作用：简单的讲就是允许父类调用子类的方法\n\n很多人喜欢将多态与多态性二者混为一谈，然后百思不得其解，其实只要分开看，就会很明朗。\n\n多态指的是一类事物有多种形态，（一个抽象类有多个子类，因而多态的概念依赖于继承）\n多态性是指具有不同功能的函数可以使用相同的函数名，这样就可以用一个函数名调用不同内容的函数。在面向对象方法中一般是这样表述多态性：向不同的对象发送同一条消息，不同的对象在接收时会产生不同的行为（即方法）。也就是说，每个对象可以用自己的方式去响应共同的消息。所谓消息，就是调用函数，不同的行为就是指不同的实现，即执行不同的函数。\n\n12345678910111213141516171819202122232425262728# 多态举例class Animal:    def __init__(self, name):    # Constructor of the class        self.name = name    def talk(self):              # Abstract method, defined by convention only        raise NotImplementedError(&quot;Subclass must implement abstract method&quot;)class Cat(Animal):    def talk(self):        return &#x27;Meow!&#x27;class Dog(Animal):    def talk(self):        return &#x27;Woof! Woof!&#x27;animals = [Cat(&#x27;Missy&#x27;),           Dog(&#x27;Lassie&#x27;)]for animal in animals:    print(animal.name + &#x27;: &#x27; + animal.talk())# 运行结果：# Missy: Meow!# Lassie: Woof! Woof!\n\nPython中多态的特点\n\n只关心对象的实例方法是否同名，不关心对象所属的类型；\n对象所属的类之间，继承关系可有可无；\n多态的好处可以增加代码的外部调用灵活度，让代码更加通用，兼容性比较强；\n多态是调用方法的技巧，不会影响到类的内部设计。\n\n3. 属性\n类的公有属性public_attrs：能在类的外部被使用或直接访问。在类内部的方法中使用时 public_attrs_attrs，在类的外部class_name.public_attrs。\n类的私有属性__private_attrs：两个下划线开头，声明该属性为私有，不能在类的外部被使用或直接访问。在类内部的方法中使用时 self.__private_attrs。\n类的(公有)方法在类的内部，使用 def 关键字来定义一个方法，与一般函数定义不同，类方法必须包含参数 self，且为第一个参数，self 代表的是类的实例。self 的名字并不是规定死的，也可以使用 this，但是最好还是按照约定是用 self。\n类的私有方法__private_method：两个下划线开头，声明该方法为私有方法，只能在类的内部调用 ，不能在类的外部调用。self.__private_methods。\n\n4. 反射: hasattr、getattr、setattr 和 delattr在做程序开发中，我们常常会遇到这样的需求：需要执行对象里的某个方法，或需要调用对象中的某个变量，但是由于种种原因我们无法确定这个方法或变量是否存在，这是我们需要用一个特殊的方法或机制要访问和操作这个未知的方法或变量，这中机制就称之为反射。\n反射就是通过字符串的形式，导入模块；通过字符串的形式，去模块寻找指定函数，并执行。利用字符串的形式去对象（模块）中操作（查找/获取/删除/添加）成员，一种基于字符串的事件驱动！–反射详解–\n四大属性:\nhasattr(ogj,name_str) 判断一个对象里是否有对应的字符串方法\n1234567891011class Dog(object):    def eat(self,food):        print(&quot;eat method!!!&quot;)d = Dog()#hasattr判断对象d是否有eat方法，有返回True，没有返回Falseprint(hasattr(d,&#x27;eat&#x27;))     #Trueprint(hasattr(d,&#x27;cat&#x27;))     #False12345678\n\ngetattr(obj,name_str) 根据字符串去获取obj对象里的对应的方法的内存地址\n123456789101112class Dog(object):    def eat(self):        print(&quot;eat method!!!&quot;)d = Dog()if hasattr(d,&#x27;eat&#x27;):          # hasattr判断实例是否有eat方法    func = getattr(d, &#x27;eat&#x27;)  # getattr获取实例d的eat方法内存地址    func()                    # 执行实例d的eat方法#运行结果：  eat method!!!123456789\n\n使用stattr给类实例对象动态添加一个新的方法\n12345678910111213141516171819def abc(self):    print(&quot;%s正在交谈&quot;%self.name)class Person(object):    def __init__(self,name):        self.name = namep = Person(&quot;汇森&quot;)setattr(p,&quot;talk&quot;,abc)   # 将abc函数添加到对象中p中，并命名为talkp.talk(p)               # 调用talk方法，因为这是额外添加的方法，需手动传入对象# 打印结果 汇森正在交谈setattr(p,&quot;age&quot;,30)     # 添加一个变量age,复制为30print(p.age)            # 打印结果:3012345678910111213141516\n\ndelattr删除对象中的变量。注意：不能用于删除方法\n12345678910111213class Person(object):    def __init__(self,name):        self.name = name    def talk(self):        print(&quot;%s正在交谈&quot;%self.name)p = Person(&quot;汇森&quot;)delattr(p,&quot;name&quot;)       # 删除name变量print(p.name)           # 此时将报错12345678910\n\n\n\nPython基础1. 深拷贝浅拷贝1.1 预备知识一——python的变量及其存储\n\npython的一切变量都是对象，变量的存储，采用了引用语义的方式，存储的只是一个变量的值所在的内存地址，而不是这个变量的只本身\n不管多么复杂的数据结构，浅拷贝都只会copy一层。理解：两个人公用一张桌子，只要桌子不变，桌子上的菜发生了变化两个人是共同感受的。\n\n1.2 浅copy与deepcopy\n\n浅copy： 不管多么复杂的数据结构，浅拷贝都只会copy一层\ndeepcopy : 深拷贝会完全复制原变量相关的所有数据，在内存中生成一套完全一样的内容，我们对这两个变量中任意一个修改都不会影响其他变量\n\n12345678910111213import copysourceList = [1,2,3,[4,5,6]]copyList = copy.copy(sourceList)deepcopyList = copy.deepcopy(sourceList)sourceList[3][0]=100print(sourceList)           # [1, 2, 3, [100, 5, 6]]print(copyList)             # [1, 2, 3, [100, 5, 6]]print(deepcopyList)         # [1, 2, 3, [4, 5, 6]]\n\n\n\n2. python垃圾回收机制2.1 引用计数:\n\n当一个对象的引用被创建或者复制时，对象的引用计数加1；当一个对象的引用被销毁时，对象的引用计数减1.\n当对象的引用计数减少为0时，就意味着对象已经再没有被使用了，可以将其内存释放掉。\n\n2.2 标记－清除:\n\n它分为两个阶段：第一阶段是标记阶段，GC会把所有的活动对象打上标记，第二阶段是把那些没有标记的对象非活动对象进行回收。\n对象之间通过引用（指针）连在一起，构成一个有向图\n从根对象（root object）出发，沿着有向边遍历对象，可达的（reachable）对象标记为活动对象，不可达的对象就是要被清除的非活动对象，根对象就是全局变量、调用栈、寄存器。\n\n123注：像是PyIntObject、PyStringObject这些不可变对象是不可能产生循环引用的，因为它们内部不可能持有其它对象的引用。\n\n\n\n在上图中，可以从程序变量直接访问块1，并且可以间接访问块2和3,程序无法访问块4和5\n第一步将标记块1，并记住块2和3以供稍后处理。\n第二步将标记块2，第三步将标记块3，但不记得块2，因为它已被标记。\n扫描阶段将忽略块1，2和3，因为它们已被标记，但会回收块4和5。\n\n2.3 分代回收：\n\n分代回收是建立在标记清除技术基础之上的，是一种以空间换时间的操作方式。\nPython将内存分为了3“代”，分别为年轻代（第0代）、中年代（第1代）、老年代（第2代）\n他们对应的是3个链表，它们的垃圾收集频率与对象的存活时间的增大而减小。\n新创建的对象都会分配在年轻代，年轻代链表的总数达到上限时，Python垃圾收集机制就会被触发\n把那些可以被回收的对象回收掉，而那些不会回收的对象就会被移到中年代去，依此类推\n老年代中的对象是存活时间最久的对象，甚至是存活于整个系统的生命周期内。\n\n3 上下文管理3.1 什么是with语句\n\nwith是一种上下文管理协议，目的在于从流程图中把 try,except 和finally 关键字和资源分配释放相关代码统统去掉，简化try….except….finlally的处理流程。\n所以使用with处理的对象必须有enter()和exit()这两个方法\nwith通过enter方法初始化（enter方法在语句体执行之前进入运行）\n然后在exit中做善后以及处理异常（exit()方法在语句体执行完毕退出后运行）\n\n\n\n\n\n3.2 with语句使用场景\n\nwith 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行必要的“清理”操作，释放资源\n比如文件使用后自动关闭、线程中锁的自动获取和释放等。\n\n3.3 with处理文件操作的实例\n12345678with open(&#x27;/etc/passwd&#x27;) as f:    for line in f:\t\tprint(line)# 这段代码的作用：打开一个文件，如果一切正常，把文件对象赋值给f，然后用迭代器遍历文件中每一行，当完成时，关闭文件；# 而无论在这段代码的任何地方，如果发生异常，此时文件仍会被关闭。\n\n\n\n4 高阶函数4.1 lambda基本使用\n\nlambda只是一个表达式，函数体比def简单很多。\nlambda的主体是一个表达式，而不是一个代码块。仅仅能在lambda表达式中封装有限的逻辑进去。\nlambda表达式是起到一个函数速写的作用。允许在代码内嵌入一个函数的定义。\n格式：lambda的一般形式是关键字lambda后面跟一个或多个参数，紧跟一个冒号，之后是一个表达式。\n\n12345678f = lambda x,y,z:x+y+zprint(f(1,2,3))                    # 6my_lambda = lambda arg : arg + 1print(my_lambda(10))                # 1112345\n\n4.2 三元运算:\n\n三元运算格式： result=值1 if x&lt;y else 值2 if条件成立result=1,否则result=2\n作用：三元运算，又称三目运算，主要作用是减少代码量，是对简单的条件语句的缩写\n\n12345678name = &#x27;Tom&#x27; if 1 == 1 else &#x27;fly&#x27;print(name)# 运行结果： Tomf = lambda x:x if x % 2 != 0 else x + 100print(f(10))                    # 110\n\nmap()函数用法：map(function, iterable, …)功能：\n\n将第一个参数 function 依次作用在参数可迭代对象中的每一个元素上，返回包含每次 function 函数返回值的新迭代器\nmap() 会根据提供的函数对指定序列做映射。(映射及对应)\n第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表。\n\n参数：function – 函数，有两个参数iterable – 一个或多个可迭代对象（如：序列）\n返回值：Python 3.x 返回迭代器\n1234567891011# demodef func(x):    return x*xret = map(func, [1, 2, 3, 4, 5, 6, 7, 8, 9])print(list(ret))# 运行结果：[1, 4, 9, 16, 25, 36, 49, 64, 81]\n\nreduce()函数语法reduce(function, iterable[, initializer])功能：\n\n函数将一个数据集合（链表，元组等）中的所有数据进行下列操作：用传给 reduce 中的函数 function（有两个参数）先对集合中的第 1、2 个元素进行操作，得到的结果再与第三个数据用 function 函数运算，最后得到一个结果。其效果类似：reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4)\nreduce() 函数会对参数序列中元素进行累积。\n函数将一个数据集合（链表，元组等）中的所有数据进行下列操作：用传给 reduce 中的函数 function（有两个参数）先对集合中的第 1、2 个元素进行操作，得到的结果再与第三个数据用 function 函数运算，最后得到一个结果。\n\n参数：function – 函数，有两个参数iterable – 可迭代对象initializer – 可选，初始参数\n返回值：返回函数计算结果。\n123456789101112131415# demo:from functools import reducedef add(x, y):    return x + y r = reduce(add, [1, 3, 5, 7, 9])print(r)# 1. 运行结果：25\n\nfilter()函数：filter(function, iterable)功能\n\n该接收两个参数，第一个为函数，第二个为序列，序列的每个元素作为参数传递给函数进行判断，然后返回 True 或 False，最后将返回 True 的元素放到新迭代器对象中\nfilter() 函数用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表。\n\n参数：function – 判断函数iterable – 可迭代对象（如：序列）\n返回值：返回一个迭代器对象\n12345678910111213# demo：def is_odd(n):    return n % 2 == 1 tmplist = filter(is_odd, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])newlist = list(tmplist)print(newlist)# 运行结果：[1, 3, 5, 7, 9]\n\nsorted()函数sorted(iterable, key=abs, reverse=False)功能:\n\n对所有可迭代的对象进行排序操作\n\n参数:iterable – 可迭代对象。key – key指定的函数将作用于可迭代对象上的每一个元素，并根据key函数返回的结果进行排序reverse – 排序规则，reverse = True 降序 ， reverse = False 升序（默认）\n返回值:返回重新排序的列表\n123456789101112# demo:print(sorted([36, 5, -12, 9, -21]))运行结果：[-21, -12, 5, 9, 36]print(sorted([36, 5, -12, 9, -21], key=abs))#abs 匿名函数运行结果：[5, 9, -12, -21, 36]\n\n返回函数：高阶函数除了可以接受函数作为参数外，还可以把函数作为结果值返回。\n12345678910111213def lazy_sum(*args):    def sum():        ax = 0        for n in args:            ax = ax + n        return ax    return sum        #将定义的函数sum()作为结果值返回 f = lazy_sum(1, 3, 5, 7, 9)f()\n\n\n\n\n\n4.3 filter()函数可以对序列做过滤处理\n利用 filter、lambda表达式 获取l1中元素小于33的所有元素 l1 = [11, 22, 33, 44, 55]\n123456l1= [11,22,33,44,55]a = filter(lambda x: x&lt;33, l1)print(list(a))\n\n4.4 Map是对序列根据设定条件进行操作后返回他设置的是操作方法利用map，lambda表达式将所有偶数元素加100\n1234567l1= [11,22,33,44,55]ret = map(lambda x:x if x % 2 != 0 else x + 100,l1)print(list(ret))# 运行结果： [11, 122, 33, 144, 55]\n\n4.5 reduce函数\n使用reduce进行求和运算\n\nreduce()函数即为化简函数，它的执行过程为：每一次迭代，都将上一次的迭代结果与下一个元素一同传入二元func函数中去执行。\n在reduce()函数中，init是可选的，如果指定，则作为第一次迭代的第一个元素使用，如果没有指定，就取seq中的第一个元素。\n\n1234567891011121314from functools import reducedef f(x, y):\t return x + yprint(reduce(f, [1, 3, 5, 7, 9]))  # 25# 1、先计算头两个元素：f(1, 3)，结果为4；# 2、再把结果和第3个元素计算：f(4, 5)，结果为9；# 3、再把结果和第4个元素计算：f(9, 7)，结果为16；# 4、再把结果和第5个元素计算：f(16, 9)，结果为25；# 5、由于没有更多的元素了，计算结束，返回结果25。print( reduce(lambda x, y: x + y, [1, 3, 5, 7, 9])  )  # 25\n\n4.6 sorted函数\nsorted对字典排序\n123456d = &#123;&#x27;k1&#x27;:1, &#x27;k3&#x27;: 3, &#x27;k2&#x27;:2&#125;# d.items() = [(&#x27;k1&#x27;, 1), (&#x27;k3&#x27;, 3), (&#x27;k2&#x27;, 2)]a = sorted(d.items(), key=lambda x: x[1])print(a)            # [(&#x27;k1&#x27;, 1), (&#x27;k2&#x27;, 2), (&#x27;k3&#x27;, 3)]\n\n\n\n\nsubprocess模块subprocess是Python 2.4中新增的一个模块，它允许你生成新的进程，连接到它们的 input/output/error 管道，并获取它们的返回（状态）码。这个模块的目的在于替换几个旧的模块和方法，如：\n\nos.system\nos.spawn*\n\nsubprocess模块中的常用函数\nsubprocess.run()Python 3.5中新增的函数。执行指定的命令，等待命令执行完成后返回一个包含执行结果的CompletedProcess类的实例。\nsubprocess.call()执行指定的命令，返回命令执行状态，其功能类似于os.system(cmd)。\nsubprocess.check_call()Python 2.5中新增的函数。 执行指定的命令，如果执行成功则返回状态码，否则抛出异常。其功能等价于subprocess.run(…, check=True)。\nsubprocess.check_output()Python 2.7中新增的的函数。执行指定的命令，如果执行状态码为0则返回命令执行结果，否则抛出异常。\nsubprocess.getoutput(cmd)接收字符串格式的命令，执行命令并返回执行结果，其功能类似于os.popen(cmd).read()和commands.getoutput(cmd)。\nsubprocess.getstatusoutput(cmd)执行cmd命令，返回一个元组(命令执行状态, 命令执行结果输出)，其功能类似于commands.getstatusoutput()。\n\nparamiko模块paramiko是一个用于做远程控制的模块，使用该模块可以对远程服务器进行命令或文件操作,paramiko是用python语言写的一个模块，遵循SSH2协议，支持以加密和认证的方式，进行远程服务器的连接。\n由于使用的是python这样的能够跨平台运行的语言，所以所有python支持的平台，如Linux, Solaris, BSD, MacOS X, Windows等，paramiko都可以支持，因此，如果需要使用SSH从一个平台连接到另外一个平台，进行一系列的操作时，paramiko是最佳工具之一。\npython２和python３的区别：\npython2 解释器默认编码：ascii　　python3 解释器默认编码：utf-8\nrange在Python2中返回列表，而在Python3中返回range可迭代对象。\n在Python2中有两个不等运算符!=和&lt;&gt;，在Python3中去掉了&lt;&gt;，只有!=符号表示不等\n在Python2中long是比int取值范围更大的整数，Python3中取消了long类型，int的取值范围扩大到之前的long类型范围。\npython2 的代码混乱，重复较多，冗余。python3源码规范、清晰、简单优美。\npython3x：unicode 默认是4个字节表示一个字符、python2x :unicode 默认2个字节表示一个字符\n\n","categories":["python"],"tags":["python"]},{"title":"Django + uWSGI + Nginx 的生产环境部署，及WSGI & uwsgi & uWSGI 的作用","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/08/29/Django + uWSGI + Nginx 的生产环境部署，及WSGI & uwsgi & uWSGI 的作用/","content":"uWSGI的 介绍 看这里\nNginx\nNginx是一个Web服务器,其中的HTTP服务器功能和uWSGI功能很类似\n但是Nginx还可以用作更多用途，比如最常用的反向代理、负载均衡、拦截攻击等，而且性能极高\n\nDjango\nDjango是一个Web框架，框架的作用在于处理request和 reponse，其他的不是框架所关心的内容。\n所以如何部署Django不是Django所需要关心的。\n\n\n123456789这里举个小例子：我们的浏览器是 出生在海外的华侨，通过HTTP(飞机) 从 Nginx(日本) 回到  (Python 中国)下飞机 到了之后发现语言不通，急忙的找到了我。他说日语(uwsgi)“こんにちは“我会日语啊 我把这句话 转换成了 汉语(uWSGI)给 别人听这样就可以 在 中国(Python) 让日本人(浏览器) 生活了。但是 博主是东北人 光会转义 日语-&gt;普通话 还不行 还要和我的家人们说东北话(WSGI)此时 就已经非常完美的解决了语言不通的问题了。12345678\n\n\n请求处理整体流程\n\n\nnginx接收到浏览器发送过来的http请求，将包进行解析，分析url\n静态文件请求：就直接访问用户给nginx配置的静态文件目录，直接返回用户请求的静态文件\n动态接口请求：那么nginx就将请求转发给uWSGI，最后到达django处理\n\n\n各模块作用\n\n\nnginx：是对外的服务器，外部浏览器通过url访问nginx，nginx主要处理静态请求\n\nuWSGI：是对内的服务器，主要用来处理动态请求\n\nuwsgi：是一种web协议，接收到请求之后将包进行处理，处理成wsgi可以接受的格式，并发给wsgi\n\nwsgi：是python专用的web协议，根据请求调用应用程序（django）的某个文件，某个文件的某个函数\n\ndjango：是真正干活的，查询数据等资源，把处理的结果再次返回给WSGI， WSGI 将返回值进行打包，打包成uwsgi能够接收的格式\n\nuwsgi接收wsgi发送的请求，并转发给nginx,nginx最终将返回值返回给浏览器\n\n\nDjango + uwsgi方案\n没有nginx而只有uwsgi的服务器，则是Internet请求直接由uwsgi处理，并反馈到web项目中。\nnginx可以实现安全过滤，防DDOS等保护安全的操作，并且如果配置了多台服务器，nginx可以保证服务器的负载相对均衡。\n而uwsgi则是一个web服务器，实现了WSGI协议(Web Server Gateway Interface)，http协议等，它可以接收和处理请求，发出响应等。所以只用uwsgi也是可以的。\n\nnginx和uWSGI特点\nnginx的作用\n\n\n反向代理，可以拦截一些web攻击，保护后端的web服务器\n负载均衡，根据轮询算法，分配请求到多节点web服务器\n缓存静态资源，加快访问速度，释放web服务器的内存占用，专项专用\n\n\nuWSGI的适用\n\n\n单节点服务器的简易部署\n\n轻量级，好部署\n\n\nDjango + Uwsgi + Nginx 的生产环境部署\n在centos 7中安装python3环境1234567891011121314151617181920212223242526272829303132# 1、yum更新yum源yum update# 2、安装Python 3.7所需的依赖否则安装后没有pip3包yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel libffi-devel gcc make# 3、在官网下载所需版本，这里用的是3.7.0版本wget https://www.python.org/ftp/3.7.0/Python-3.7.0.tgz123456# 1、yum更新yum源yum update# 2、安装Python 3.7所需的依赖否则安装后没有pip3包yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel libffi-devel gcc make# 3、在官网下载所需版本，这里用的是3.7.0版本wget https://www.python.org/ftp/3.7.0/Python-3.7.0.tgz　　2、安装Python# 1、解压tar -xvf Python-3.7.0.tgz#2、配置编译cd Python-3.7.0./configure --prefix=/usr/local/python3  # 配置编译的的路径（这里--prefix是指定编译安装的文件夹）./configure --enable-optimizations  # 执行该代码后，会编译安装到 /usr/local/bin/ 下，且不用添加软连接或环境变量make &amp;&amp; make installln -s /usr/local/python3/bin/python3 /usr/bin/python3  # 添加软连接ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3#3、将/usr/local/python3/bin加入PATH[root@linux-node1 testProj]# vim /etc/profile#然后在文件末尾添加export PATH=$PATH:/usr/local/python3/bin[root@linux-node1 testProj]# source /etc/profile # 修改完后，还需要让这个环境变量在配置信息中生效，执行命令\n\n初始化一个django项目12345678910# 初始化一个django项目[root@linux-node1 /] pip3 install django==2.0.4[root@linux-node1 /] mkdir /code/[root@linux-node1 /] cd /code/[root@linux-node1 testProj] django-admin startproject mmcsite[root@linux-node1 testProj] cd /code/mmcsite[root@linux-node1 testProj] python3 manage.py runserver 0.0.0.0:8000页面中访问：http://192.168.56.11:8000/\n\n安装uwsgi 并使用uWSGI启动这个服务123456789101112131415161718192021222324252627282930313233343536&#x27;&#x27;&#x27;1. 安装uwsgi&#x27;&#x27;&#x27;[root@linux-node1 /]# pip3 install uwsgi[root@linux-node1 /]# ln -s /usr/local/python3/bin/uwsgi /usr/bin/uwsgi&#x27;&#x27;&#x27;2. 配置uwsgi.ini启动文件&#x27;&#x27;&#x27;[root@linux-node1 /]# vim uwsgi.ini[uwsgi]socket = 0.0.0.0:3031chdir = /code/mmcsitewsgi-file = /code/mmcsite/wsgi.pyprocesses = 5threads = 30master = truedaemonize = /code/mmcsite/uwsgi.logmodule=mmcsite.wsgipidfile = /code/mmcsite/uwsgi.pidchmod-socket=666enable-threads = true&#x27;&#x27;&#x27;3. 使用uwsgi启动django：一定要在这个项目目录中&#x27;&#x27;&#x27;[root@linux-node1 /]# uwsgi --http 192.168.56.11:80 --file mmcsite/wsgi.py --static-map=/static=static访问项目：http://192.168.56.11[root@linux-node2 demo2]# vim /code/mmcsite/uwsgi.ini  # uwsgi.ini文件[uwsgi]socket = 0.0.0.0:3031                  # 指定socket监听的地址和端口chdir = /code/mmcsite                  # 项目路径 wsgi-file = /code/mmcsite/wsgi.py      # django的wsgi文件路径processes = 5                          # 启动五个进程threads = 30                           # 每个进程启动30个线程master = truedaemonize = /code/mmcsite/uwsgi.log    # 日志存放路径module=mmcsite.wsgi                    # 使用mmcsite.wsgi模块pidfile = /code/mmcsite/uwsgi.pid      # uwsgi启动进程id存放路径chmod-socket=666                       # socket权限enable-threads = true                  # 允许用内嵌的语言启动线程，这将允许你在app程序中产生一个子线程\n\n安装配置nginx123&#x27;&#x27;&#x27;1. 配置nginx YUM源&#x27;&#x27;&#x27;[root@linux-node1 /] vim /etc/yum.repos.d/nginx.repo12\n\n[nginx]name=nginx repo\n下面这行centos根据你自己的操作系统修改比如：OS/rehel6是你Linux系统的版本，可以通过URL查看路径是否正确baseurl=http://nginx.org/packages/centos/7/$basearch/gpgcheck=0enabled=1\n1234567891011121314151617181920212223242526272829303132&#39;&#39;&#39;2. 安装nginx&#39;&#39;&#39;[root@linux-node1 &#x2F;] yum -y install nginx安装nginx1234[root@linux-node1 &#x2F;]# vim &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;django.conf server &#123;    listen       8888;    server_name  192.168.56.11;    client_max_body_size 5M;    gzip on;    gzip_buffers 32 4K;#压缩在内存中缓冲32块 每块4K    gzip_comp_level 6 ;#压缩级别 推荐6    gzip_min_length 4000;#开始压缩的最小长度4bit        gzip_types text&#x2F;plain application&#x2F;json application&#x2F;javascript application&#x2F;x-javascript application&#x2F;css application&#x2F;xml application&#x2F;xml+rss text&#x2F;javascript application&#x2F;x-httpd-php image&#x2F;jpeg image&#x2F;gif image&#x2F;png image&#x2F;x-ms-bmp;        location &#x2F; &#123;              include uwsgi_params;              uwsgi_pass 127.0.0.1:3031;              uwsgi_ignore_client_abort on;        &#125;        error_page 404 &#x2F;404.html;            location &#x3D; &#x2F;40x.html &#123;        &#125;        error_page 500 502 503 504 &#x2F;50x.html;            location &#x3D; &#x2F;50x.html &#123;        &#125;&#125;配置nginx\n\n启动项目1234[root@linux-node1 demo2]# systemctl restart nginx   # 开启nginx[root@linux-node1 demo2]# uwsgi --ini uwsgi.ini     # 启动uwsgi的django项目# http://192.168.56.11:8888/ 访问项目[root@linux-node1 demo2]# uwsgi --stop uwsgi.pid    # 关闭uwsgi","categories":["docker"],"tags":["python"]},{"title":"Docker 的基本常用命令","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/08/27/Docker 的基本常用命令/","content":"\ndocker 镜像管理常用命令\n\n\n\n命令\n示意\n\n\n\ndocker help\n查看docker帮助\n\n\ndocker image –help\n查看 docker中 镜像相关帮助\n\n\ndocker image ls\n查看当前所有镜像\n\n\ndocker image inspect nginx\n查看指定镜像（nginx镜像）详细信息\n\n\ndocker pull nginx:1.14\n下载指定版本镜像 nginx\n\n\ndocker image rm nginx:1.14\n删除nginx 1.14版本\n\n\ndocker image save nginx &gt; nginx.tar\n导出niginx镜像\n\n\ndocker创建容器常用命令\ndocker run 常用参数\n\n\n\n命令\n示意\n\n\n\n-d:\n后台运行容器，并返回容器ID；\n\n\n-i:\n以交互模式运行容器，通常与 -t 同时使用；\n\n\n-t:\n为容器重新分配一个伪输入终端，通常与 -i 同时使用；\n\n\n-P:\n随机端口映射，容器内部端口随机映射到主机的高端口\n\n\n-p:\n指定端口映射，格式为：主机(宿主)端口:容器端口\n\n\n–name=“nginx-lb”:\n为容器指定一个名称；\n\n\n–dns 8.8.8.8:\n指定容器使用的DNS服务器，默认和宿主一致；\n\n\ndocker run 其他参数\n\n\n\n命令\n示意\n\n\n\n–dns-search example.com:\n指定容器DNS搜索域名，默认和宿主一致；\n\n\n-h “mars”:\n指定容器的hostname；\n\n\n-e username=“ritchie”:\n设置环境变量；\n\n\n–env-file=[]:\n从指定文件读入环境变量；\n\n\n–cpuset=“0-2” or –cpuset=“0,1,2”:\n绑定容器到指定CPU运行；\n\n\n-m :\n设置容器使用内存最大值；\n\n\n–net=“bridge”:\n指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型；\n\n\n–link=[]:\n添加链接到另一个容器；\n\n\n–expose=[]:\n开放一个端口或一组端口；\n\n\n–volume , -v:\n绑定一个卷\n\n\n-a stdin:\n指定标准输入输出内容类型，可选 STDIN/STDOUT/STDERR 三项；\n\n\ndocker run\n其他参数\n\n\n\n\n\n命令\n示意\n\n\n\ndocker run –help\n查看创建容器帮助\n\n\ndocker run -it centos\n创建centos镜像并进入终端\n\n\ndocker run -d nginx\n后台启动nginx容器\n\n\ndocker stop 6bb09dce461f\n关闭一个容器\n\n\ndocker ps -l\n查看最近运行的容器\n\n\ndocker run -itd centos\n启用一个伪终端守护centos容器\n\n\ndocker container run -d –name web3 -e test=123456 -p 8800:80 -h webhostname –restart always nginx\n\n\n\n-d\n后台启动nginx容器\n\n\n–name web3\n自定义容器名字(默认会是一段随机字符串)\n\n\n-e test=123456\n启动容器添加变量 test=123456 (echo $test)\n\n\n-p 8800:80\n宿主机的8800端口映射到docker容器的80端口中\n\n\n-h webhostname\ndocker容器主机名 (a300f394af88)\n\n\n–restart always\n宿主机重启自动拉起这个docker容器\n\n\nnginx\n使用这个nginx镜像启动容器\n\n\ndocker logs web\n查看上面启动的web容器的日志\n\n\ndocker exec -it web bash\n进入容器web\n\n\n容器资源限制\n内存限额： 允许容器最多使用500M内存和100M的Swap，并禁用 OOM Killerdocker run -d –name nginx03 –memory=“500m” –memory-swap=“600m” –oom-kill-disable nginx\n\nCPU限额：docker run -d –name nginx04 –cpus=“1.5” nginx # 允许容器最多使用一个半的CPUdocker run -d –name nginx05 –cpus=”.5” nginx # 允许容器最多使用50%的CPU\n\n\ndocker 管理 容器常用命令\n\n\n\n命令\n示意\n\n\n\ndocker ps\n仅列出当前运行的容器\n\n\ndocker ps -l\n列出最新创建得容器\n\n\ndocker ps -a\n列出素有容器(包括 未运行的)\n\n\ndocker inspect web4\n列出指定容器的详细信息\n\n\n持久化容器\n\n\n\ndocker exec -it web4 bash\n进入容器web4中\n\n\n\ntouch 1.txt 2.txt\n对容器进行修改\n\n\ndocker commit web4 nginx:web4\n将修改后的web4容器提交为一个新镜像 nginx:web4\n\n\ndocker images\n可以看到 多了一个 TAG标记为 web4 的镜像\n\n\ndocker run -d –name web4-1 nginx:web4\n使用刚刚提交的镜像web4创建一个容器web4-1\n\n\ndocker exec -it web4-1 bash\n进入web4-1的bash环境\n\n\n从宿主机复制文件到docker容器\n\n\n\ndocker cp nginx.tar web4-1:/home\n将宿主机nginx.tar文件拷贝到容器web4-1的/home目录中\n\n\n\ndocker exec -it web4-1 ls /home\n在容器web4-1中执行 “ls /home” 命令\n\n\n容器常用查询命令\n\n\n\ndocker logs web4-1\n查看web4-1中控制台日志\n\n\n\ndocker port 55f870061ed9\n查看指定容器端口映射\n\n\ndocker top 00f7ddc96622\n查看容器中有哪些进程\n\n\ndocker stats 00f7ddc96622\n查看容器资源使用情况\n\n\n启动、停止、删除 容器\n\n\n\ndocker ps -a\n列出素有容器(包括 未运行的)\n\n\n\ndocker start web\n启动容器web\n\n\ndocker stop web\n停止容器web\n\n\ndocker rm web\n删除容器\n\n\n","categories":["docker"],"tags":["python"]},{"title":"Docker 镜像 & 容器和镜像的联系 读写层","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/08/25/Docker 镜像 & 容器和镜像的联系 读写层/","content":"\nDocker 镜像\n镜像是什么？\n一个分层存储的文件\n一个软件的环境\n一个镜像可以创建N个容器\n一个标准化的交付\n一个不包含Linux内核而又精简的Linux操作系统\n\n人性化一点 ↓\n\ndocker镜像不包含Linux内核而又精简的Linux操作系统\ndocker镜像是一个分层存储的文件，一个镜像可以创建N个容器\n可以这么理解，docker 镜像是 docker 容器的静态视角，docker 容器是 docker 镜像的运行状态。\n容器只是对docker镜像的引用，如果docker镜像删除，此镜像创建的容器也都失效\n\n镜像不是一个单一的文件，而是有多层结构。我们可以通过 docker history &lt;ID/NAME&gt; 查看镜像中各层内容及大小，每层对应着DOckerfile 中的一条命令。Docker镜像默认存储在 /var/lib/docker/中。\n是在驱动中 所以说目录下的文件都有可能 最佳推荐：overlay2\n123docker info  查看Storage Driver: overlay2\n\n镜像从哪里来的？Docker Hub 是由 Docker公司负责维护公共注册中心，包含大量的容器镜像，Docker工具默认从这个公共镜像库下载镜像地址: https://hub.docker.com/explore\n我们看一下 镜像默认源 docker info拿到了地址 我们 ping 一下amazonaws:亚马逊服务器也可以浏览器 访问一下端口\n配置镜像加速器https://www.daocloud/mirror\n12curl -sSL https://get.daocloud.io/daotools/set_mirror.sh |sh -s http://f1361db2.m.daocloud.io1\n\n我们可以通过 命令查看 镜像加速的地址\n12345cat /etc/docker/daemon.json# 结果&#123;&quot;registry-mirrors&quot;: [&quot;http://f1361db2.m.daocloud.io&quot;]&#125;1234\n\n配置完 重启docker\n12systemctl restart docker1\n\n\n\nDocker 镜像和容器的联系、区别\n图中可以看出\n\n容器其实是在镜像的最上面加了一层读写层，在运行容器里文件改动时，会先从镜像里要写的文件复制到容器自己的文件系统中（读写层）。\n如果容器删除了，最上面的读写层也就删除了，改动也就丢失了。\n所以无论多少个容器共享一个镜像，所做的写操作都是从镜像的文件系统中复制（引用）过来的操作的，并不会修改镜像的源文件，这种方式提高磁盘利用率。\n若想持久化这些改动，可以通过 docker commit 将容器保存成一个新的镜像。\n\n举例：123456# 创建 nginx容器docker run -itd nginx# 查看docker ps12345\n\n我们访问这个 容器的ID\n12345# 进入镜像docker exec -it 5ceaf43bd114 bashls1234\n\n可以看出这个是和 镜像一样的。由此证出，docker 容器只是对 镜像进行了引用\n***读写层的意思就是说，\n\n镜像会将所有数据拷贝到读写层，然后进行操作\n\n你在容器中 进行写入东西的时候 不管你写入多少，是对 引用的镜像是无反应的。***\n\n\ndocker 容器和镜像的具体区别\n当由 ubuntu:14.04 镜像启动容器时，ubuntu:14.04 镜像的镜像层内容将作为容器的 rootfs；\n而 ubuntu:14.04 镜像的 json 文件，会由 docker daemon 解析，并提取出其中的容器执行入口 CMD 信息，以及容器进程的环境变量 ENV 信息，最终初始化容器进程。\n当然，容器进程的执行入口来源于镜像提供的 rootfs。\n\n\nrootfs\nrootfs 是 docker 容器在启动时内部进程可见的文件系统，即 docker 容器的根目录。\nrootfs 通常包含一个操作系统运行所需的文件系统，例如可能包含典型的类 Unix 操作系统中的目录系统，如 /dev、/proc、/bin、/etc、/lib、/usr、/tmp 及运行 docker 容器所需的配置文件、工具等。\n在传统的 Linux 操作系统内核启动时，首先挂载一个只读的 rootfs，当系统检测其完整性之后，再将其切换为读写模式。\n而在 docker 架构中，当 docker daemon 为 docker 容器挂载 rootfs 时，沿用了 Linux 内核启动时的做法，即将 rootfs 设为只读模式。\n在挂载完毕之后，利用联合挂载(union mount)技术在已有的只读 rootfs 上再挂载一个读写层。\n这样，可读写的层处于 docker 容器文件系统的最顶层，其下可能联合挂载了多个只读的层，\n只有在 docker 容器运行过程中文件系统发生变化时，才会把变化的文件内容写到可读写层，并隐藏只读层中的旧版本文件。\n\n查看容器中镜像内容123456# 创建web1 容器docker run -itd --name=web1 -p 192.168.56.14:81:80 nginx:latest# 查看容器内容docker inspect web1 | more\n\n以下就是容器的详细信息\n1234567891011121314151617181920212223242526272829303132# 只摘选局部[    &#123;        &quot;GraphDriver&quot;: &#123;            &quot;Data&quot;: &#123;                &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/51692869029f819494bb402dc70aa2869b8d1848c3b65c549a010a260e44cc13-init/diff:/var/lib/docker/overlay2/09fa1314e484781dfc1fb25a6cf5df2502fe35dea9025a373a3cb0202732ccce/diff:/var/lib/docker/overlay2/162ec5c9be56e5d718011c09ed087eda04b755e1a68bd1953c60f175e6635e68/diff:/var/lib/docker/overlay2/7c1b27ff59a397ae7d6bd106db579e90476f57bb1ecef9fcb1a6f1ad5ce43b7c/diff&quot;,                &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/51692869029f819494bb402dc70aa2869b8d1848c3b65c549a010a260e44cc13/merged&quot;,                &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/51692869029f819494bb402dc70aa2869b8d1848c3b65c549a010a260e44cc13/diff&quot;,                &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/51692869029f819494bb402dc70aa2869b8d1848c3b65c549a010a260e44cc13/work&quot;            &#125;,            &quot;Name&quot;: &quot;overlay2&quot;        &#125;,        &quot;NetworkSettings&quot;: &#123;            &quot;Ports&quot;: &#123;                &quot;80/tcp&quot;: [                    &#123;                        &quot;HostIp&quot;: &quot;192.168.56.14&quot;,                        &quot;HostPort&quot;: &quot;81&quot;                    &#125;                ]            &#125;,            &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;,            &quot;Networks&quot;: &#123;                &quot;bridge&quot;: &#123;                    &quot;Gateway&quot;: &quot;172.17.0.1&quot;,                    &quot;IPAddress&quot;: &quot;172.17.0.2&quot;,                    &quot;IPPrefixLen&quot;: 16,                &#125;            &#125;        &#125;    &#125;]\n\n可以看到 读写层（自己 吧下面得数字 Ctrl + F 看吧）\n123456[root@linux-node4 diff] cd /var/lib/docker/overlay2/51692869029f819494bb402dc70aa2869b8d1848c3b65c549a010a260e44cc13[root@linux-node4 51692869029f819494bb402dc70aa2869b8d1848c3b65c549a010a260e44cc13]# ls# 读写层的文件 diff  link  lower  merged  work\n\nwork 是工作的地方diff 是镜像和读写层的差异merged 是镜像引用下来的 读写层\n","categories":["docker"],"tags":["python"]},{"title":"Nginx 配置文件","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/08/23/Nginx 配置文件/","content":"\nNginx 配置\nnginx配置文件注释123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125#运行用户user nobody;#启动进程,通常设置成和cpu的数量相等worker_processes  1; #全局错误日志及PID文件#error_log  logs/error.log;#error_log  logs/error.log  notice;#error_log  logs/error.log  info; #pid        logs/nginx.pid; #工作模式及连接数上限events &#123;    #epoll是多路复用IO(I/O Multiplexing)中的一种方式,    #仅用于linux2.6以上内核,可以大大提高nginx的性能    use   epoll;      #单个后台worker process进程的最大并发链接数        worker_connections  1024;     # 并发总数是 worker_processes 和 worker_connections 的乘积    # 即 max_clients = worker_processes * worker_connections    # 在设置了反向代理的情况下，max_clients = worker_processes * worker_connections / 4  为什么    # 为什么上面反向代理要除以4，应该说是一个经验值    # 根据以上条件，正常情况下的Nginx Server可以应付的最大连接数为：4 * 8000 = 32000    # worker_connections 值的设置跟物理内存大小有关    # 因为并发受IO约束，max_clients的值须小于系统可以打开的最大文件数    # 而系统可以打开的最大文件数和内存大小成正比，一般1GB内存的机器上可以打开的文件数大约是10万左右    # 我们来看看360M内存的VPS可以打开的文件句柄数是多少：    # $ cat /proc/sys/fs/file-max    # 输出 34336    # 32000 &lt; 34336，即并发连接总数小于系统可以打开的文件句柄总数，这样就在操作系统可以承受的范围之内    # 所以，worker_connections 的值需根据 worker_processes 进程数目和系统可以打开的最大文件总数进行适当地进行设置    # 使得并发总数小于操作系统可以打开的最大文件数目    # 其实质也就是根据主机的物理CPU和内存进行配置    # 当然，理论上的并发总数可能会和实际有所偏差，因为主机还有其他的工作进程需要消耗系统资源。    # ulimit -SHn 65535 &#125;  http &#123;    #设定mime类型,类型由mime.type文件定义    include    mime.types;    default_type  application/octet-stream;    #设定日志格式    log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;                      &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;                      &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;     access_log  logs/access.log  main;     #sendfile 指令指定 nginx 是否调用 sendfile 函数（zero copy 方式）来输出文件，    #对于普通应用，必须设为 on,    #如果用来进行下载等应用磁盘IO重负载应用，可设置为 off，    #以平衡磁盘与网络I/O处理速度，降低系统的uptime.    sendfile     on;    #tcp_nopush     on;     #连接超时时间    #keepalive_timeout  0;    keepalive_timeout  65;    tcp_nodelay     on;     #开启gzip压缩    gzip  on;    gzip_disable &quot;MSIE [1-6].&quot;;     #设定请求缓冲    client_header_buffer_size    128k;    large_client_header_buffers  4 128k;      #设定虚拟主机配置    server &#123;        #侦听80端口        listen    80;        #定义使用 www.nginx.cn访问        server_name  www.nginx.cn;         #定义服务器的默认网站根目录位置        root html;         #设定本虚拟主机的访问日志        access_log  logs/nginx.access.log  main;         #默认请求        location / &#123;                        #定义首页索引文件的名称            index index.php index.html index.htm;            &#125;         # 定义错误提示页面        error_page   500 502 503 504 /50x.html;        location = /50x.html &#123;        &#125;         #静态文件，nginx自己处理        location ~ ^/(images|javascript|js|css|flash|media|static)/ &#123;                        #过期30天，静态文件不怎么更新，过期可以设大一点，            #如果频繁更新，则可以设置得小一点。            expires 30d;        &#125;         #PHP 脚本请求全部转发到 FastCGI处理. 使用FastCGI默认配置.        location ~ .php$ &#123;            fastcgi_pass 127.0.0.1:9000;            fastcgi_index index.php;            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;            include fastcgi_params;        &#125;         #禁止访问 .htxxx 文件            location ~ /.ht &#123;            deny all;        &#125;     &#125;&#125;\n\n\n\nnginx配置举例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697user  work;worker_processes 8;worker_rlimit_nofile 65535;error_log  logs/error.log warn;#error_log  logs/error.log  notice;#error_log  logs/error.log  info;pid        logs/nginx.pid;events &#123;    use epoll;    worker_connections  65535;&#125;# load modules compiled as Dynamic Shared Object (DSO)##dso &#123;#    load ngx_http_fastcgi_module.so;#    load ngx_http_rewrite_module.so;#&#125;http &#123;    include       mime.types;    default_type  application/octet-stream;    server_names_hash_bucket_size 128;    sendfile        on;    tcp_nopush      on;    tcp_nodelay     on;    fastcgi_connect_timeout 5;    fastcgi_send_timeout 10;    fastcgi_read_timeout 10;    fastcgi_buffer_size 64k;    fastcgi_buffers 4 64k;    fastcgi_busy_buffers_size 128k;    fastcgi_temp_file_write_size 128k;    #keepalive_timeout  0;    keepalive_timeout  60;    keepalive_requests 1024;    client_header_buffer_size 4k;    large_client_header_buffers 4 32k;    client_max_body_size 10m;    client_body_buffer_size 512k;    client_body_timeout 600;    client_header_timeout 600;    send_timeout 600;    proxy_connect_timeout   1000ms;    proxy_send_timeout      2000000ms;    proxy_read_timeout      2000000ms;    proxy_buffers           64 8k;    proxy_busy_buffers_size    128k;    proxy_temp_file_write_size 64k;    proxy_redirect off;    #proxy_next_upstream off ;    gzip on;    gzip_min_length 1k;    gzip_buffers 4 16k;    gzip_http_version 1.0;    gzip_comp_level 2;    gzip_types text/plain application/x-javascript text/css application/xml;    gzip_vary on;    add_header X-Frame-Options &quot;ALLOW-FROM  http://cloud.njsig.cn&quot;;    proxy_set_header X-Real-IP $remote_addr;    proxy_set_header X-Real-Port $remote_port;    proxy_set_header Host $host;    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;    log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;        &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_cookie&quot; &quot;$http_user_agent&quot; &#x27;        &#x27;$request_time $remote_addr $server_addr $upstream_addr $host &#x27;        &#x27;&quot;$http_x_forwarded_for&quot; $upstream_response_time&#x27;;    set_real_ip_from 10.0.0.0/8;    real_ip_header X-Real-IP;    #example#     server &#123;#        listen 8000;#        server_name www;#        access_log logs/access.log  main;#        location / &#123;#            proxy_pass http://127.0.0.1:8001;#        &#125;###      &#125;    include vhosts/*.conf;    &#125;#####\n\n\n\nnginx/conf/vhosts/opwf.conf django项目简单配置12345678910server &#123;        listen 80;        server_name aaa.test.com bbb.test.com;        access_log  /home/work/nginx/logs/opwf_access.log main;        location / &#123;            proxy_pass http://127.0.0.1:8001;        &#125;&#125;\n\n\n\nnginx/conf/vhosts/opwf.conf django项目简单配置123456789101112server &#123;        listen 80;        server_name ccc.test.com;        access_log  /home/work/nginx/logs/nj1_access.log main;        root /home/work/project/frontopwf/dist;        location / &#123;                try_files $uri $uri/ @router;        &#125;        location @router &#123;                rewrite ^.*$ /index.html last;        &#125;&#125;","categories":["nginx"],"tags":["python"]},{"title":"Docker 介绍，Linux简单安装","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/08/18/Docker 介绍，Linux简单安装/","content":"\n什么是 Docker？\n\n一个简单的应用程序打包工具\n使用最广泛的开源容器\n一种操作系统级的虚拟化技术\n依赖于Linux内核特性：Namespace 和 Cgroups\n\n\nDocker 是应用最广泛的开源容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中\n然后发布到任何流行的 Linux或Windows 机器上，也可以实现虚拟化。\n每个容器拥有一套和宿主机完全隔离的文件系统（共用linux内核），程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。\n\n容器：\n容器是一个操作系统级别下的虚拟化技术，运行一个容器就行运行一个进程一样\n\n容器依赖linux内核特性：Namespace（资源隔离）和Cgroups（资源限制）\n\n\nDocker的设计目标\n\n提供了简单的应用程序打包\n\n开发人员和运维人员职责逻辑分离\n\n多环境保持一致性\n\n\nDocker的思想\n\nDocker的思想源于集装箱，集装箱解决了什么问题呢？\n\n在早期运输货物需要不同分类的船，例如运输水果的船，运输生活用品的船\n\n有了集装箱后，在大船上，可以把货物分类到不同的集装箱中，水果一个集装箱，生活用品一个集装箱\n\n它们之间互不影响，只要把货物封装好集装箱里，就可以把不同类的货物一起运走。\n\n通过Docker logo也可以看出所以然来，Docker就像大船，集装箱就是容器。\n\n一条鲸鱼拖着若干个集装箱的经典形象已经深入人心。\n\n\nDocker的基本组成\n\n\n\n\nDockerClient\n客户端\n\n\n\nDocker Daemon\n守护进程\n\n\nDocker Images\n镜像\n\n\nDocker Container\n容器\n\n\nDocker Registry\n镜像仓库\n\n\n虚拟机 和 容器\n\nVM\n\n\n\nInfrastructure\n代表他 可以是 台式 笔记本 服务器 云主机\n\n\n\nHypervisor\n运行在物理设备上 和 操作糸统之间的 中间软件管理层 ，也叫 宿主机\n\n\nGuest OS\n虚拟出的系统\n\n\nBins/Libx\n二进制文件 / 库\n\n\nApp\n部署的应用\n\n\nContainer\n\n\n\nInfrastructure\n代表他 可以是 台式 笔记本 服务器 云主机\n\n\n\nHost OS\n主机OS，这台机器的操作系统\n\n\nDocker\n系统级别的虚拟化技术，所以要基于已有的操作系统之上\n\n\nBins/Libx\n二进制文件 / 库\n\n\nApp\n部署的应用\n\n\n\n由上图可看 容器呢 只包含了 二进制文件 和 库、应用程序，而虚拟机是 Guest OS 承载的 二进制文件 和 库、应用程序\n对比\n\n\n\nContainer\nVM\n\n\n\n启动速度\n秒级\n分钟级\n\n\n运动性能\n接近原生\n5%左右损失\n\n\n磁盘占用\nMB\nGB\n\n\n数量\n成百上千\n一般几十台\n\n\n隔离性\n进程级别\n系统级（更彻底）\n\n\n操作系统\n只支持Linux\n几乎所有\n\n\n封装程度\n只打包项目代码和依赖关系，共享宿主机内核\n完整的操作系统\n\n\n解答：\ndocker设计小巧，部署迁移快速，运行高效，按照应用隔离，管理人员可以看到所有容器的内容。\n虚拟化技术比较臃肿，需要先创建新的系统，按照系统隔离，管理员无法看到系统内部信息。\n\n举例：\nDocker就是手机中的各种APP，只需要一个系统就可以下载自己所需的应用\n\n虚拟化技术相当于苹果手机安装一个庞大软件，这个软件上安装安卓系统、魅族系统等，每个系统上还要安装各类应用。\n\n\nDocker 的应用场景\n\n应用程序打包和发布\n应用程序隔离\n持续集成\n部署微服务\n快速搭建测试环境\n提供Paas产品（平台即服务）\n\n详解：\n节省项目环境部署时间\n\n单项目打包\n\n\n每次部署项目到测试、生产等环境，都要部署一大堆依赖的软件、工具，时间久，出错概率大。\nDocker主要理念就是环境打包部署，可在任意Docker Engine运行。\n我们只需要将每个项目环境打包到镜像，push到镜像仓库，当有需要部署这个项目时，直接pull镜像启动容器，这个项目就可以访问了！一次构建多次部署，一劳永逸。\n\n\n整套项目打包\n\n\n比如有一个产品可以整套部署到客户那里，以往都是派一名实施工程师到客户那部署。\n如果用了Docker，我们可以前期将这套项目封装打包起来，实现一键部署，分分钟钟搞定，就不需要再派人过去了。比如官方的Docker Compose编排工具。\n\n\n新开源技术试用\n\n\n有时，我们想调研一些开源项目，我们可以直接从公共镜像仓库pull项目官方做好镜像启动容器即可。\n\n\n环境一致性\n\n\n\n项目在开发电脑本地运行没问题，到了测试或生产环境就运行不起来。\nDocker将项目环境打包成镜像，可以在任何Docker Engine部署。\n\n\n持续集成\n\n\n一个项目版本快速迭代的测试场景，需要一个合理的CI（持续集成）/CD（持续部署）环境支撑。\nCI/CD是一个周期性自动化项目测试流程，包括构建、部署、测试、发布等工作，很少需要人工干预。\nDocker通过项目镜像构建和快速部署，打通测试环境与生产环境，高度保持多个环境之间一致性。\n\n\n微服务\n\n\n微服务指尽可能细粒度拆分业务程序架构，由多个独立服务组成业务系统。\n\nDocker容器作为这些独立服务的部署单元，每个服务单独部署到一个docker容器中。\n\n\nLinux 上安装 Docker\ndocker版本1docker V1.13 版本的时候区分的 以下两个版本，大约是在17年初，我们用的是 最新版 V18.03\n\n\n社区版（Community Edition, CE）\n企业版（Enterprise Edition, EE）\n\ndocker安装参考官方文档\ndocker官方文档：https://docs.docker.com/\ncentos安装docker：https://docs.docker.com/install/linux/docker-ce/centos/注：docker CE只支持 centos7 不支持centos6\n\n\n123456789101112131415161718192021# 1.安装依赖包yum install -y yum-utils device-mapper-persistent-data lvm2# 2.添加Docker软件包源(否则doker安装的不是新版本)yum-config-manager \\--add-repo \\https://download.docker.com/linux/centos/docker-ce.repo# 3.安装Docker CEyum install -y docker-ce# 4.启动Docker服务并设置开机启动systemctl start dockersystemctl enable docker# 5.测试docker是否安装成功（hello-world是官方提供的一个测试镜像）docker run hello-world# 6.查看docker基本信息docker infodocker version\n\ndocker简单使用 创建Nginx容器12345678910111213# 1、创建一个nginx容器docker run -it nginx # 2、查看docker运行的容器(可以获取到这个容器的id)docker ps# 3、访问这个容器# 进入这个nginx容器（进入的文件系统和宿主机是完全隔离的，有自己独立的文件系统）docker exec -it 73877e65c07d bash# 4、查看当前容器的 IPdocker inspect 73877e65c07d   # 73877e65c07d是通过docekr ps查看到的容器IDcurl 172.17.0.2               # 测试这个nginx容器是否可以访问","categories":["docker"],"tags":["python"]},{"title":"Python Django 支付宝 扫码支付","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/08/12/Python Django 支付宝 扫码支付/","content":"**安装python-alipay-sdk*\\***\n1pip install python-alipay-sdk --upgrade\n\n\n\n\\原理介绍：**\n　　1.进行秘钥配置，因为传输的数据必须要进行签名加密，ubuntu内置命令openssl可以生成私钥，根据私钥生成公钥\n1234　　openssl　　OpenSSL&gt; genrsa -out app_private_key.pem   2048  # 私钥 2048对应的是rsa加密时候的复杂程度，即rsa2　　OpenSSL&gt; rsa -in app_private_key.pem -pubout -out app_public_key.pem # 导出公钥　　OpenSSL&gt; exit\n\n　　2.cat app_publict_key.pem 查看公钥的内容\n　　将—–BEGIN PUBLIC KEY—–和—–END PUBLIC KEY—–中间的内容保存在支付宝的用户配置中（沙箱或者正式）\n​    https://openhome.alipay.com/platform/appDaily.htm?tab=info\n　　3.配置好公钥后，支付宝会生成公钥，将公钥的内容复制保存到一个文本文件中(alipay_pubilc_key.pem)，注意需要在文本的首尾添加标记位(—–BEGIN 　　PUBLIC KEY—–和—–END PUBLIC KEY—–) \n　　4.将刚刚生成的私钥app_private_key.pem和支付宝公钥alipay_public_key.pem放到我们的项目目录中\n　　5.使用支付宝 python包的初始化\n　　6.调用支付接口\n　　https://docs.open.alipay.com/270/alipay.trade.page.pay/\n　　7.获取支付结果接口\n　　https://docs.open.alipay.com/api_1/alipay.trade.query\n\\代码部分：**\n1.整个项目架构\n\nindex.html代码\n12345678910111213141516171819202122232425262728293031323334&lt;!DOCTYPE html&gt;&lt;html lang&#x3D;&quot;en&quot;&gt;&lt;head&gt;    &lt;meta charset&#x3D;&quot;UTF-8&quot;&gt;    &lt;title&gt;Title&lt;&#x2F;title&gt;    &lt;script src&#x3D;&quot;static&#x2F;js&#x2F;jquery-1.4.2.min.js&quot;&gt;&lt;&#x2F;script&gt;    &lt;script&gt;        $(function () &#123;            $(&#39;#btn&#39;).click(function () &#123;                var order_id &#x3D; &quot;20180105002&quot;;                var req_data &#x3D; &#123;                    order_id: order_id,                    csrfmiddlewaretoken: &quot;&#123;&#123; csrf_token &#125;&#125;&quot;                &#125;;                $.post(&quot;&#x2F;pay&#x2F;&quot;, req_data, function (data) &#123;                    window.open(data.url)                &#125;);                $.get(&quot;&#x2F;check_pay&#x2F;?order_id&#x3D;&quot; + order_id, function (data) &#123;                    if (0 &#x3D;&#x3D; data.code) &#123;                        &#x2F;&#x2F; 支付成功                        alert(&quot;支付成功&quot;);                        location.reload();                    &#125; else &#123;                        alert(data.message)                    &#125;                &#125;)            &#125;)        &#125;)    &lt;&#x2F;script&gt;&lt;&#x2F;head&gt;&lt;body&gt;&lt;input type&#x3D;&quot;button&quot; id&#x3D;&quot;btn&quot; value&#x3D;&quot;支付&quot;&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt;\n\n2.AppTest.views.py代码\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#coding:utf-8from django.shortcuts import renderfrom django.shortcuts import renderfrom django.http import JsonResponsefrom alipay import AliPayimport osfrom django.conf import settingsdef index(request):    return render(request, &quot;index.html&quot;,locals())def pay(request):    order_id &#x3D; request.POST.get(&quot;order_id&quot;)    # 创建用于进行支付宝支付的工具对象    alipay &#x3D; AliPay(        appid&#x3D;settings.ALIPAY_APPID,        app_notify_url&#x3D;None,  # 默认回调url        app_private_key_path&#x3D;os.path.join(settings.BASE_DIR, &quot;AppTest&#x2F;app_private_key.pem&quot;),        alipay_public_key_path&#x3D;os.path.join(settings.BASE_DIR, &quot;AppTest&#x2F;alipay_public_key.pem&quot;),        # 支付宝的公钥，验证支付宝回传消息使用，不是你自己的公钥,        sign_type&#x3D;&quot;RSA2&quot;,  # RSA 或者 RSA2        debug&#x3D;True  # 默认False  配合沙箱模式使用    )    # 电脑网站支付，需要跳转到https:&#x2F;&#x2F;openapi.alipay.com&#x2F;gateway.do? + order_string    order_string &#x3D; alipay.api_alipay_trade_page_pay(        out_trade_no&#x3D;order_id,        total_amount&#x3D;str(0.01),  # 将Decimal类型转换为字符串交给支付宝        subject&#x3D;&quot;测试订单&quot;,        return_url&#x3D;&quot;https:&#x2F;&#x2F;example.com&quot;,        notify_url&#x3D;&quot;https:&#x2F;&#x2F;example.com&#x2F;notify&quot;  # 可选, 不填则使用默认notify url    )    # 让用户进行支付的支付宝页面网址    url &#x3D; settings.ALIPAY_URL + &quot;?&quot; + order_string    return JsonResponse(&#123;&quot;code&quot;: 0, &quot;message&quot;: &quot;请求支付成功&quot;, &quot;url&quot;: url&#125;)def check_pay(request):    # 创建用于进行支付宝支付的工具对象    order_id &#x3D; request.GET.get(&quot;order_id&quot;)    alipay &#x3D; AliPay(        appid&#x3D;settings.ALIPAY_APPID,        app_notify_url&#x3D;None,  # 默认回调url        app_private_key_path&#x3D;os.path.join(settings.BASE_DIR, &quot;AppTest&#x2F;app_private_key.pem&quot;),        alipay_public_key_path&#x3D;os.path.join(settings.BASE_DIR, &quot;AppTest&#x2F;alipay_public_key.pem&quot;),        # 支付宝的公钥，验证支付宝回传消息使用，不是你自己的公钥,        sign_type&#x3D;&quot;RSA2&quot;,  # RSA2,官方推荐，配置公钥的时候能看到        debug&#x3D;True  # 默认False  配合沙箱模式使用    )    while True:        # 调用alipay工具查询支付结果        response &#x3D; alipay.api_alipay_trade_query(order_id)  # response是一个字典        # 判断支付结果        code &#x3D; response.get(&quot;code&quot;)  # 支付宝接口调用成功或者错误的标志        trade_status &#x3D; response.get(&quot;trade_status&quot;)  # 用户支付的情况        if code &#x3D;&#x3D; &quot;10000&quot; and trade_status &#x3D;&#x3D; &quot;TRADE_SUCCESS&quot;:            # 表示用户支付成功            # 返回前端json，通知支付成功            return JsonResponse(&#123;&quot;code&quot;: 0, &quot;message&quot;: &quot;支付成功&quot;&#125;)        elif code &#x3D;&#x3D; &quot;40004&quot; or (code &#x3D;&#x3D; &quot;10000&quot; and trade_status &#x3D;&#x3D; &quot;WAIT_BUYER_PAY&quot;):            # 表示支付宝接口调用暂时失败，（支付宝的支付订单还未生成） 后者 等待用户支付            # 继续查询            print(code)            print(trade_status)            continue        else:            # 支付失败            # 返回支付失败的通知            return JsonResponse(&#123;&quot;code&quot;: 1, &quot;message&quot;: &quot;支付失败&quot;&#125;)# Create your views here.\n\n3.主urls.py\n1234567from django.conf.urls import include, urlfrom django.contrib import adminurlpatterns &#x3D; [    url(r&#39;^admin&#x2F;&#39;, include(admin.site.urls)),    url(r&#39;^&#39;, include(&#39;AppTest.urls&#39;)),]\n\n4.AppTest urls.py\n12345678from django.conf.urls import include, urlfrom views import *urlpatterns &#x3D; [    url(r&quot;^$&quot;, index),    url(r&quot;^pay&#x2F;$&quot;, pay),    url(r&quot;^check_pay&#x2F;$&quot;, check_pay),]\n\n5.setttings.py中设置\n修改templates部分\n1234567891011121314151617TEMPLATES &#x3D; [    &#123;        &#39;BACKEND&#39;: &#39;django.template.backends.django.DjangoTemplates&#39;,        &#39;DIRS&#39;: [            os.path.join(BASE_DIR,&quot;template&quot;).replace(&quot;\\\\&quot;,&quot;&#x2F;&quot;)        ],        &#39;APP_DIRS&#39;: True,        &#39;OPTIONS&#39;: &#123;            &#39;context_processors&#39;: [                &#39;django.template.context_processors.debug&#39;,                &#39;django.template.context_processors.request&#39;,                &#39;django.contrib.auth.context_processors.auth&#39;,                &#39;django.contrib.messages.context_processors.messages&#39;,            ],        &#125;,    &#125;,]\n\n末尾加入\n123456STATIC_URL &#x3D; &#39;&#x2F;static&#x2F;&#39;STATICFILES_DIRS &#x3D; [os.path.join(BASE_DIR, &quot;static&quot;)]# 支付宝配置参数ALIPAY_APPID &#x3D; &quot;2017072407880788&quot;ALIPAY_URL &#x3D; &quot;https:&#x2F;&#x2F;openapi.alipay.com&#x2F;gateway.do&quot;\n\n\n\n\n\n测试效果：\n\n\n返回结果\nhttps://example.com/?total_amount=0.01&amp;timestamp=2018-01-06+21%3A28%3A38&amp;sign=fg19hD85DPPuN1aaI%2B%2BskuomKUxaDGE%2FdvyttEyV3vubVkVvBDXVziZaGybXqZs5o4bXYojx587qNBb8e%2FjAJOBCwKwYZxd7qR3AKlVabkPDzEOlzvEaSW7HTQpsWsVeX6BW%2ByEO8pWQ8c%2BS8B8tS8a8AFtQxeW92as4hdNjQU2YBZ2SVxtKSohWbFWpny1gDWXinQ3y2HNo4t5lmA8fRknB0MaUPwR1SzWa0k%2BylYjpWEnzC6OihP0Er21Ad8fiUwtSxZqH4xIAhnbofAy%2BHYZZVsv5lYg%2Brb87eM6Yz7xwUe5v5dDEoz%2FOLjsuB0GDRTdvhHqs39cGIoMXFfEpbw%3D%3D&amp;trade_no=2018010621001004260217512776&amp;sign_type=RSA2&amp;auth_app_id=2017072407880788&amp;charset=utf-8&amp;seller_id=2088221936946848&amp;method=alipay.trade.page.pay.return&amp;app_id=2017072407880788&amp;out_trade_no=20180105002&amp;version=1.0\n","categories":["支付宝扫码"],"tags":["python"]},{"title":"ElasticSearch 索引、类型、文档。接口的方式 CURD","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/08/05/ElasticSearch 索引、类型、文档。接口的方式 CURD/","content":"参考文档：https://blog.csdn.net/gwd1154978352/article/details/82740424\n索引索引是ElasticSearch存放数据的地方，可以理解为Mysql中的一个数据库。你可以这么理解，你不能就说他是\n我们的数据被 存储和索引在分片(shards)中，索引只是一个 把一个或多个分片分组在一起的逻辑空间。（就好像说）这是一些内部细节我们不用关心分片。\n对于我们的程序而言，文档存储在索引(index)中。剩下的细节由Elasticsearch关心既可。（索引的名字必须是全部小写，不能以下划线开头，不能包含逗号）\n\n类型类型用于区分同一个索引下不同的数据类型，相当于MySQL中的表。在Elasticsearch中，我们使用相同类型(type)的文档表示相同的“事物”，因为他们的数据结构也是相同的。每个类型(type)都有自己的映射(mapping)或者结构定义，就像传统数据库表中的列一样。所有类型下的文档被存储在同一个索引下，但是类型的映射(mapping)会告诉Elasticsearch不同的文档如何被索引。\n这个还是比较好理解的\n注：5.0版本之前，一个索引可以对应多个类型。5.0版本之后，一个索引只能对应一个类型\n文档文档是ElasticSearch中存储的实体，类比MySQL，每个文档相当于数据库表中的一行数据。 在Elasticsearch中，文档 (document) 这个术语有着特殊含义。它特指最顶层结构或者根对象(root object)序列化成的JSON数据（以唯一ID标识并存储于Elasticsearch中）。文档由字段组成，相当于关系数据库中列的属性，不同的是ES的不同文档可以具有不同的字段集合。对比关系型数据库：\n123Relational DB -&gt; Databases -&gt; Tables -&gt; Rows -&gt; ColumnsElasticsearch -&gt; Indices   -&gt; Types  -&gt; Documents -&gt; Fields12\n\n文档元数据一个文档不只有数据。它还包含了元数据(metadata)——关于文档的信息。三个必须的元数据节点是：\n\n\n\n节点\n说明\n\n\n\n_index\n文档存储的地方\n\n\n_type\n文档代表的对象的类\n\n\n_id\n文档的唯一标识\n\n\n\n_index：索引\n_type：类型\n_id：id仅仅是一个字符串，它与_index和_type组合时，就可以在Elasticsearch中唯一标识一个文档。当创建一个文档，你可以自定义_id，也可以让Elasticsearch帮你自动生成。\n\n说的也就是这个 ↓\n索引创建的规则\n类似的数据放在一个索引，非类似的数据放不同索引\nindex中包含了很多类似的document\n索引名称必须是小写的，不能用下划线开头，不能包含逗号：product，website，blog\n\n创建索引、类型、文档（接口的方式）刚才大家也看到了我的截图是在 Kibana上操作的，但现在书写的是以 接口的方式创建，Kibana 会在后续更新出来。\n就随便写写\n12345678curl -H &#x27;Content-Type:application/json&#x27; -XPUT http://localhost:9200/Only_for_you/testing_data/1 -d &quot;&#123;      &quot;private_id&quot;: &quot;1&quot;,      &quot;title&quot;: &quot;This is an interesting piece of data&quot;,      &quot;content&quot;: &quot;I Really Really, really like you&quot;,      &quot;tags&quot;: [&quot;elasticsearch&quot;, &quot;Love Letter&quot;]&#125;&quot;1234567\n\n执行后 就会OK了，大家看到我在 类型的后面写了个 1，这个1 就代表了ID。如果不写的话 系统会自动生成的id，长度为20个字符，URL安全，base64编码，GUID，分布式系统并行生成时不可能会发生冲突。\n就比如这条数据，我们更新他 更新就不要用 put了，会error\n12345678curl -H &#x27;Content-Type:application/json&#x27; -XPOST http://localhost:9200/Only_for_you/testing_data/ -d &quot;&#123;      &quot;private_id&quot;: &quot;1&quot;,      &quot;title&quot;: &quot;This is an interesting piece of data&quot;,      &quot;content&quot;: &quot;I Really Really, really like you&quot;,      &quot;tags&quot;: [&quot;elasticsearch&quot;, &quot;Love Letter&quot;]&#125;&quot;1234567\n\n返回结果\n12345678910111213141516&#123;  &quot;_index&quot; : &quot;Only_for_you&quot;,  &quot;_type&quot; : &quot;testing_data&quot;,  &quot;_id&quot; : &quot;oJVnqXIBlXk7LCfggxrl&quot;,  &quot;_version&quot; : 2,  &quot;result&quot; : &quot;updated&quot;,  &quot;_shards&quot; : &#123;    &quot;total&quot; : 2,    &quot;successful&quot; : 1,    &quot;failed&quot; : 0  &#125;,  &quot;_seq_no&quot; : 2,  &quot;_primary_term&quot; : 1&#125;1234567891011121314这里简单的概述下：POST 和 PUT 上的区别\n\n\nPOST不用加具体的id，它是作用在一个集合资源之上的（/uri）\nPUT操作是作用在一个具体资源之上的（/uri/xxx），所以要指定ID\n\n查看文档查看的话，方法很多种。\n\n浏览器直接 拼地址\n\n\n使用ES浏览器插件\n插件地址：https://download.csdn.net/download/weixin_44685869/12518847我已经上传到我的 CSDN上传里了，你们可以在那里面找\n\n在Linux中使用如下脚本：\n\n\n12curl -H &#x27;Content-Type:application/json&#x27; -XGET http://localhost:9200/Only_for_you/testing_data/1?pretty1\n\n美化返回结果返回的响应包含了现在熟悉的元数据节点，增加了_source字段，它包含了在创建索引时我们发送给Elasticsearch的原始文档。\n\npretty：在任意的查询字符串中增加pretty参数，类似于上面的例子。会让Elasticsearch美化输出(pretty-print)JSON响应以便更加容易阅读。\n_source字段不会被美化，它的样子与我们输入的一致，现在只包含我们请求的字段，而且过滤了date字段。\n\n如果你只想得到_source字段而不要其他的元数据，你可以这样请求：\n12curl -H &#x27;Content-Type:application/json&#x27; -XGET http://localhost:9200/Only_for_you/testing_data/1/_source1\n\n返回响应头消息请求返回的响应内容包括 {“found”: true}。这意味着文档已经找到。如果我们请求一个不存在的文档，依旧会得到一个JSON，不过found值变成了false。此外，HTTP响应状态码也会变成’404 Not Found’代替’200 OK’。我们可以在curl后加-i参数得到响应头：\n12curl -H &#x27;Content-Type:application/json&#x27; -i -XGET http://localhost:9200/Only_for_you/testing_data/1?pretty1\n\n返回结果：\n123456HTTP/1.1 200 OKcontent-type: application/json; charset=UTF-8content-length: 337*********** 以下数据省略 ************123456\n\n更新文档123curl -H &#x27;Content-Type:application/json&#x27; -XPOST http://localhost:9200/Only_for_you/testing_data/1/_update -d &#x27;&#123;  &quot;script&quot;: &quot;ctx._source.content = \\&quot;new content\\&quot;&quot;123\n\n删除文档12curl -XDELETE http://localhost:9200/blog/article/1 1\n\n增删改查 简单的概述下\n\n\n命令\n含义\n\n\n\nPOST\n/uri 创建\n\n\nDELETE\n/uri/xxx 删除\n\n\nPUT\n/uri/xxx 更新或创建\n\n\nGET\n/uri/xxx 查看\n\n\n","categories":["ES检索"],"tags":["python"]},{"title":"什么是ElasticSearch？","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/08/01/ElasticSearch 介绍/","content":"\n什么是ElasticSearch？Elasticsearch 是一个开源的搜索引擎，建立在一个全文搜索引擎库 Apache Lucene基础之上。 Lucene 可以说是当下最先进、高性能、全功能的搜索引擎库—无论是开源还是私有。\nLucece 这个 可是个很牛逼的东西，整理好资料我会在接下来的博客中 发布。\n优点：\n\n一个分布式的实时文档存储，每个字段 可以被索引与搜索\n一个分布式实时分析搜索引擎\n能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据\n\nElasticsearch 可以横向扩展至数百（甚至数千）的服务器节点，同时可以处理PB级数据。Elasticsearch 天生就是分布式的，并且在设计时屏蔽了分布式的复杂性。\n用它来干什么?Elasticsearch 是将所有的功能打包成一个单独的服务，这样你可以通过程序与它提供的简单的 RESTful API 进行通信， 可以使用自己喜欢的编程语言充当 web 客户端 和 命令行 去充当这个客户端。\nElasticsearch 在速度和可扩展性方面都表现出色，而且还能够索引多种类型的内容，这意味着其可用于多种用例：\n\n应用程序搜索\n网站搜索\n企业搜索\n日志处理和分析\n基础设施指标和容器监测\n应用程序性能监测\n地理空间数据分析和可视化\n安全分析\n业务分析\n\n为何使用 Elasticsearch？Elasticsearch 快，真男人的那种快。 由于 Elasticsearch 是在 Lucene 基础上构建而成的，所以在全文本搜索方面表现十分牛逼。Elasticsearch 同时还是一个近实时的搜索平台，这意味着从文档索引操作到文档变为可搜索状态之间的延时很短，一般只有一秒。因此，Elasticsearch 非常适用于对时间有严苛要求的用例，例如安全分析和基础设施监测。\nElasticsearch 具有分布式的本质特征。 Elasticsearch 中存储的文档分布在不同的容器中，这些容器称为分片，可以进行复制以提供数据冗余副本，以防发生硬件故障。Elasticsearch 的分布式特性使得它可以扩展至数百台（甚至数千台）服务器，并处理 PB 量级的数据。\nElasticsearch 包含一系列广泛的功能。 除了速度、可扩展性和弹性等优势以外，Elasticsearch 还有大量强大的内置功能（例如数据汇总和索引生命周期管理），可以方便用户更加高效地存储和搜索数据。\nElastic Stack 简化了数据采集、可视化和报告过程。 通过与 Beats 和 Logstash 进行集成，用户能够在向 Elasticsearch 中索引数据之前轻松地处理数据。同时，Kibana 不仅可针对 Elasticsearch 数据提供实时可视化，同时还提供 UI 以便用户快速访问应用程序性能监测 (APM)、日志和基础设施指标等数据。\nelastic search 在项目中的位置\nElasticsearch 的工作原理是什么？原始数据会从多个来源（包括日志、系统指标和网络应用程序）输入到 Elasticsearch 中。数据采集指在 Elasticsearch 中进行索引之前解析、标准化并充实这些原始数据的过程。这些数据在 Elasticsearch 中索引完成之后，用户便可针对他们的数据运行复杂的查询，并使用聚合来检索自身数据的复杂汇总。在 Kibana 中，用户可以基于自己的数据创建强大的可视化，分享仪表板，并对 Elastic Stack 进行管理。\nElasticsearch 索引是什么？Elasticsearch 索引指相互关联的文档集合。Elasticsearch 会以 JSON 文档的形式存储数据。每个文档都会在一组键（字段或属性的名称）和它们对应的值（字符串、数字、布尔值、日期、数值组、地理位置或其他类型的数据）之间建立联系。\nElasticsearch 使用的是一种名为 倒排索引 的数据结构，这一结构的设计可以允许十分快速地进行全文本搜索。倒排索引会列出在所有文档中出现的每个特有词汇，并且可以找到包含每个词汇的全部文档。\n在索引过程中，Elasticsearch 会存储文档并构建倒排索引，这样用户便可以近实时地对文档数据进行搜索。索引过程是在索引 API 中启动的，通过此 API 您既可向特定索引中添加 JSON 文档，也可更改特定索引中的 JSON 文档。\nELK &#x3D; Elasticsearch, Logstash, Kibana 是一套实时数据收集，存储，索引，检索，统计分析及可视化的解决方案。最新版本已经改名为Elastic Stack，并新增了Beats项目。\n\nLogstash 的用途是什么？Logstash 是 Elastic Stack 的核心产品之一，可用来对数据进行聚合和处理，并将数据发送到 Elasticsearch。Logstash 是一个开源的服务器端数据处理管道，允许您在将数据索引到 Elasticsearch 之前同时从多个来源采集数据，并对数据进行充实和转换。\nKibana 的用途是什么？Kibana 是用于 Elasticsearch 的数据可视化和管理工具，实时的提供直方图、线形图、饼状图和地图。Kibana 同时还包括诸如 Canvas 和 Elastic Maps 等高级应用程序；Canvas 允许用户基于自身数据创建定制的动态信息图表，而 Elastic Maps 则可用来对地理空间数据进行可视化。\n","categories":["ES检索"],"tags":["python"]},{"title":"Django Admin上传图片","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/07/22/Django Admin上传图片/","content":"上传图片Django有提供文件系统支持，在Admin站点中可以轻松上传图片。\n使用Admin站点保存图片，需要安装Python的图片操作包\n12pip install Pillow1\n\n1 配置默认情况下，Django会将上传的图片保存在本地服务器上，需要配置保存的路径。\n我们可以将上传的文件保存在静态文件目录中，如我们之前设置的static目录中在settings.py 文件中添加如下上传保存目录信息\n12MEDIA_ROOT=os.path.join(BASE_DIR,&quot;static/media&quot;)1\n\n\n2 为模型类添加ImageField字段我们为之前的BookInfo模型类添加一个ImageFiled\n1234567class BookInfo(models.Model):    ...    image = models.ImageField(upload_to=&#x27;book&#x27;, verbose_name=&#x27;图片&#x27;, null=True)    upload_to 选项指明该字段的图片保存在MEDIA_ROOT目录中的哪个子目录123456\n\n\n进行数据库迁移操作\n\n123python manage.py makemigrationspython manage.py migrate12\n\n3 使用Admin站点上传图片进入Admin站点的图书管理页面，选择一个图书，能发现多出来一个上传图片的字段\n\n选择一张图片并保存后，图片会被保存在static/media/book/目录下。\n在数据库中，我们能看到image字段被设置为图片的路径\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Django Admin 调整站点信息（更改 Admin站点的名称信息）","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/07/16/Django Admin 调整站点信息（更改 Admin站点的名称信息）/","content":"调整站点信息Admin站点的名称信息也是可以自定义的。\n未调整前如下图：\n\n\nadmin.site.site_header 设置网站页头\nadmin.site.site_title 设置页面标题\nadmin.site.index_title 设置首页标语\n\n在 admin.py 文件中添加一下信息\n123456from django.contrib import adminadmin.site.site_header = &#x27;传智书城&#x27;admin.site.site_title = &#x27;传智书城MIS&#x27;admin.site.index_title = &#x27;欢迎使用传智书城MIS&#x27;12345\n\n刷新网站，效果如下\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Python Admin 调整编辑页展示","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/07/09/Python Admin 调整编辑页展示/","content":"调整编辑页展示1. 显示字段属性如下：\n12fields=[]1\n\n\n1）点击某行ID的链接，可以转到修改页面，默认效果如下图：\n\n\n\n2）打开book/admin.py文件，修改BookInfoAdmin类如下：\n\n1234class BookInfoAdmin(admin.ModelAdmin):    ...    fields = [&#x27;name&#x27;, &#x27;pub_date&#x27;]123\n\n\n3）刷新浏览器效果如下图：\n\n\n2. 分组显示属性如下：\n12345fieldsets=(    (&#x27;组1标题&#x27;,&#123;&#x27;fields&#x27;:(&#x27;字段1&#x27;,&#x27;字段2&#x27;)&#125;),    (&#x27;组2标题&#x27;,&#123;&#x27;fields&#x27;:(&#x27;字段3&#x27;,&#x27;字段4&#x27;)&#125;),)1234\n\n\n1）打开 admin.py 文件，修改 BookInfoAdmin 类如下：\n\n1234567891011class BookInfoAdmin(admin.ModelAdmin):    ...    # fields = [&#x27;name&#x27;, &#x27;pub_date&#x27;]    fieldsets = (        (&#x27;基本&#x27;, &#123;&#x27;fields&#x27;: [&#x27;name&#x27;, &#x27;pub_date&#x27;]&#125;),        (&#x27;高级&#x27;, &#123;            &#x27;fields&#x27;: [&#x27;readcount&#x27;, &#x27;commentcount&#x27;],            &#x27;classes&#x27;: (&#x27;collapse&#x27;,)  # 是否折叠显示        &#125;)    )12345678910\n\n\n2）刷新浏览器效果如下图：说明：fields与fieldsets两者选一使用。\n\n3. 关联对象在一对多的关系中，可以在一端的编辑页面中编辑多端的对象，嵌入多端对象的方式包括表格、块两种。\n\n类型InlineModelAdmin：表示在模型的编辑页面嵌入关联模型的编辑。\n子类TabularInline：以表格的形式嵌入。\n子类StackedInline：以块的形式嵌入。\n\n\n1）打开book/admin.py文件，创建PeopleInfoStackInLine类。\n\n1234class PeopleInfoStackInLine(admin.StackedInline):    model = PeopleInfo #要关联的模型    extra = 2 #附加编辑的数量123\n\n\n2）打开book/admin.py文件，修改BookInfoAdmin类如下：\n\n12345class BookInfoAdmin(admin.ModelAdmin):    ...    inlines = [PeopleInfoStackInLine]1234\n\n\n3）刷新浏览器效果如下图：\n\n可以用表格的形式嵌入。\n\n1）打开book/admin.py文件，创建PeopleInfoTabularInline类。\n\n1234class PeopleInfoTabularInline(admin.TabularInline):    model = PeopleInfo #要关联的模型    extra = 2 #附加编辑的数量123\n\n\n2）打开book/admin.py文件，修改BookInfoAdmin类如下：\n\n12345class BookInfoAdmin(admin.ModelAdmin):    ...    inlines = [PeopleInfoTabularInline]1234\n\n\n3）刷新浏览器效果如下图：\n\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Django Admin 列表页展示","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/07/04/Django Admin 列表页展示/","content":"调整列表页展示1 页大小每页中显示多少条数据，默认为每页显示100条数据，属性如下：\n12list_per_page=1001\n\n\n1）打开 admin.py 文件，修改 AreaAdmin 类如下：\n\n123class BookInfoAdmin(admin.ModelAdmin):    list_per_page = 212\n\n\n2）在浏览器中查看区域信息的列表页面，效果如下图：\n\n\n2 “操作选项”的位置顶部显示的属性，设置为True在顶部显示，设置为False不在顶部显示，默认为True。\n12actions_on_top=True1\n\n底部显示的属性，设置为True在底部显示，设置为False不在底部显示，默认为False。\n12actions_on_bottom=False1\n\n\n1）打开 admin.py 文件，修改 BookInfoAdmin 类如下：\n\n12345class BookInfoAdmin(admin.ModelAdmin):    ...    actions_on_top = True    actions_on_bottom = True1234\n\n2）在浏览器中刷新效果如下图：\n\n3 列表中的列属性如下：\n12list_display=[模型字段1,模型字段2,...]1\n\n\n1）打开 admin.py 文件，修改 BookInfoAdmin 类如下：\n\n1234class BookInfoAdmin(admin.ModelAdmin):    ...    list_display = [&#x27;id&#x27;,&#x27;name&#x27;]123\n\n\n2）在浏览器中刷新效果如下图：\n\n点击列头可以进行升序或降序排列。\n4 将方法作为列列可以是模型字段，还可以是模型方法，要求方法有返回值。\n通过设置short_description属性，可以设置在admin站点中显示的列名。\n\n1）打开 models.py 文件，修改 BookInfo 类如下：\n\n123456789class BookInfo (models.Model):    ...def bookname(self):        return &#x27;&lt;&lt;&#x27; + self.name + &#x27;&gt;&gt;&#x27;    bookname.short_description = &#x27;书名&#x27;  # 设置方法字段在admin中显示的标题12345678\n\n\n2）打开 admin.py 文件，修改 BookInfoAdmin 类如下：\n\n123456class BookInfoAdmin (admin.ModelAdmin) :    ...   list_display = [&#x27;id&#x27;,&#x27;name&#x27;,&#x27;bookname&#x27;]1234\n\n\n3）在浏览器中刷新效果如下图：\n\n方法列是不能排序的，如果需要排序需要为方法指定排序依据。\n123admin_order_field=模型类字段1\n\n\n1）打开 models.py文件，修改 BookInfo 类如下：\n\n1234567891011class BookInfo(models.Model):    ...def bookname(self):        return &#x27;&lt;&lt;&#x27; + self.name + &#x27;&gt;&gt;&#x27;    bookname.short_description = &#x27;书名&#x27;  # 设置方法字段在admin中显示的标题    bookname.admin_order_field = &#x27;name&#x27;123456789\n\n\n2）在浏览器中刷新效果如下图：\n\n5 关联对象无法直接访问关联对象的属性或方法，可以在模型类中封装方法，访问关联对象的成员。\n\n1）打开 models.py 文件，修改 PeopleInfo 类如下：\n\n123456789class PeopleInfo(models.Model):    ...    def readcount(self):        return self.book.readcount    readcount.short_description = &#x27;图书阅读量&#x27;1234567\n\n2）打开 admin.py 文件，修改 PeopleInfoAdmin 类如下：\n123456@admin.register(PeopleInfo)class PeopleInfoAdmin(admin.ModelAdmin):    list_display = [&#x27;id&#x27;,&#x27;name&#x27;,&#x27;book&#x27;,&#x27;readcount&#x27;]1234\n\n3）在浏览器中刷新效果如下图：\n\n6 右侧栏过滤器属性如下，只能接收字段，会将对应字段的值列出来，用于快速过滤。一般用于有重复值的字段。\n123list_filter=[]1\n\n\n1）打开 admin.py 文件，修改 PeopleInfoAdmin 类如下：\n\n12345678@admin.register(PeopleInfo)class PeopleInfoAdmin(admin.ModelAdmin):    list_display = [&#x27;id&#x27;,&#x27;name&#x27;,&#x27;book&#x27;,&#x27;readcount&#x27;]    list_filter = [&#x27;book&#x27;,&#x27;gender&#x27;]123456\n\n\n2）在浏览器中刷新效果如下图：\n\n\n7 搜索框属性如下，用于对指定字段的值进行搜索，支持模糊查询。列表类型，表示在这些字段上进行搜索。\n123search_fields=[]1\n\n\n1）打开admin.py文件，修改PeopleInfoAdmin类如下：\n\n123456789@admin.register(PeopleInfo)class PeopleInfoAdmin(admin.ModelAdmin):    list_display = [&#x27;id&#x27;,&#x27;name&#x27;,&#x27;book&#x27;,&#x27;readcount&#x27;]    list_filter = [&#x27;book&#x27;,&#x27;gender&#x27;]    search_fields = [&#x27;name&#x27;]12345678\n\n\n2）在浏览器中刷新效果如下图：\n\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Django Admin站点","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/07/02/Django Admin站点/","content":"Admin站点假设我们要设计一个新闻网站，我们需要编写展示给用户的页面，网页上展示的新闻信息是从哪里来的呢？是从数据库中查找到新闻的信息，然后把它展示在页面上。 但是我们的网站上的新闻每天都要更新，这就意味着对数据库的增、删、改、查操作，那么我们需要每天写sql语句操作数据库吗? 如果这样的话，是不是非常繁琐，所以我们可以设计一个页面，通过对这个页面的操作来实现对新闻数据库的增删改查操作。那么问题来了，老板说我们需要在建立一个新网站，是不是还要设计一个页面来实现对新网站数据库的增删改查操作，但是这样的页面具有一个很大的重复性，那有没有一种方法能够让我们很快的生成管理数据库表的页面呢？有，那就是我们接下来要给大家讲的Django的后台管理。Django能够根据定义的模型类自动地生成管理页面。\n使用Django的管理模块，需要按照如下步骤操作：\n\n管理界面本地化\n创建管理员\n注册模型类\n自定义管理页面\n\n1 管理界面本地化在settings.py中设置语言和时区\n123456#设置中文LANGUAGE_CODE = &#x27;zh-Hans&#x27;#亚洲上海时区TIME_ZONE = &#x27;Asia/Shanghai&#x27;12345\n\n2 创建超级管理员创建管理员的命令如下，按提示输入用户名、邮箱、密码。\n12python manage.py createsuperuser1\n\n\n打开浏览器，在地址栏中输入如下地址后回车。\n12http://127.0.0.1:8000/admin/1\n\n输入前面创建的用户名、密码完成登录。\n\n登录成功后界面如下，但是并没有我们自己应用模型的入口，接下来进行第三步操作。\n\n3 注册模型类登录后台管理后，默认没有我们创建的应用中定义的模型类，需要在自己应用中的admin.py文件中注册，才可以在后台管理中看到，并进行增删改查操作。\n打开booktest/admin.py文件，编写如下代码：\n123456789from django.contrib import admin#导入模型from book.models import BookInfo,PeopleInfo# Register your models here.#注册书籍模型admin.site.register(BookInfo)#注册人物模型admin.site.register(PeopleInfo)12345678\n\n到浏览器中刷新页面，可以看到模型类BookInfo和PeopleInfo的管理了。点击类名称”BookInfo”（图书）可以进入列表页，默认只有一列。在列表页中点击”增加”可以进入增加页，Django会根据模型类的不同，生成不同的表单控件，按提示填写表单内容后点击”保存”，完成数据创建，创建成功后返回列表页。\n在列表页中点击某行的第一列可以进入修改页。\n按照提示进行内容的修改，修改成功后进入列表页。在修改页点击“删除”可以删除一项。\n删除：在列表页勾选想要删除的复选框，可以删除多项。\n点击执行后进入确认页面，删除后回来列表页面。\n\n4 定义与使用Admin管理类Django提供的Admin站点的展示效果可以通过自定义ModelAdmin类来进行控制。\n定义管理类需要继承自admin.ModelAdmin类，如下\n12345from django.contrib import adminclass BookInfoAdmin(admin.ModelAdmin):    pass1234\n\n使用管理类有两种方式：\n\n注册参数\n\n12admin.site.register(BookInfo,BookInfoAdmin1\n\n\n装饰器\n\n123@admin.register(BookInfo)class BookInfoAdmin(admin.ModelAdmin):    pass\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Django 模板 进阶","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/06/29/Django 模板 进阶/","content":"1 配置在工程中创建模板目录templates。\n在settings.py配置文件中修改TEMPLATES配置项的DIRS值：\n123456789101112131415TEMPLATES = [    &#123;        &#x27;BACKEND&#x27;: &#x27;django.template.backends.django.DjangoTemplates&#x27;,        &#x27;DIRS&#x27;: [os.path.join(BASE_DIR, &#x27;templates&#x27;)],  # 此处修改        &#x27;APP_DIRS&#x27;: True,        &#x27;OPTIONS&#x27;: &#123;            &#x27;context_processors&#x27;: [                &#x27;django.template.context_processors.debug&#x27;,                &#x27;django.template.context_processors.request&#x27;,                &#x27;django.contrib.auth.context_processors.auth&#x27;,                &#x27;django.contrib.messages.context_processors.messages&#x27;,            ],        &#125;,    &#125;,]\n\n2 定义模板在templates目录中新建一个模板文件，如index.html\n1234567891011html&lt;!DOCTYPE html&gt;&lt;html lang&#x3D;&quot;en&quot;&gt;&lt;head&gt;    &lt;meta charset&#x3D;&quot;UTF-8&quot;&gt;    &lt;title&gt;Title&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;&lt;body&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt;\n\n3 模板渲染调用模板分为两步骤：\n\n找到模板 loader.get_template(模板文件在模板目录中的相对路径) -&gt; 返回模板对象\n渲染模板 模板对象.render(context=None, request=None) -&gt; 返回渲染后的html文本字符串 context 为模板变量字典，默认值为None request 为请求对象，默认值为None\n\n例如，定义一个视图\n12345678910from django.http import HttpResponsefrom django.template import loaderdef index(request):    # 1.获取模板    template=loader.get_template(&#x27;index.html&#x27;)    context=&#123;&#x27;city&#x27;: &#x27;北京&#x27;&#125;    # 2.渲染模板    return HttpResponse(template.render(context))\n\nDjango提供了一个函数render可以简写上述代码。\nrender(request对象, 模板文件路径, 模板数据字典)\n12345from django.shortcuts import renderdef index(request):    context=&#123;&#x27;city&#x27;: &#x27;北京&#x27;&#125;    return render(request,&#x27;index.html&#x27;,context)\n\n4 模板语法4.1 模板变量变量名必须由字母、数字、下划线（不能以下划线开头）和点组成。\n语法如下：\n模板变量可以使python的内建类型，也可以是对象。\n12345678910def index(request):    context = &#123;        &#x27;city&#x27;: &#x27;北京&#x27;,        &#x27;adict&#x27;: &#123;            &#x27;name&#x27;: &#x27;西游记&#x27;,            &#x27;author&#x27;: &#x27;吴承恩&#x27;        &#125;,        &#x27;alist&#x27;: [1, 2, 3, 4, 5]    &#125;    return render(request, &#x27;index.html&#x27;, context)\n\n\n4.2 模板语句\nfor循环：\n\n\nif条件：\n\n\n\n比较运算符如下：\n123456==!=&lt;&gt;&lt;=&gt;=\n\n布尔运算符如下：\n123andornot\n\n注意：运算符左右两侧不能紧挨变量或常量，必须有空格。\n4.3 过滤器语法如下:\n\n使用管道符号|来应用过滤器，用于进行计算、转换操作，可以使用在变量、标签中。\n如果过滤器需要参数，则使用冒号:传递参数。\n变量|过滤器:参数\n\n列举几个如下：\n\nsafe，禁用转义，告诉模板这个变量是安全的，可以解释执行\n\nlength，长度，返回字符串包含字符的个数，或列表、元组、字典的元素个数。\n\ndefault，默认值，如果变量不存在时则返回默认值。\n\ndata|default:&#39;默认值&#39;\n1\n1234567891011121314- date，日期，用于对日期类型的值进行字符串格式化，常用的格式化字符如下：  - Y表示年，格式为4位，y表示两位的年。  - m表示月，格式为01,02,12等。  - d表示日, 格式为01,02等。  - j表示日，格式为1,2等。  - H表示时，24进制，h表示12进制的时。  - i表示分，为0-59。  - s表示秒，为0-59。- &#96;&#96;&#96;python  value|date:&quot;Y年m月j日  H时i分s秒&quot;  \n\n\n\n4.4 注释\n单行注释语法如下：\n\n\n多行注释使用comment标签，语法如下：\n\n\n\n4.5 模板继承模板继承和类的继承含义是一样的，主要是为了提高代码重用，减轻开发人员的工作量。\n父模板\n如果发现在多个模板中某些内容相同，那就应该把这段内容定义到父模板中。\n标签block：用于在父模板中预留区域，留给子模板填充差异性的内容，名字不能相同。 为了更好的可读性，建议给endblock标签写上名字，这个名字与对应的block名字相同。父模板中也可以使用上下文中传递过来的数据。\n子模板\n标签extends：继承，写在子模板文件的第一行。\n子模版不用填充父模版中的所有预留区域，如果子模版没有填充，则使用父模版定义的默认值。\n\n填充父模板中指定名称的预留区域。\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Django 类视图 和 中间件","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/06/18/Django 类视图 和 中间件/","content":"类视图思考：一个视图，是否可以处理两种逻辑？比如get和post请求逻辑。\n如何在一个视图中处理get和post请求\n注册视图处理get和post请求以函数的方式定义的视图称为 函数视图，函数视图便于理解。但是遇到一个视图对应的路径提供了多种不同HTTP请求方式的支持时，便需要在一个函数中编写不同的业务逻辑，代码可读性与复用性都不佳。\n1234567891011def register(request):    &quot;&quot;&quot;处理注册&quot;&quot;&quot;    # 获取请求方法，判断是GET/POST请求    if request.method == &#x27;GET&#x27;:        # 处理GET请求，返回注册页面        return render(request, &#x27;register.html&#x27;)    else:        # 处理POST请求，实现注册逻辑        return HttpResponse(&#x27;这里实现注册逻辑&#x27;)12345678910\n\n类视图使用在Django中也可以使用类来定义一个视图，称为类视图。\n使用类视图可以将视图对应的不同请求方式以类中的不同方法来区别定义。如下所示\n12345678910111213from django.views.generic import Viewclass RegisterView(View):    &quot;&quot;&quot;类视图：处理注册&quot;&quot;&quot;    def get(self, request):        &quot;&quot;&quot;处理GET请求，返回注册页面&quot;&quot;&quot;        return render(request, &#x27;register.html&#x27;)    def post(self, request):        &quot;&quot;&quot;处理POST请求，实现注册逻辑&quot;&quot;&quot;        return HttpResponse(&#x27;这里实现注册逻辑&#x27;)123456789101112\n\n类视图的好处：\n\n代码可读性好\n类视图相对于函数视图有更高的复用性 ， 如果其他地方需要用到某个类视图的某个特定逻辑，直接继承该类视图即可\n\n定义类视图需要继承自Django提供的父类 View，可使用 from django.views.generic import View 或者from django.views.generic.base import View 导入，定义方式如上所示。\n配置路由时，使用类视图的 as_view() 方法来添加。\n1234567urlpatterns = [    # 视图函数：注册    # url(r&#x27;^register/$&#x27;, views.register, name=&#x27;register&#x27;),    # 类视图：注册    url(r&#x27;^register/$&#x27;, views.RegisterView.as_view(), name=&#x27;register&#x27;),]123456\n\n类视图原理123456789101112131415161718192021222324252627282930313233@classonlymethod    def as_view(cls, **initkwargs):        &quot;&quot;&quot;        Main entry point for a request-response process.        &quot;&quot;&quot;        ...省略代码...        def view(request, *args, **kwargs):            self = cls(**initkwargs)            if hasattr(self, &#x27;get&#x27;) and not hasattr(self, &#x27;head&#x27;):                self.head = self.get            self.request = request            self.args = args            self.kwargs = kwargs            # 调用dispatch方法，按照不同请求方式调用不同请求方法            return self.dispatch(request, *args, **kwargs)        ...省略代码...        # 返回真正的函数视图        return view    def dispatch(self, request, *args, **kwargs):        # Try to dispatch to the right method; if a method doesn&#x27;t exist,        # defer to the error handler. Also defer to the error handler if the        # request method isn&#x27;t on the approved list.        if request.method.lower() in self.http_method_names:            handler = getattr(self, request.method.lower(), self.http_method_not_allowed)        else:            handler = self.http_method_not_allowed        return handler(request, *args, **kwargs)1234567891011121314151617181920212223242526272829303132\n\n类视图使用装饰器为类视图添加装饰器，可以使用三种方法。\n为了理解方便，我们先来定义一个为 函数视图准备的装饰器（在设计装饰器时基本都以函数视图作为考虑的被装饰对象），及一个要被装饰的类视图。\n123456789101112131415161718def login_require(func_view):    def wrapper(request，*args,**kwargs):        if False:            return func_view(request,*args,**kwargs)        else:            return HttpResponse(&quot;您没有登陆&quot;)    return wrapperclass CenterView(View):    def get(self,request):        return HttpResponse(&quot;OK&quot;)    def post(self,request):        return HttpResponse(&quot;OK&quot;)1234567891011121314151617\n\n4.1 在URL配置中装饰123456from book.views import login_requireurlpatterns = [    url(r&#x27;^center/$&#x27;,login_require(CenterView.as_view()),name=&#x27;center&#x27;),]12345\n\n此种方式最简单，但因装饰行为被放置到了url配置中，单看视图的时候无法知道此视图还被添加了装饰器，不利于代码的完整性，不建议使用。\n此种方式会为类视图中的所有请求方法都加上装饰器行为（因为是在视图入口处，分发请求方式前）。\n4.2 在类视图中装饰在类视图中使用为函数视图准备的装饰器时，不能直接添加装饰器，需要使用 method_decorator 将其转换为适用于类视图的装饰器。\nmethoddecorator装饰器还支持使用name参数指明被装饰的方法。method_decorator\n123456789@method_decorator(login_require,name=&#x27;dispatch&#x27;)class CenterView(View):    def get(self,request):        return HttpResponse(&quot;OK&quot;)    def post(self,request):        return HttpResponse(&quot;OK&quot;)12345678\n\n4.3 构造Mixin扩展类使用面向对象多继承的特性。\n123456789101112131415class LoginRequireMixin(object):    @classmethod    def as_view(cls,**kwargs):        view = super().as_view(**kwargs)        view = login_require(view)        return viewclass CenterView(LoginRequireMixin,View):    def get(self,request):        return HttpResponse(&quot;OK&quot;)    def post(self,request):        return HttpResponse(&quot;OK&quot;)1234567891011121314\n\n使用Mixin扩展类，也会为类视图的所有请求方法都添加装饰行为。\n中间件Django中的中间件是一个轻量级、底层的插件系统，可以介入Django的请求和响应处理过程，修改Django的输入或输出。中间件的设计为开发者提供了一种无侵入式的开发方式，增强了Django框架的健壮性。\n我们可以使用中间件，在Django处理视图的不同阶段对输入或输出进行干预。\n中间件文档： https://docs.djangoproject.com/en/1.11/topics/http/middleware/\n1 中间件的定义方法定义一个中间件工厂函数，然后返回一个可以被调用的中间件。\n中间件工厂函数需要接收一个可以调用的get_response对象。\n返回的中间件也是一个可以被调用的对象，并且像视图一样需要接收一个request对象参数，返回一个response对象。\n123456789101112131415def simple_middleware(get_response):    # 此处编写的代码仅在Django第一次配置和初始化的时候执行一次。    def middleware(request):        # 此处编写的代码会在每个请求处理视图前被调用。        response = get_response(request)        # 此处编写的代码会在每个请求处理视图之后被调用。        return response    return middleware12345678910111213\n\n例如，在book应用中新建一个middleware.py文件，\n12345678910def my_middleware(get_response):    print(&#x27;init 被调用&#x27;)    def middleware(request):        print(&#x27;before request 被调用&#x27;)        response = get_response(request)        print(&#x27;after response 被调用&#x27;)        return response    return middleware12345678\n\n定义好中间件后，需要在settings.py 文件中添加注册中间件\n123456789101112MIDDLEWARE = [    &#x27;django.middleware.security.SecurityMiddleware&#x27;,    &#x27;django.contrib.sessions.middleware.SessionMiddleware&#x27;,    &#x27;django.middleware.common.CommonMiddleware&#x27;,    # &#x27;django.middleware.csrf.CsrfViewMiddleware&#x27;,    &#x27;django.contrib.auth.middleware.AuthenticationMiddleware&#x27;,    &#x27;django.contrib.messages.middleware.MessageMiddleware&#x27;,    &#x27;django.middleware.clickjacking.XFrameOptionsMiddleware&#x27;,    &#x27;book.middleware.my_middleware&#x27;,  # 添加中间件]12345678910\n\n定义一个视图进行测试\n12345def middleware(request):    print(&#x27;view 视图被调用&#x27;)    return HttpResponse(&#x27;OK&#x27;)123\n\n执行结果注意：Django运行在调试模式下，中间件init部分有可能被调用两次。\n2 多个中间件的执行顺序\n在请求视图被处理前，中间件由上至下依次执行\n在请求视图被处理后，中间件由下至上依次执行\n\n\n示例：\n定义两个中间件\n12345678910111213141516171819def my_middleware(get_response):    print(&#x27;init 被调用&#x27;)    def middleware(request):        print(&#x27;before request 被调用&#x27;)        response = get_response(request)        print(&#x27;after response 被调用&#x27;)        return response    return middlewaredef my_middleware2(get_response):    print(&#x27;init2 被调用&#x27;)    def middleware(request):        print(&#x27;before request 2 被调用&#x27;)        response = get_response(request)        print(&#x27;after response 2 被调用&#x27;)        return response    return middleware1234567891011121314151617\n\n注册添加两个中间件\n12345678910111213MIDDLEWARE = [    &#x27;django.middleware.security.SecurityMiddleware&#x27;,    &#x27;django.contrib.sessions.middleware.SessionMiddleware&#x27;,    &#x27;django.middleware.common.CommonMiddleware&#x27;,    # &#x27;django.middleware.csrf.CsrfViewMiddleware&#x27;,    &#x27;django.contrib.auth.middleware.AuthenticationMiddleware&#x27;,    &#x27;django.contrib.messages.middleware.MessageMiddleware&#x27;,    &#x27;django.middleware.clickjacking.XFrameOptionsMiddleware&#x27;,    &#x27;users.middleware.my_middleware&#x27;,  # 添加    &#x27;users.middleware.my_middleware2&#x27;,  # 添加]1234567891011\n\n执行结果\n12345678init2 被调用init 被调用before request 被调用before request 2 被调用view 视图被调用after response 2 被调用after response 被调用1234567\n\ndjango中间件的5个方法\nprocess_request : 请求进来时,权限认证 。\nprocess_view : 路由匹配之后,能够得到视图函数\nprocess_exception : 异常时执行\nprocess_template_responseprocess : 模板渲染时执行\nprocess_response : 请求有响应时执行\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Django 状态保持 Cookie 和 Session","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/06/12/Django 状态保持 Cookie 和 Session/","content":"状态保持\n浏览器请求服务器是无状态的。\n\n无状态：指一次用户请求时，浏览器、服务器无法知道之前这个用户做过什么，每次请求都是一次新的请求。\n\n无状态原因：浏览器与服务器是使用Socket套接字进行通信的，服务器将请求结果返回给浏览器之后，会关闭当前的Socket连接，而且服务器也会在处理页面完毕之后销毁页面对象。\n\n有时需要保持下来用户浏览的状态，比如用户是否登录过，浏览过哪些商品等\n\n实现状态保持主要有两种方式：\n在客户端存储信息使用Cookie在服务器端存储信息使用Session\n\n\nCookieCookie，有时也用其复数形式Cookies，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密）。Cookie最早是网景公司的前雇员Lou Montulli在1993年3月的发明。Cookie是由服务器端生成，发送给User-Agent（一般是浏览器），浏览器会将Cookie的key/value保存到某个目录下的文本文件内，下次请求同一网站时就发送该Cookie给服务器（前提是浏览器设置为启用cookie）。Cookie名称和值可以由服务器端开发自己定义，这样服务器可以知道该用户是否是合法用户以及是否需要重新登录等。服务器可以利用Cookies包含信息的任意性来筛选并经常性维护这些信息，以判断在HTTP传输中的状态。Cookies最典型记住用户名。\nCookie是存储在浏览器中的一段纯文本信息，建议不要存储敏感信息如密码，因为电脑上的浏览器可能被其它人使用。\nCookie的特点\nCookie以键值对的格式进行信息的存储。\nCookie基于域名安全，不同域名的Cookie是不能互相访问的，如访问itcast.cn时向浏览器中写了Cookie信息，使用同一浏览器访问baidu.com时，无法访问到itcast.cn写的Cookie信息。\n当浏览器请求某网站时，会将浏览器存储的跟网站相关的所有Cookie信息提交给网站服务器。\n\n1 设置Cookie可以通过HttpResponse对象中的set_cookie方法来设置cookie。\n12HttpResponse.set_cookie(cookie名, value=cookie值, max_age=cookie有效期)1\n\n\nmax_age单位为秒，默认为None 。如果是临时cookie，可将max_age设置为None。\n\n示例：\n123456def cookie(request):    response = HttpResponse(&#x27;ok&#x27;)    response.set_cookie(&#x27;itcast1&#x27;, &#x27;python1&#x27;)  # 临时cookie    response.set_cookie(&#x27;itcast2&#x27;, &#x27;python2&#x27;, max_age=3600)  # 有效期一小时    return response12345\n\n2 读取Cookie可以通过 HttpResponse 对象的 COOKIES 属性来读取本次请求携带的 cookie 值。request.COOKIES为字典类型。\n12345def cookie(request):    cookie1 = request.COOKIES.get(&#x27;itcast1&#x27;)    print(cookie1)    return HttpResponse(&#x27;OK&#x27;)1234\n\n3 删除Cookie可以通过 HttpResponse 对象中的delete_cookie方法来删除。\n12response.delete_cookie(&#x27;itcast2&#x27;)1\n\n\n\nSession1 启用SessionDjango项目默认启用Session。\n可以在settings.py文件中查看，如图所示\n如需禁用session，将上图中的session中间件注释掉即可。\n2 存储方式在settings.py文件中，可以设置session数据的存储方式，可以保存在数据库、本地缓存等。\n2.1 数据库存储在数据库中，如下设置可以写，也可以不写，这是默认存储方式。\n12SESSION_ENGINE=&#x27;django.contrib.sessions.backends.db&#x27;1\n\n如果存储在数据库中，需要在项INSTALLED_APPS中安装Session应用。\n数据库中的表如图所示\n表结构如下由表结构可知，操作Session包括三个数据：键，值，过期时间。\n2.2 本地缓存存储在本机内存中，如果丢失则不能找回，比数据库的方式读写更快。\n12SESSION_ENGINE=&#x27;django.contrib.sessions.backends.cache&#x27;1\n\n2.3 混合存储优先从本机内存中存取，如果没有则从数据库中存取。\n12SESSION_ENGINE=&#x27;django.contrib.sessions.backends.cached_db&#x27;1\n\n2.4 Redis在redis中保存session，需要引入第三方扩展，我们可以使用django-redis来解决。\n\n1） 安装扩展\n12pip install django-redis1\n\n2）配置\n在settings.py文件中做如下设置\n\n\n12345678910111213CACHES = &#123;    &#x27;default&#x27;: &#123;        &#x27;BACKEND&#x27;: &#x27;django_redis.cache.RedisCache&#x27;,        &#x27;LOCATION&#x27;: &#x27;redis://127.0.0.1:6379/1&#x27;,        &#x27;OPTIONS&#x27;: &#123;            &#x27;CLIENT_CLASS&#x27;: &#x27;django_redis.client.DefaultClient&#x27;,        &#125;    &#125;&#125;SESSION_ENGINE = &#x27;django.contrib.sessions.backends.cache&#x27;SESSION_CACHE_ALIAS = &#x27;default&#x27;1234567891011\n\n注意如果redis的ip地址不是本地回环127.0.0.1，而是其他地址，访问Django时，可能出现Redis连接错误，如下：\n\n解决方法：\n修改redis的配置文件，添加特定ip地址。\n打开redis的配置文件\n123sudo vim /etc/redis/redis.conf1\n\n在如下配置项进行修改（如要添加10.211.55.5地址）\n\n重新启动redis服务\n123sudo service redis-server restart1\n\n3 Session操作通过HttpRequest对象的session属性进行会话的读写操作。\n\n1） 以键值对的格式写session。\n\n123request.session[&#x27;键&#x27;]=值1\n\n\n2）根据键读取值。\n\n123request.session.get(&#x27;键&#x27;,默认值)1\n\n\n3）清除所有session，在存储中删除值部分。\n\n123request.session.clear()1\n\n\n4）清除session数据，在存储中删除session的整条数据。\n\n123request.session.flush()1\n\n\n5）删除session中的指定键及值，在存储中只删除某个键及对应的值。\n\n12del request.session[&#x27;键&#x27;]1\n\n\n6）设置session的有效期\n\n12request.session.set_expiry(value)1\n\n\n如果value是一个整数，session将在value秒没有活动后过期。\n如果value为0，那么用户session的Cookie将在用户的浏览器关闭时过期。\n如果value为None，那么session有效期将采用系统默认值，默认为两周，可以通过在settings.py中设置SESSION_COOKIE_AGE来设置全局默认值。\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Django HTTP Request对象 和 HTTP Response 对象","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/06/04/Django HTTP Request对象 和 HTTP Response 对象/","content":"HttpRequest对象回想一下，利用HTTP协议向服务器传参有几种途径？\n\n提取URL的特定部分，如/weather/beijing/2018，可以在服务器端的路由中用正则表达式截取；\n查询字符串（query string)，形如key1=value1&amp;key2=value2；\n请求体（body）中发送的数据，比如表单数据、json、xml；\n在http报文的头（header）中。\n\n1 URL路径参数\n如果想从URL中获取值，需要在正则表达式中使用分组，\n\n获取值分为两种方式\n\n位置参数参数的位置不能错\n关键字参数参数的位置可以变，跟关键字保持一致即可\n\n\n注意：两种参数的方式不要混合使用，在一个正则表达式中只能使用一种参数方式\n\n分别使用以上两种获取URL值的方式提取出18 188\n12http://127.0.0.1:8000/18/188/1\n\n\n\n位置参数\n应用中urls.py\n12url(r&#x27;^(\\d+)/(\\d+)/$&#x27;, views.index),1\n\n视图中函数: 参数的位置不能错\n12345def index(request, value1, value2):\t# 构造上下文\tcontext = &#123;&#x27;v1&#x27;:value1, &#x27;v2&#x27;:value2&#125;\treturn render(request, &#x27;Book/index.html&#x27;, context)1234\n\n\n\n关键字参数\n应用中urls.py\n其中 ?P 部分表示为这个参数定义的名称为value1可以是其它名称，起名要做到见名知意\n12url(r&#x27;^(?P&lt;value1&gt;\\d+)/(?P&lt;value2&gt;\\d+)/$&#x27;, views.index),1\n\n视图中函数: 参数的位置可以变，跟关键字保持一致即可\n12345def index(request, value2, value1):       # 构造上下文    context = &#123;&#x27;v1&#x27;:value1, &#x27;v2&#x27;:value2&#125;    return render(request, &#x27;Book/index.html&#x27;, context)1234\n\n\n\n2 Django中的QueryDict对象HttpRequest对象的属性GET、POST都是QueryDict类型的对象\n与python字典不同，QueryDict类型的对象用来处理同一个键带有多个值的情况\n\n方法get()：根据键获取值\n如果一个键同时拥有多个值将获取最后一个值\n如果键不存在则返回None值，可以设置默认值进行后续处理\n12get(&#x27;键&#x27;,默认值)1\n\n方法getlist()：根据键获取值，值以列表返回，可以获取指定键的所有值\n如果键不存在则返回空列表[]，可以设置默认值进行后续处理\n12getlist(&#x27;键&#x27;,默认值)1\n\n\n\n3. 查询字符串Query String获取请求路径中的查询字符串参数（形如?k1=v1&amp;k2=v2），可以通过request.GET属性获取，返回QueryDict对象。\n123456789101112# /get/?a=1&amp;b=2&amp;a=3def get(request):    a = request.GET.get(&#x27;a&#x27;)    b = request.GET.get(&#x27;b&#x27;)    alist = request.GET.getlist(&#x27;a&#x27;)    print(a)  # 3    print(b)  # 2    print(alist)  # [&#x27;1&#x27;, &#x27;3&#x27;]    return HttpResponse(&#x27;OK&#x27;)12345678910重要：查询字符串不区分请求方式，即假使客户端进行POST方式的请求，依然可以通过request.GET获取请求中的查询字符串数据。\n\n4 请求体请求体数据格式不固定，可以是表单类型字符串，可以是JSON字符串，可以是XML字符串，应区别对待。\n可以发送请求体数据的请求方式有 POST、PUT、PATCH、DELETE。\nDjango默认开启了CSRF防护，会对上述请求方式进行CSRF防护验证，在测试时可以关闭CSRF防护机制，方法为在settings.py文件中注释掉CSRF中间件，如：\n4.1 表单类型 Form Data前端发送的表单类型的请求体数据，可以通过request.POST属性获取，返回QueryDict对象。\n12345678910def post(request):    a = request.POST.get(&#x27;a&#x27;)    b = request.POST.get(&#x27;b&#x27;)    alist = request.POST.getlist(&#x27;a&#x27;)    print(a)    print(b)    print(alist)    return HttpResponse(&#x27;OK&#x27;)12345678\n\n4.2 非表单类型 Non-Form Data非表单类型的请求体数据，Django无法自动解析，可以通过request.body属性获取最原始的请求体数据，自己按照请求体格式（JSON、XML等）进行解析。request.body返回bytes类型。\n例如要获取请求体中的如下JSON数据\n123&#123;&quot;a&quot;: 1, &quot;b&quot;: 2&#125;1\n\n可以进行如下方法操作：\n1234567891011import jsondef post_json(request):    json_str = request.body    json_str = json_str.decode()  # python3.6 无需执行此步    req_data = json.loads(json_str)    print(req_data[&#x27;a&#x27;])    print(req_data[&#x27;b&#x27;])    return HttpResponse(&#x27;OK&#x27;)123456789\n\n5 请求头可以通过request.META属性获取请求头headers中的数据，request.META为字典类型。\n常见的请求头如：\n\nCONTENT_LENGTH– The length of the request body (as a string).\nCONTENT_TYPE– The MIME type of the request body.\nHTTP_ACCEPT– Acceptable content types for the response.\nHTTP_ACCEPT_ENCODING– Acceptable encodings for the response.\nHTTP_ACCEPT_LANGUAGE– Acceptable languages for the response.\nHTTP_HOST– The HTTP Host header sent by the client.\nHTTP_REFERER– The referring page, if any.\nHTTP_USER_AGENT– The client’s user-agent string.\nQUERY_STRING– The query string, as a single (unparsed) string.\nREMOTE_ADDR– The IP address of the client.\nREMOTE_HOST– The hostname of the client.\nREMOTE_USER– The user authenticated by the Web server, if any.\nREQUEST_METHOD– A string such as”GET”or”POST”.\nSERVER_NAME– The hostname of the server.\nSERVER_PORT– The port of the server (as a string).\n\n具体使用如:\n12345def get_headers(request):    print(request.META[&#x27;CONTENT_TYPE&#x27;])    return HttpResponse(&#x27;OK&#x27;)123\n\n6 其他常用HttpRequest对象属性\nmethod：一个字符串，表示请求使用的HTTP方法，常用值包括：‘GET’、‘POST’。\n\nuser：请求的用户对象。\n\npath：一个字符串，表示请求的页面的完整路径，不包含域名和参数部分。\n\nencoding：一个字符串，表示提交的数据的编码方式。\n如果为None则表示使用浏览器的默认设置，一般为utf-8。这个属性是可写的，可以通过修改它来修改访问表单数据使用的编码，接下来对属性的任何访问将使用新的encoding值。\n\nFILES：一个类似于字典的对象，包含所有的上传文件。\n\n\n\nHttpResponse对象视图在接收请求并处理后，必须返回HttpResponse对象或子对象。HttpRequest对象由Django创建，HttpResponse对象由开发人员创建。\n1 HttpResponse可以使用 django.http.HttpResponse 来构造响应对象。\n123HttpResponse(content=响应体, content_type=响应体数据类型, status=状态码)1\n\n也可通过HttpResponse对象属性来设置响应体、响应体数据类型、状态码：\n\ncontent：表示返回的内容。\nstatus_code：返回的HTTP响应状态码。\ncontent_type：指定返回数据的的MIME类型。\n\n响应头可以直接将HttpResponse对象当做字典进行响应头键值对的设置：\n1234response = HttpResponse()response[&#x27;itcast&#x27;] = &#x27;Python&#x27;  # 自定义响应头Itcast, 值为Python12\n\n示例：\n1234567891011from django.http import HttpResponsedef response(request):    return HttpResponse(&#x27;itcast python&#x27;, status=400)    或者    response = HttpResponse(&#x27;itcast python&#x27;)    response.status_code = 400    response[&#x27;itcast&#x27;] = &#x27;Python&#x27;    return response123456789\n\n2 HttpResponse子类Django提供了一系列HttpResponse的子类，可以快速设置状态码\n\nHttpResponseRedirect 301\nHttpResponsePermanentRedirect 302\nHttpResponseNotModified 304\nHttpResponseBadRequest 400\nHttpResponseNotFound 404\nHttpResponseForbidden 403\nHttpResponseNotAllowed 405\nHttpResponseGone 410\nHttpResponseServerError 500\n\n3 JsonResponse若要返回json数据，可以使用JsonResponse来构造响应对象，作用：\n\n帮助我们将数据转换为json字符串\n设置响应头Content-Type为application/json\n\n12345from django.http import JsonResponsedef response(request):    return JsonResponse(&#123;&#x27;city&#x27;: &#x27;beijing&#x27;, &#x27;subject&#x27;: &#x27;python&#x27;&#125;)1234\n\n4 redirect重定向1234from django.shortcuts import redirectdef response(request):    return redirect(&#x27;/get_header&#x27;)\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Django URLconf，路由命名与reverse反解析（逆向）","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/06/01/Django URLconf，路由命名与reverse反解析（逆向）/","content":"URLconf\n浏览者通过在浏览器的地址栏中输入网址请求网站\n对于Django开发的网站，由哪一个视图进行处理请求，是由url匹配找到的\n\n配置URLconf\nsettings.py中\n指定url配置\n12ROOT_URLCONF = &#x27;项目.urls&#x27;1\n\n项目中urls.py\n匹配成功后，包含到应用的urls.py\n12url(正则, include(&#x27;应用.urls&#x27;))1\n\n应用中urls.py\n匹配成功后，调用views.py对应的函数\n12url(正则, views.函数名)1\n\n提示\n1234567891011121314151. 正则部分推荐使用 r，表示字符串不转义，这样在正则表达式中使用 \\ 只写一个就可以2. 不能在开始加反斜杠，推荐在结束加反斜杠    正确：path/    正确：path    错误：/path    错误：/path/3. 请求的url被看做是一个普通的python字符串，进行匹配时不包括域名、get或post参数    3.1 如请求地址如下：        http://127.0.0.1:8000/18/?a=10    3.2 去掉域名和参数部分后，只剩下如下部分与正则匹配        18/1234567891011121314\n\n\n\n路由命名与reverse反解析（逆向）1. 路由命名在定义路由的时候，可以为路由命名，方便查找特定视图的具体路径信息。\n\n在使用include函数定义路由时，可以使用namespace参数定义路由的命名空间，如\n12url(r&#x27;^&#x27;,include(&#x27;book.urls&#x27;,namespace=&#x27;book&#x27;))1\n\n命名空间表示，凡是book.urls中定义的路由，均属于namespace指明的book名下。命名空间的作用：避免不同应用中的路由使用了相同的名字发生冲突，使用命名空间区别开。\n\n在定义普通路由时，可以使用name参数指明路由的名字，如\n1234567urlpatterns = [    url(r&#x27;^$&#x27;,index),    # 匹配书籍列表信息的URL,调用对应的bookList视图    url(r&#x27;^booklist/$&#x27;,bookList,name=&#x27;index&#x27;),    url(r&#x27;^testproject/$&#x27;,views.testproject,name=&#x27;test&#x27;),]123456\n\n\n\n2. reverse反解析使用reverse函数，可以根据路由名称，返回具体的路径，如：\n12345678910111213141516from django.core.urlresolvers import reverse#或者from django.urls import reversedef testproject(request):    return HttpResponse(&quot;OK&quot;)# 定义视图：提供书籍列表信息def bookList(request):    url = reverse(&#x27;book:test&#x27;)    print(url)    return HttpResponse(&#x27;index&#x27;)123456789101112131415\n\n\n对于未指明namespace的，reverse(路由name)\n对于指明namespace的，reverse(命名空间namespace:路由name)\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Django 视图介绍","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/05/26/Django 视图介绍/","content":"视图介绍和项目准备视图介绍\n\n视图就是应用中views.py文件中的函数\n视图的第一个参数必须为HttpRequest对象，还可能包含下参数如通过正则表达式组获取的位置参数通过正则表达式组获得的关键字参数\n视图必须返回一个HttpResponse对象或子对象作为响应子对象： JsonResponse HttpResponseRedirect\n视图负责接受Web请求HttpRequest，进行逻辑处理，返回Web响应HttpResponse给请求者响应内容可以是HTML内容，404错误，重定向，json数据…\n视图处理过程如下图：\n\n\n使用视图时需要进行两步操作，两步操作不分先后\n\n配置URLconf\n在应用/views.py中定义视图\n\n详细操作：视图和URL: https://blog.csdn.net/weixin_44685869/article/details/105353646\n项目准备项目准备整体流程：https://blog.csdn.net/weixin_44685869/article/details/105362452\n","categories":["djangobook"],"tags":["python"]},{"title":"Django Manager管理器","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/05/19/Django Manager管理器/","content":"管理器Manager管理器是Django的模型进行数据库操作的接口，Django应用的每个模型类都拥有至少一个管理器。\n我们在通过模型类的objects属性提供的方法操作数据库时，即是在使用一个管理器对象objects。当没有为模型类定义管理器时，Django会为每一个模型类生成一个名为objects的管理器，它是models.Manager类的对象。\n自定义管理器我们可以自定义管理器，并应用到我们的模型类上。\n1注意：一旦为模型类指明自定义的过滤器后，Django不再生成默认管理对象objects。\n\n自定义管理器类主要用于两种情况\n\n准备工作：把bookinfo表中的一条记录is_delete字段修改成True\n问题：逻辑删除字段为True的那条记录依然会被查询出来\n\n\n修改原始查询集，重写all()方法。\na、 打开book/models.py文件，定义类BookInfoManager\n1234567class BookInfoManager(models.Manager):    def all(self):        #默认查询未删除的图书信息        #调用父类的成员语法为：super().方法名        return super().filter(is_delete=False)\n\nb、 在模型类BookInfo中定义管理器\n1234class BookInfo(models.Model):    ...    books = BookInfoManager()\n\nc、 使用方法\n1234&gt;&gt;&gt; books = BookInfo.books.all()&gt;&gt;&gt; books&lt;QuerySet [&lt;BookInfo: 天龙八部&gt;, &lt;BookInfo: 笑傲江湖&gt;, &lt;BookInfo: 雪山飞狐&gt;, &lt;BookInfo: python入门&gt;]&gt;\n\nd、 视图方法\n12345678910# 书籍列表信息视图  def bookList(request):    # 查询数据库书籍列表数据    # books = BookInfo.objects.all()    books = BookInfo.books.all()    # 构造上下文    context = &#123;&#x27;books&#x27;:books&#125;    # 数据交给模板处理，处理完成后通过视图响应给客户端    return render(request, &#x27;Book/booklist.html&#x27;, context)\n\n在管理器类中补充定义新的方法\na、 打开book/models.py文件，定义方法create。\n123456789class BookInfoManager(models.Manager):    def create_book(self,name,pub_date):        # 创建模型类对象self.model可以获得模型类        book = self.model()        book.name = name        book.pub_date = pub_date        book.save()        return book\n\nb、 为模型类BookInfo定义管理器books语法如下\n1234class BookInfo(models.Model):      ...    books = BookInfoManager()\n\nc、 调用语法如下：\n1234&gt;&gt;&gt; from book.models import BookInfo&gt;&gt;&gt; book = BookInfo.books.create_book(&#x27;python高级&#x27;,&#x27;2010-1-1&#x27;)&gt;&gt;&gt; book&lt;BookInfo: python高级&gt;\n\n\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Python自省机制","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/05/17/Python自省机制/","content":"什么是自省？在日常生活中，自省（introspection）是一种自我检查行为。\n在计算机编程中，自省是指这种能力：检查某些事物以确定它是什么、它知道什么以及它能做什么。自省向程序员提供了极大的灵活性和控制力。\n\n说的更简单直白一点：\n自省就是面向对象的语言所写的程序在运行时，能够知道对象的类型。简单一句就是，运行时能够获知对象的类型。\n\n例如：\npython, buby, object-C, c++都有自省的能力，这里面的c++的自省的能力最弱，只能够知道是什么类型，而像python可以知道是什么类型，还有什么属性。\n\n\n最好的理解自省就是通过例子： [Type introspectionhttps://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Type_introspection\n](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Type_introspection)这里是各种编程语言中自省（introspection）的例子。链接里面满满的干货\nPython 中比较常见的自省（introspection）机制(函数用法)有： dir()，type(), hasattr(), isinstance()，通过这些函数，我们能够在程序运行时得知对象的类型，判断对象是否存在某个属性，访问对象的属性。\ndir()dir() 函数可能是 Python 自省机制中最著名的部分了。它返回传递给它的任何对象的属性名称经过排序的列表。如果不指定对象，则 dir() 返回当前作用域中的名称。让我们将 dir() 函数应用于 keyword 模块，并观察它揭示了什么：\n1234&gt;&gt;&gt; import keyword&gt;&gt;&gt; dir(keyword)[&#x27;__all__&#x27;, &#x27;__builtins__&#x27;, &#x27;__doc__&#x27;, &#x27;__file__&#x27;, &#x27;__name__&#x27;, &#x27;__package__&#x27;, &#x27;iskeyword&#x27;, &#x27;kwlist&#x27;, &#x27;main&#x27;] 123\n\ntype()type() 函数有助于我们确定对象是字符串还是整数，或是其它类型的对象。它通过返回类型对象来做到这一点，可以将这个类型对象与 types 模块中定义的类型相比较：\n12345&gt;&gt;&gt; type(42)&lt;class &#x27;int&#x27;&gt;&gt;&gt;&gt; type([])&lt;class &#x27;list&#x27;&gt;1234\n\nhasattr()对象拥有属性，并且 dir() 函数会返回这些属性的列表。但是，有时我们只想测试一个或多个属性是否存在。如果对象具有我们正在考虑的属性，那么通常希望只检索该属性。这个任务可以由 hasattr() 和 getattr() 函数来完成.\n123&gt;&gt;&gt; hasattr(id, &#x27;__doc__&#x27;)True12\n\nisinstance()可以使用 isinstance() 函数测试对象，以确定它是否是某个特定类型或定制类的实例：\n123&gt;&gt;&gt; isinstance(&quot;python&quot;, str)True12\n\n参考原址：https://www.cnblogs.com/ArsenalfanInECNU/p/9110262.html\n","categories":["python"],"tags":["python"]},{"title":"Python 单元测试","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/05/10/Python 单元测试/","content":"Python 中有一个经常使用的测试框架 unitest\n什么是unittestunittest是Python单元测试框架，Java也有个类似的 叫 JUnit。\nunitest主要功能模块介绍unitest主要包含TestCase、TestSuite、TestLoader、TextTestRunner、TextTestResult这几个功能模块。\n\nTestCase：一个 TestCase 实例就是一个测试用例，一个测试用例就是一个完整的测试流程，包括测试前环境的搭建，测试代码的执行，以及测试后环境的还原或者销毁。元测试的本质也就在这里，一个测试用例是一个完整的测试单元，可以对某一具体问题进行检查验证。\nTestSuite：多个测试用例集合在一起就是 TestSuite，TestSuite 也可以嵌套TestSuite。\nTestLoader：TestLoader 的作用是将 Testcase 加载到 TestSuite 中。\nTextTestRunner：TextTestRunner 是用来执行测试用例的，其中的run(test)会执行 TestSuite/TestCase 中的 run(result) 方法。\nTextTestResult：TextTestResult 用来保存测试结果，其中包括运行了多少测试用例，成功了多少，失败了多少等信息。\n\n整个流程为：写好 TestCase，然后由 TestLoader 加载 TestCase 到 TestSuite，然后由 TextTestRunner 来运行 TestSuite，运行的结果保存在 TextTestResult 中。\n实例介绍\n首先准备几个待测的方法，写在 test_func.py中。\n\n1234567891011121314151617# test_func.pydef add(a, b):    return a + bdef multi(a, b):    return a * bdef lower_str(string):    return string.lower()def square(x):    return x ** 212345678910111213141516\n\n\n准备好几个待测的方法之后，为这些方法写一个测试用例,写入 our_testcase.py中。\n\n123456789101112131415161718192021222324252627282930313233# our_testcase.pyimport unittestfrom test_func import *   # 测试使用，所以可以 *class TestFunc(unittest.TestCase):    &quot;&quot;&quot;Test test_func.py&quot;&quot;&quot;    def test_add(self):        &quot;&quot;&quot;Test func add&quot;&quot;&quot;        self.assertEqual(3, add(1, 2))      # 1 + 2 = 3        self.assertNotEqual(3, add(1, 3))    # 1 + 3 != 3    def test_multi(self):        &quot;&quot;&quot;Test func multi&quot;&quot;&quot;        self.assertEqual(6, multi(2, 3))      # 2 * 3 = 6        self.assertNotEqual(8, multi(3, 3))      # 3 * 3 != 8    def test_lower_str(self):        &quot;&quot;&quot;Test func lower_str&quot;&quot;&quot;        self.assertEqual(&quot;abc&quot;, lower_str(&quot;ABC&quot;))     # ABC 小写后是 abc        self.assertNotEqual(&quot;Dce&quot;, lower_str(&quot;DCE&quot;))      # DCE 小写不是 Dce    def test_square(self):        &quot;&quot;&quot;Test func square&quot;&quot;&quot;        self.assertEqual(17, square(4))  # 这里故意设计一个会出错的用例，测试4的平方等于17，实际上并不等于。        self.assertNotEqual(35, square(6))     # 6的2次方不是 35if __name__ == &#x27;__main__&#x27;:    unittest.main()1234567891011121314151617181920212223242526272829303132\n\n这里写好之后，进入命令行终端，执行python our_testcase.py，执行结果如下。\n123456789101112131415...F======================================================================FAIL: test_square (__main__.TestFunc)Test func square----------------------------------------------------------------------Traceback (most recent call last):  File &quot;our_testcase.py&quot;, line 27, in test_square    self.assertEqual(17, square(4))AssertionError: 17 != 16----------------------------------------------------------------------Ran 4 tests in 0.000sFAILED (failures=1)1234567891011121314\n\n这里分析一下这个执行结果。首先能够看到一共运行了4个测试用例，失败了1个，并且给出了失败原因，AssertionError: 17 != 16，这是我们故意留下的错误漏洞，被测试用例测试出来了。\n第一行…F中，一个点.代表测试成功，F代表失败，我们的测试结果中，前三个成功了，第四个失败了，总共是四个测试，其余的符号中E代表出错，S代表跳过。\n1特别说明的一点是，测试的执行顺序跟方法的顺序没有关系，四个测试是随机先后执行的。\n\n每个测试方法编写的时候，都要以 test开头，比如test_square，否则是不被unitest识别的。\n进阶，查看程序在unitest.main()中加上verbosity参数可以控制输出的错误报告的详细程序，默认是1，如果设为0，则不输出每一用例的执行结果，即上面的第一行的执行结果内容。如果设为2，则输出详细的执行结果。\n\n修改our_testcase.py中主函数。\n\n123if __name__ == &#x27;__main__&#x27;:    unittest.main(verbosity=2)12\n\n\n执行结果如下。\n\n1234567891011121314151617181920212223test_add (__main__.TestFunc)Test func add ... oktest_lower_str (__main__.TestFunc)Test func lower_str ... oktest_multi (__main__.TestFunc)Test func multi ... oktest_square (__main__.TestFunc)Test func square ... FAIL======================================================================FAIL: test_square (__main__.TestFunc)Test func square----------------------------------------------------------------------Traceback (most recent call last):  File &quot;our_testcase.py&quot;, line 27, in test_square    self.assertEqual(17, square(4))AssertionError: 17 != 16----------------------------------------------------------------------Ran 4 tests in 0.000sFAILED (failures=1)12345678910111213141516171819202122\n\n可以看到，每一个用例的详细执行情况以及用例名，用例描述均被输出了出来，在测试方法下加代码示例中的 Doc String，在用例执行时，会将该字符串作为此用例的描述，加合适的注释能够使输出的测试报告更加便于阅读。\n一、组织TestSuite 顺序执行按照上面的测试方法，我们无法控制用例执行的顺序，这样显然是不合理的，因为在一些测试过程中，我们肯定需要控制先测试某些用例，再测试某些用例，这些用例有先后的因果关系。在这里，我们就需要用到TestSuite。我们添加到TestSuite中的case是会按照添加的顺序执行的。\n还有一个问题是，我们现在只有一个测试文件，我们直接执行该文件即可，但如果有多个测试文件，怎么进行组织，总不能一个个文件执行吧，答案也在TestSuite中。\n\n新建一个文件，test_suite.py。\n\n123456789101112# test_suite.pyimport unittestfrom our_testcase import TestFuncif __name__ == &#x27;__main__&#x27;:    suite = unittest.TestSuite()    tests = [TestFunc(&quot;test_square&quot;), TestFunc(&quot;test_lower_str&quot;), TestFunc(&quot;test_multi&quot;)]    suite.addTests(tests)    runner = unittest.TextTestRunner(verbosity=2)    runner.run(suite)1234567891011\n\n执行结果如下。\n123456789101112131415161718192021test_square (our_testcase.TestFunc)Test func square ... FAILtest_lower_str (our_testcase.TestFunc)Test func lower_str ... oktest_multi (our_testcase.TestFunc)Test func multi ... ok======================================================================FAIL: test_square (our_testcase.TestFunc)Test func square----------------------------------------------------------------------Traceback (most recent call last):  File &quot;/Users/luyuze/projects/test/our_testcase.py&quot;, line 27, in test_square    self.assertEqual(17, square(4))AssertionError: 17 != 16----------------------------------------------------------------------Ran 3 tests in 0.000sFAILED (failures=1)1234567891011121314151617181920\n\n这样，用例执行的顺序就是按照我们添加进去的顺序来执行的了。\n上面使用的是TestSuite的addTests()方法，并直接传入TestCase列表，也有一些其他的方法可以向TestSuite中添加用例。\n12345678910111213# 直接用addTest方法添加单个TestCasesuite.addTest(TestMathFunc(&quot;test_multi&quot;))# 使用loadTestFromName,传入模块名.TestCase名，下面俩方法效果相同suite.addTests(unittest.TestLoader().loadTestsFromName(&#x27;our_testcase.TestFunc&#x27;))suite.addTests(unittest.TestLoader().loadTestsFromNames([&#x27;our_testcase.TestFunc&#x27;]))# loadTestsFromTestCase()，传入TestCasesuite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestFunc))1234567891011\n\n用TestLoader的方法是无法对case进行排序的，同时，suite中也可以套suite。\n二、输出到文件用例组织好了，但是结果只能输出到控制台，这样没办法查看之前的执行记录，我们想将结果输出到文件。\n\n修改test_suite.py。\n\n123456789101112131415# test_suite.pyimport unittestfrom our_testcase import TestFuncif __name__ == &#x27;__main__&#x27;:    suite = unittest.TestSuite()    tests = [TestFunc(&quot;test_square&quot;), TestFunc(&quot;test_lower_str&quot;), TestFunc(&quot;test_multi&quot;)]    suite.addTests(tests)    with open(&#x27;UnitestTextReport.txt&#x27;, &#x27;a&#x27;) as f:        runner = unittest.TextTestRunner(stream=f, verbosity=2)        runner.run(suite)12345678910111213\n\n三、测试前后的处理在之前的测试中，可能会存在这样的问题：如果要在测试之前准备环境，测试完成之后做一些清理怎么办？这里需要用到的是setUp()和tearDown()。\n\n修改our_testcase.py。\n\n12345678910111213141516171819202122232425262728293031323334353637383940# our_testcase.pyimport unittestfrom test_func import *class TestFunc(unittest.TestCase):    &quot;&quot;&quot;Test test_func.py&quot;&quot;&quot;    def setUp(self):        print(&quot;do something before testcase&quot;)    def test_add(self):        &quot;&quot;&quot;Test func add&quot;&quot;&quot;        self.assertEqual(3, add(1, 2))        self.assertNotEqual(3, add(1, 3))    def test_multi(self):        &quot;&quot;&quot;Test func multi&quot;&quot;&quot;        self.assertEqual(6, multi(2, 3))        self.assertNotEqual(8, multi(3, 3))    def test_lower_str(self):        &quot;&quot;&quot;Test func lower_str&quot;&quot;&quot;        self.assertEqual(&quot;abc&quot;, lower_str(&quot;ABC&quot;))        self.assertNotEqual(&quot;Dce&quot;, lower_str(&quot;DCE&quot;))    def test_square(self):        &quot;&quot;&quot;Test func square&quot;&quot;&quot;        self.assertEqual(17, square(4))        self.assertNotEqual(35, square(6))    def tearDownClass(self):        print(&quot;do something after testcase&quot;)if __name__ == &#x27;__main__&#x27;:    unittest.main(verbosity=2)1234567891011121314151617181920212223242526272829303132333435363738\n\n\n执行结果：\n\n1234567891011121314151617181920212223242526272829303132test_add (__main__.TestFunc)Test func add ... do something before testcasedo something after testcaseoktest_lower_str (__main__.TestFunc)Test func lower_str ... do something before testcasedo something after testcaseoktest_multi (__main__.TestFunc)Test func multi ... do something before testcasedo something after testcaseoktest_square (__main__.TestFunc)Test func square ... do something before testcasedo something after testcaseFAIL======================================================================FAIL: test_square (__main__.TestFunc)Test func square----------------------------------------------------------------------Traceback (most recent call last):  File &quot;our_testcase.py&quot;, line 30, in test_square    self.assertEqual(17, square(4))AssertionError: 17 != 16----------------------------------------------------------------------Ran 4 tests in 0.001sFAILED (failures=1)123456789101112131415161718192021222324252627282930\n\n可以发现 setUp() 和 tearDown() 在每个 case 前后都执行了一次。如果要在所有 case 执行之前和所有 case 执行之后准备和清理环境，我们可以使用 setUpClass() 与 tearDownClass()。\n123456789101112class TestFunc(unittest.TestCase):    &quot;&quot;&quot;Test test_func.py&quot;&quot;&quot;    @classmethod    def setUpClass(cls):        print &quot;This setUpClass() method only called once.&quot;    @classmethod    def tearDownClass(cls):        print &quot;This tearDownClass() method only called once too.&quot;12345678910\n\n四、跳过case如果我们临时想要跳过某个 case 不执行，unitest 也有相应的方法。\n\n1、skip装饰器\n\n1234567891011121314151617181920212223242526272829303132333435# our_testcase.pyimport unittestfrom test_func import *class TestFunc(unittest.TestCase):    &quot;&quot;&quot;Test test_func.py&quot;&quot;&quot;    @unittest.skip(&#x27;do not run this case&#x27;)    def test_add(self):        &quot;&quot;&quot;Test func add&quot;&quot;&quot;        self.assertEqual(3, add(1, 2))        self.assertNotEqual(3, add(1, 3))    def test_multi(self):        &quot;&quot;&quot;Test func multi&quot;&quot;&quot;        self.assertEqual(6, multi(2, 3))        self.assertNotEqual(8, multi(3, 3))    def test_lower_str(self):        &quot;&quot;&quot;Test func lower_str&quot;&quot;&quot;        self.assertEqual(&quot;abc&quot;, lower_str(&quot;ABC&quot;))        self.assertNotEqual(&quot;Dce&quot;, lower_str(&quot;DCE&quot;))    def test_square(self):        &quot;&quot;&quot;Test func square&quot;&quot;&quot;        self.assertEqual(17, square(4))        self.assertNotEqual(35, square(6))if __name__ == &#x27;__main__&#x27;:    unittest.main(verbosity=2)123456789101112131415161718192021222324252627282930313233\n\n执行结果：\n123456789101112131415161718192021222324test_add (__main__.TestFunc)Test func add ... skipped &#x27;do not run this case&#x27;test_lower_str (__main__.TestFunc)Test func lower_str ... oktest_multi (__main__.TestFunc)Test func multi ... oktest_square (__main__.TestFunc)Test func square ... FAIL======================================================================FAIL: test_square (__main__.TestFunc)Test func square----------------------------------------------------------------------Traceback (most recent call last):  File &quot;our_testcase.py&quot;, line 28, in test_square    self.assertEqual(17, square(4))AssertionError: 17 != 16----------------------------------------------------------------------Ran 4 tests in 0.000sFAILED (failures=1, skipped=1)12345678910111213141516171819202122\n\n结果显示为，总共执行4个测试，1个失败，1个被跳过。\nskip装饰器一共有三个:\n\nunittest.skip(reason)\nunittest.skipIf(condition, reason)\nunittest.skipUnless(condition, reason)\n\nskip无条件跳过，skipIf 当 condition 为 True 时跳过，skipUnless当condition 为 False 时跳过。\n\n2、TestCase.skipTest()方法\n\n1234567891011# our_testcase.pyclass TestFunc(unittest.TestCase):    &quot;&quot;&quot;Test test_func.py&quot;&quot;&quot;    def test_add(self):        &quot;&quot;&quot;Test func add&quot;&quot;&quot;        self.skipTest(&quot;do not run this case&quot;)        self.assertEqual(3, add(1, 2))        self.assertNotEqual(3, add(1, 3))12345678910\n\n效果与第一种是一样的。\n参考地址：https://zhuanlan.zhihu.com/p/95687793\n","categories":["python"],"tags":["python"]},{"title":"Python 中 uwsgi 获取不到全局变量","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/05/03/Python 中 uwsgi 获取不到全局变量/","content":"在使用 uwsgi 启动Python 脚本，或者项目的时候，会出现 访问不到 全局变量的问题。大家都知道 uwsgi 是可以指定子进程的数目的，我设置了进程数 为1，那么 我有两个函数，那么他只会处理一个（在同一时刻）。uwsgi为了线程的并发，独立运行了多个进程，所以他具有多个全局变量。\n\nuwsgi 的工作方式是启动指定个进程监听端口，接收到请求时就去执行对应的 uwsgi 入口文件，然后将结果返回给指定监听的端口。\n当涉及到在一个 python 中修改全局变量或者多个线程访问一个变量的时候会不能获取到全局变量的修改。因为每次请求都会重新执行一遍 python 脚本，从而每次都会重新初始化全局变量。\n\n解决办法将全局变量放置在进程里已经不可取了，所以将其拿出来，放入一个“公共访问区域”，让所有进程访问并修改该“公共访问区域”的全局变量。\n\n目前该“公共访问区域”可以有多种实现方式：\n\n\n使用现成的数据库：redis，mongodb，mysql等;\n自己实现一个管理进程对数据进行管理(参考multiprocessing模块);\n利用 uwsgi 内置的缓存功能\n\n这里着重讲解第3种方式解决,可以最大限度的不依赖与第三方工具,并且借助 uwsgi 内置的方法.\n12345关于“新一代”的缓存，它由uWSGI 1.9引入。 对于旧式缓存 (现在简单称其为“web缓存”)，可以看看 WebCaching框架1uWSGI包含了一个非常快速、全内存访问、零IPC、SMP安全、不断优化、高度可调的、键值存储的简单的“缓存框架”。单个uWSGI实例可以使用不同的设置，出于不同的目的，创建无限个不同的“缓存”。12\n\n\n通过如下命令启动 uwsgi:\n\n12uwsgi --cache2 name=mycache,items=100 --socket :30311\n\n这将会创建一个名为”mycache”的缓存，它最多拥有100个项。每个项最大可以是64k。\n\n或者在 uwsgi 配置文件中添加如下配置(第一行是注释说明)\n\n123; create a cache with 100 items (default size per-item is 64k)cache2 = name=mycache,items=100,blocksize=500000012\n\n配置创建了一个 名为 mycache 的缓存,它最多拥有 100 个项,每个项最大可以是:5MB, 其中 blocksize 用来设置单个缓存的大小,单位是字节(Byte)\n在添加好 uwsgi 缓存后就可以在 python 脚本中设置 需要公共访问的变量了,通过引入 uwsgi 模块并且调用下面的方法便可以使用 uwsgi 缓存.\n\ncache_get(key[,cache])\ncache_set(key,value[,expires,cache])\ncache_update(key,value[,expires,cache])\ncache_exists(key[,cache])\ncache_del(key[,cache])\ncache_clear([cache])\n\n如果调用该缓存 API 的语言/平台区分了字符串和字节 (例如 Python 3和Java)，那么你必须假设键是字符串，而值是字节 (或者在java之下，是字节数组)。否则，键和值都是无特定编码的字符串，因为在内部，缓存值和缓存键都是简单的二进制blob。\n\nexpires 参数 (默认为0，表示禁用) 是对象失效的秒数 (并当未设置 purge_lru 的时候，由缓存清道夫移除，见下)\ncache 参数是所谓的“魔法标识符”，它的语法是 cache[@node]\n\n具体配置方式可以参考:\n\nuWSGI 缓存烹饪指南：https://uwsgi-docs-zh.readthedocs.io/zh_CN/latest/tutorials/CachingCookbook.html\nuWSGI 缓存框架https://uwsgi-docs-zh.readthedocs.io/zh_CN/latest/Caching.html\n\n除此之外,推荐使用 uwsgi_cache 这个模块,这个模块利用了 python 本身的 pickle 模块对 uwsgi 中的cache api 进行了简单的封装,使得 uwsgi 可以进行缓存 python 的对象,在使用是需要注意的是避免缓存 pickle 不支持的对象,如 python 模块或者打开的文件等.具体原因不在此处讲解,可以参考 python pickle 模块 这篇博文.\nuwsgi_cache 这个模块本身提供了两个方法,具体内容可以参考 uwsgi_cache 主页\n\n使用 uwsgi_cache 模块完成缓存: 首先配置缓存:\n\n12uwsgi test.py --cache2 name=api,items=10001\n\n然后在 test.py 文件中调用 uwsgi 缓存 api 进行全局缓存\n123456789101112131415161718192021222324from uwsgi_cache.cache import CacheManager#        :params:#            name: the name of the cache that you would like to create#            expires: the expire time of the cache data# 这里需要传入两个参数 ,name 为缓存的名字,# expires 为缓存的有效期, 0 表示禁用(即该缓存永不失效)cache = CacheManager(&quot;cache_name_01&quot;, 0)# 调用缓存方法cache = 0if not cache.exists(&quot;count&quot;):\tcache.set(&quot;count&quot;,0)\tprint(&quot;count value is %d&quot;,0)else:\tcount = cache.get(&quot;count&quot;)\tcount +=1\tprint(&quot;count value is %d&quot;,count)\tcache.set(&quot;count&quot;,count)def application(environ, start_response):\tstatus = &#x27;200 OK&#x27;\tresponse_body = &quot;count value is %d&quot;,count\tresponse_header = [(&#x27;Content-Type&#x27;, &#x27;text/html&#x27;)]\tstart_response(status, response_header)\treturn [response_body.encode()]1234567891011121314151617181920212223\n\n启动uwsgi,便会发现 count 的值会随着每次请求调用而不断增加.\n12uwsgi test.py --cache2 name=api,items=10001\n\n参考原址:https://my.oschina.net/ghimi/blog/2705982\n","categories":["python"],"tags":["python"]},{"title":"Python *args 和 **kwargs","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/04/30/Python 的args 和 kwargs/","content":"在 python 中，args 和 **kwargs 都代表 1个 或 多个 参数的意思。args 传入tuple 类型的无名参数，而 **kwargs 传入的参数是 dict 类型。下文举例说明。\n\n*args 的用法，以下测试代码：\n\n123456789101112def test(*args):      print(args)    for i in args:        print(i)test(1,2,3)# 输出值分别为# (1,2,3)# 1# 2# 3\n\n\n**kwargs 的用法，以下测试代码：\n\n12345678910111213def test(**kwargs):    print(kwargs)    keys = kwargs.keys()    value = kwargs.values()    print(keys)    print(value)test(a=1,b=2,c=3,d=4)# 输出值分别为# &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3, &#x27;d&#x27;: 4&#125;# dict_keys([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;])# dict_values([1, 2, 3, 4])\n\n参考原址：https://www.cnblogs.com/shenh/p/10487626.html\n","categories":["python"],"tags":["python"]},{"title":"什么是 编程？","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/04/26/什么是 编程？/","content":"\n什么是编程语言？编程语言（programming language）可以简单的理解为一种计算机和人都能识别的语言。一种计算机语言让程序员能够准确地定义计算机所需要使用的数据，并精确地定义在不同情况下所应当采取的行动。\n编程语言种类计算机编程语言能够实现人与机器之间的交流和沟通，而计算机编程语言主要包括汇编语言、机器语言以及高级语言，具体内容如下：\n\n汇编语言\n该语言主要是以缩写英文作为标符进行编写的，运用汇编语言进行编写的一般都是较为简练的小程序，其在执行方面较为便利，但汇编语言在程序方面较为冗长，所以具有较高的出错率。\n\n机器语言\n这种语言主要是利用二进制编码进行指令的发送，能够被计算机快速地识别，其灵活性相对较高，且执行速度较为可观，机器语言与汇编语言之间的相似性较高，但由于具有局限性，所以在使用上存在一定的约束性。\n\n高级语言\n所谓的高级语言，其实是由多种编程语言结合之后的总称，其可以对多条指令进行整合，将其变为单条指令完成输送，其在操作细节指令以及中间过程等方面都得到了适当的简化，所以，整个程序更为简便，具有较强的操作性，而这种编码方式的简化，使得计算机编程对于相关工作人员的专业水平要求不断放宽。\n\n\n编程语言的种类就是这样 ，然后 由于编程语言种类的繁多，诞生的时间线不同，根据时间和编程语言的特性，又分类如下几类：\n编程语言一般分为：低级语言、高级语言和面向对象时代。\n低级语言时代1低级语言时代（1946－1953）主要包括被称为“天书”的机器语言以及汇编语言。\n\n计算机工作基于二进制，从根本上说，计算机只能识别和接受由0和1组成的指令。这些指令的集合就是该计算机的机器语言。机器语言包括的缺点有：难学、难写、难记、难检查、难修改，难以推广使用。因此初期只有极少数的计算机专业人员会编写计算机程序。\n汇编语言由于机器语言的难以理解，莫奇莱等人开始想到用助记符来代替0，1代码，于是汇编语言出现了。\n高级语言时代高级语言时代（1954－至今）——随着世界上第一个高级语言fortran的出现，新的编程语言开始不断涌现出来。数十年来，全世界涌现了2500种以上高级语言，一些流行至今，一些则逐渐消失。\n\n第一个高级语言—Fortran。\n为了克服低级语言的缺点，20世纪50年代由美国约翰·贝克斯（John Backus）创造出了第一个计算机高级语言——FORTRAN语言。它很接近人们习惯使用的自然语言和数学语言。程序中所用运算符和运算表达式，很容易理解，使用也十方便。并且FORTRAN以其特有的功能在数值、科学和工程计算领域发挥着重要作用。\n\n第一个结构化程序设计语言—ALGOL\n这是在计算机发展史上首批清晰定义的高级语言，由欧美计算机学家合力所组成的联席大会于仍是晶体管计算机流行的1950年代所开发国际计算机学会（ACM）将ALGOL模式列为算法描述的标准，启发ALGOL类现代语言Pascal、Ada、C语言等出现。\n\n最简单的语言——BASIC\n1964年BASIC语言正式发布。是由达特茅斯学院院长、匈牙利人约翰·凯梅尼（John G.Kemeny）与数学系教师托马斯·库尔茨（Thomas E.Kurtz）共同研制出来的。该语言只有26个变量名，17条语句，12个函数和3个命令。这门语言叫做“初学者通用符号指令代码。\n\n编程语言里一个重要的里程碑——Pascal Pasca这是基于ALGOL编程语言，为纪念法国数学家、哲学家、电脑先驱布莱兹·帕斯卡而命名。它由瑞士Niklaus Wirth教授于六十年代末设计并创立的。Pascal具有语法严谨、层次分明等特点，是第一个结构化编程语言，被称为“编程语言里一个重要的里程碑”。\n\n现代程序语言革命的起点——C语言C语言的祖先是BCPL（Basic Combined Programming Language）语言，在1970年美国贝尔实验室的Ken Thompson在BCPL语言的基础上，设计出了B语言。接着在1972到1973年间，美国贝尔实验室的Dennis M.Ritchie在Ken Thompson的基础上设计出了C语言。\n\n\n面向对象时代面向对象时代（90年代初－至今）——面向对象程序设计（Object－Oriented Programming，简称OOP）如今在整个程序设计中十分重要，其最突出的特点为封装性、继承性和多态性。\n\nJava\n\nJava是由Sun Microsystem于1995年推出的高级编程语言。近几年来，Java企业级应用飞速发展，主要被运用于电信、金融、交通等行业的信息化平台建设。Java是一个普遍适用的软件平台，其具有易学易用、平台独立、可移植、多线程、健壮、动态、安全等主要特性。\n\nPython\n\n近几年来，Python语言上升势头比较迅速，其主要原因在于大数据和人工智能领域的发展，随着产业互联网的推进，Python语言未来的发展空间将进一步得到扩大。Python是一种高层次的脚本语言，目前应用于Web和Internet开发、科学计算和统计、教育、软件开发和后端开发等领域，且有着简单易学、运行速度快、可移植、可扩展、可嵌入等优点。\n","categories":["编程"],"tags":["python"]},{"title":"Python 介绍","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/04/20/Python 介绍/","content":"Python 是一种解释型、面向对象、动态数据类型的高级程序设计语言。\nPython 由 Guido van Rossum (吉多·范 罗苏姆，龟叔) 于 1989 年底发明，第一个公开发行版发行于 1991 年。\n像 Perl 语言一样, Python 源代码同样遵循 GPL(GNU General Public License) 协议。\n官方宣布，2020 年 1 月 1 日， 停止 Python 2 的更新。\nPython 2.7 被确定为最后一个 Python 2.x 版本。\nPython 成长史可爱的他 -诞生：1989年，为了打发圣诞节假期，龟叔开始写Python语言的编译器。Python这个名字，来自龟叔所挚爱的电视剧Monty Python’s Flying Circus（巨蟒剧团的飞行的马戏团）。\n他希望这个新的叫做Python的语言，能符合他的理想：创造一种C和shell之间，功能全面，易学易用，可拓展的语言。龟叔作为一个语言设计爱好者，已经有过设计语言的尝试。这一次，也不过是一次纯粹的即兴行为。\n他的 -成长经历：最初的Python完全由龟叔本人开发。Python得到龟叔同事的欢迎。他们迅速的反馈使用意见，并参与到Python的改进。\n龟叔和一些同事构成Python的核心团队。他们将自己大部分的业余时间用于改进 Python。随后，Python拓 展到研究所之外。\nPython将许多机器层面上的细节隐藏，交给编译器处理，并凸显出逻辑层面的编程思考。Python程序员可以花更多的时间用于思考程序的逻辑，而不是具体的实现细节。这一特征吸引了广大的程序员。Python开始流行。\n随着计算机硬件越来越强大，Python又容易使用，所以许多人开始转向Python。龟叔维护了一个mail list，Python用户就通过邮件进行交流。 Python用户来自许多领域，有不同的背景，对Python也有不同的需求。Python相当的开放又容易拓展，所以当用户不满足于现有功能，很容易对Python进行拓展或改造。\n随后这些用户将改动发给龟叔，并由龟叔决定是否将新的特征加入到Python或者标准库中。如果代码能被纳入Python自身或者标准库，这将极大的荣誉。由于龟叔至高无上的决定权，他因此被称为“仁慈的独裁者”\n他的 -成就Python以及其标准库的功能强大。这些是整个社区的贡献。Python的开发者来自不同领域，他们将不同领域的优点带给Python。\n比如Python标准库中的正则表达是参考Perl，而lambda, map, filter, reduce等函数参考了Lisp。Python本身的一些功能以及大部分的标准库来自于社区。Python的社 区不断扩大，进而拥有了自己的网站以及基金。\n从Python 2.0开始，Python也从mail list的开发方式，转为完全开源的开发方式。社区气氛已经形成，工作被整个社区分担，于此同时，Python也获得了更加高速的发展。\n到今天，Python的框架已经确立。Python语言以对象为核心组织代码，支持多种编程范式，采用动态类型，自动进行内存回收。Python支持解释运行，并能调用C库进行拓展。\nPython有强大的标准库。由于标准库的体系已经稳定，所以Python的生态系统开始拓展到第三方包。这些包，如Django、Flask、numpy、matplotlib、PIL，将Python升级成了物种丰富的‘热带雨林’。\nPython 国内发展史Python在被设计之后，一直是不温不火的状态。龟叔 在2005年加入了 google阵营，2012年离开。在 google带了7年，导致了一个结果。\n\ngoogle 的人大量的使用python。正是因为 这个原因，带动了一些其他的公司开始使用Python，大海在时代开启。\n\n2005年国内建立了一个影响中国影坛的公司，至今 很多人都依靠 他对电影的评分来作为基础，来对电影进行初步的认识 打分和评价。没错这就是 豆瓣，豆瓣 的网站正好也是大量的使用 Python来开发的，不能说他是国内第一个使用的公司，但的确是 python融入国内的 里程碑。\n2012年云计算兴起，毕竟那个时候 如果你想搞个网站起码 10W +，不干别的 就买服务器 要做双机热备 负载均衡，还要预留维护资金，等等等… 所以云计算 也站在了风口上。\n比如：\n\n阿里云， Amazon 这些公有云\n虚拟机 为代表的私有云\n\n随着云计算的兴起，一个开源的云计算管理平台项目加入到了人们的视角 –&gt; OpenStack\n12OpenStack为私有云和公有云提供可扩展的弹性的云计算服务。项目目标是提供实施简单、可大规模扩展、丰富、标准统一的云计算管理平台。1\n\n这个 OpenStack 就是用 Python语言开发的，这样呢 Python就又火了一下。\n2014年Artificial Intelligence 人工智能的兴起，Python 又一次的火了一波，为什么AI兴起 Python火大家都知道吧。不知道这里: https://blog.csdn.net/qq_44929388/article/details/89421247\n说白了还是代码少(开玩笑啊，这只是一方面)，大家都知道 AI的基础是靠什么？算法 + 数据。Python虽然是脚本语言，但是因为容易学，迅速成为科学家的工具（MATLAB也能搞科学计算，但是软件要钱，且很贵），从而积累了大量的工具库、架构，人工智能涉及大量的数据计算，用Python是很自然的，简单高效。Python有非常多优秀的深度学习库可用，现在大部分深度学习框架都支持Python。\n2017年在这个时候 Python就已经可以说是 大众视野了，虽然用的人没有 特别多，但是 说出来 Python，程序员都知道。我也就是在这个时候 接触了 她，学习了她 一直用到了现在。你不知道你就是 lowB，你可以不知道区块链 但是你得知道 Python，为什么？因为BTB 在国内倒台了。\n主要领域\nPython-Web公司代表 知乎\n网络编程\n爬虫\n云计算代表：OpenStack\nAI\n自动化运维\n科学运算97年开始 NASA就开始用 Python大量的进行科学运算代表作：引力波的发现程序\n游戏开发代表作：文明\n\n正因为 Python能够接触这么多的领域，所以 Python大家都叫他 万能语言。\n那些大厂都在使用Python\nGoogle\nCIA\nNASA\nYouTube\nDropbox\nInstagram\nFacebook\nRedhat\n豆瓣\n知乎\n春雨医生\n搜狐\n金山\n腾讯\n盛大\n网易\n百度\n阿里\n淘宝\n土豆\n新浪\n果壳\n\n各个版本1999年诞生了第一个 Python-Web 框架 Zope\n\nPython 1.0 - 1994.1 增加了 lambda, map, filter, reduce\nPython 2.0 - 2000.10 加入内存回收机制，构成了现在 Python语言框架的基础\nPython 2.4 - 2004.11 同年 Django诞生\nPython 2.5 - 2006.19\nPython 2.6 - 2008.10\nPython 2.7 - 2010.7\n\n在这里大家看到，Python3.0 是在 Python 2.7之前发布的，这里没有写错啊。\n因为 当时Python是由 龟叔 + 社区工作者 + 广大用户 一起来维护的，但是慢慢地龟叔发现 Python 越来越像C系 或 Java类型了，但由于植入的功能讷河模块特别多，也不方便整改，所以 龟叔 选择了推出 Python 3.x。\n但因为 Python 2.x (2.7以下) 和Python3互不兼容，你 Py2的代码，用Py3运行不了，导致了 大量的人反对，而且 Python 3.x的支持率也急剧下降。迫于无奈两年后 2010年，龟叔推出了 Python 2.7 来缓和和过度 让大家慢慢地接受Python 3.x。他和 Python 3.x 是可以兼容的 并且龟叔发出声明 2020.1 将不再对 Python 2.7进行更新和维护。\n\nPython 3.0 - 2008.12\nPython 3.1 - 2009.6\nPython 3.2 - 2011.2\nPython 3.3 - 2012.9\nPython 3.4 - 2014.3\nPython 3.5 - 2015.9\nPython 3.6 - 2016.12\nPython 3.7 - 2018.2\nPython 3.8 - 2019.1\n\n","categories":["python"],"tags":["python"]},{"title":"安装 简单使用 HeyUI","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/04/10/安装 简单使用 HeyUI/","content":"我们需要有 Vue.js 环境，要提前安装好，\nHeyUI参考使用文档：https://www.heyui.top/component/quickstart\n\n支持环境\n现代浏览器和 IE9 及以上。\n\n兼容\nHeyUI支持 Vue.js 2.x版本\n\n\n安装 步骤：\n安装heyui\n12cnpm install heyui --save-dev\n\n安装less\n12cnpm install less --save-dev\n\n安装less-loader\n12cnmp install less-loader --save-dev\n\n\n\n注：Less 是一门 CSS 预处理语言,它扩展了 CSS 语言,增加了变量、Mixin、函数等特性。Less 可以运行在 Node 或浏览器端。 是必需品\n安装 需要用的之后，我们就要 全局配置了。\n样式引用 在全局的 main.js 里\n\n12345678910// main.js// 引入 HeyUIimport HeyUI from &#x27;heyui&#x27;vue.use(HeyUI)// 导入 css 和 js 样式，全局require(&quot;heyui/themes/index.css&quot;);import &quot;heyui/themes/index.less&quot;;\n\n\n同时，HeyUI 也可以设置为全局引用，因为 HeyUI 拥有Message, Loadding等全局可以调用的方法。\n\n12345678new Vue(&#123;  el: &#x27;#app&#x27;,  router,  render: h =&gt; h(App),      // 新增  components: &#123; App &#125;,  template: &#x27;&lt;App/&gt;&#x27;&#125;);\n\n我们还可以 按需使用借助插件 babel-plugin-import 可以实现按需加载组件，减少文件体积。\n12npm install babel-plugin-import --save-dev1\n\n随便选择个样式，复制代码。\n\n\n大家可以看到效果实现了，具体的如何操作还是 观看，官方文档 。\nHeyUI还有 强大的 admin，连接 参考文档：https://heyui.github.io/heyui-admin-docs/\n","categories":["python"],"tags":["python"]},{"title":"Django 的 QuerySet结果集，两大特性 惰性查询，限制查询集","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/03/19/Django 的 QuerySet结果集，两大特性 惰性查询，限制查询集/","content":"查询集QuerySet1 概念Django的ORM中存在查询集的概念。\n查询集，也称查询结果集、QuerySet，表示从数据库中获取的对象集合。\n当调用如下过滤器方法时，Django 会返回查询集（而不是简单的列表）：\n\nall()：返回所有数据。\nfilter()：返回满足条件的数据。\nexclude()：返回满足条件之外的数据。\norder_by()：对结果进行排序。\n\n对查询集可以再次调用过滤器进行过滤，如\n12345books = BookInfo.objects.filter(readcount__gt=30).order_by(&#x27;pub_date&#x27;)books&lt;QuerySet [&lt;BookInfo: 天龙八部&gt;, &lt;BookInfo: 雪山飞狐&gt;]&gt;1234\n\n也就意味着查询集可以含有零个、一个或多个过滤器。过滤器基于所给的参数限制查询的结果。\n从SQL的角度讲，查询集与select语句等价，过滤器像where、limit、order by子句。\n判断某一个查询集中是否有数据：\n\nexists()：判断查询集中是否有数据，如果有则返回True，没有则返回False。\n\n2 两大特性1. 惰性执行创建查询集不会访问数据库，直到调用数据时，才会访问数据库，调用数据的情况包括迭代、序列化、与if合用\n例如，当执行如下语句时，并未进行数据库查询，只是创建了一个查询集books\n12books = BookInfo.objects.all()1\n\n继续执行遍历迭代操作后，才真正的进行了数据库的查询\n123for book in books:\tprint(book.name)12\n\n2. 缓存使用同一个查询集，第一次使用时会发生数据库的查询，然后Django会把结果缓存下来，再次使用这个查询集时会使用缓存的数据，减少了数据库的查询次数。\n\n情况一：如下是两个查询集，无法重用缓存，每次查询都会与数据库进行一次交互，增加了数据库的负载。\n123456from book.models import BookInfo[book.id for book in BookInfo.objects.all()][book.id for book in BookInfo.objects.all()]12345\n\n\n\n\n\n情况二：经过存储后，可以重用查询集，第二次使用缓存中的数据。\n123456books=BookInfo.objects.all()[book.id for book in books][book.id for book in books]12345\n\n\n\n\n3 限制查询集可以对查询集进行取下标或切片操作，等同于sql中的limit和offset子句。\n1注意：不支持负数索引。\n\n对查询集进行切片后返回一个新的查询集，不会立即执行查询。\n如果获取一个对象，直接使用[0]，等同于[0:1].get()，但是如果没有数据，[0]引发IndexError异常，[0:1].get()如果没有数据引发DoesNotExist异常。\n\n示例：获取第1、2项，运行查看。\n1234books = BookInfo.objects.all()[0:2]books&lt;QuerySet [&lt;BookInfo: 射雕英雄传&gt;, &lt;BookInfo: 天龙八部&gt;]&gt;\n\n\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Django 聚合函数 和 排序函数，关联查询","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/03/13/Django 聚合函数 和 排序函数，关联查询/","content":"一下介绍的数据和字段 都在此文中进行的 创建和添加 http://localhost:4000/2020/02/16/Django%20%E6%A8%A1%E5%9E%8B%E7%B1%BB%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%8C%E8%B6%85%E7%BA%A7%E8%AF%A6%E7%BB%86%E7%89%88%E6%9C%AC/\n1. 聚合函数使用aggregate()过滤器调用聚合函数。聚合函数包括：Avg平均，Count数量，Max最大，Min最小，Sum求和，被定义在django.db.models中。\n\n例：查询图书的总阅读量。\n12345from django.db.models import SumBookInfo.objects.aggregate(Sum(&#x27;readcount&#x27;))&#123;&#x27;readcount__sum&#x27;: 126&#125;\n\n\n\n123注意aggregate的返回值是一个字典类型，格式如下：&#123;&#39;属性名__聚合类小写&#39;:值&#125;1\n\n如:{‘readcount__sum’: 126}\n使用count 时一般不使用aggregate()过滤器。\n\n例：查询图书总数。\n12BookInfo.objects.count()\n\n\n\n注意count函数的返回值是一个数字。\n2. 排序使用order_by对结果进行排序\n1234567891011# 默认升序BookInfo.objects.all().order_by(&#x27;readcount&#x27;)&lt;QuerySet [&lt;BookInfo: 射雕英雄传&gt;, &lt;BookInfo: 笑傲江湖&gt;, &lt;BookInfo: 天龙八部&gt;, &lt;BookInfo: 雪山飞狐&gt;]&gt;# 降序BookInfo.objects.all().order_by(&#x27;-readcount&#x27;)&lt;QuerySet [&lt;BookInfo: 雪山飞狐&gt;, &lt;BookInfo: 天龙八部&gt;, &lt;BookInfo: 笑傲江湖&gt;, &lt;BookInfo: 射雕英雄传&gt;]&gt;\n\n\n\n\n关联查询查询书籍为1的所有人物信息查询人物为1的书籍信息\n\n由一到多的访问语法：\n一对应的模型类对象.多对应的模型类名小写_set 例：\n12345book = BookInfo.objects.get(id=1)book.peopleinfo_set.all()&lt;QuerySet [&lt;PeopleInfo: 郭靖&gt;, &lt;PeopleInfo: 黄蓉&gt;, &lt;PeopleInfo: 黄药师&gt;, &lt;PeopleInfo: 欧阳锋&gt;, &lt;PeopleInfo: 梅超风&gt;]&gt;\n\n由多到一的访问语法:\n多对应的模型类对象.多对应的模型类中的关系类属性名 例：\n12345person = PeopleInfo.objects.get(id=1)person.book&lt;BookInfo: 射雕英雄传&gt;\n\n\n\n访问一对应的模型类关联对象的id语法:多对应的模型类对象.关联类属性_id\n\n例：\n12345person = PeopleInfo.objects.get(id=1)person.book_id\n\n\n\n关联过滤查询由多模型类条件查询一模型类数据:\n语法如下：\n关联模型类名小写__属性名__条件运算符=值\n1注意：如果没有&quot;__运算符&quot;部分，表示等于。\n\n查询图书，要求图书人物为”郭靖”查询图书，要求图书中人物的描述包含”八”\n\n例：查询图书，要求图书人物为”郭靖”\n123456book = BookInfo.objects.filter(peopleinfo__name=&#x27;郭靖&#x27;)book&lt;QuerySet [&lt;BookInfo: 射雕英雄传&gt;]&gt;\n\n查询图书，要求图书中人物的描述包含”八”\n12345book = BookInfo.objects.filter(peopleinfo__description__contains=&#x27;八&#x27;)book&lt;QuerySet [&lt;BookInfo: 射雕英雄传&gt;, &lt;BookInfo: 天龙八部&gt;]&gt;\n\n\n\n由一模型类条件查询多模型类数据:语法如下：\n一模型类关联属性名__一模型类属性名__条件运算符=值\n1注意：如果没有&quot;__运算符&quot;部分，表示等于。\n\n查询书名为“天龙八部”的所有人物查询图书阅读量大于30的所有人物\n\n例：查询书名为“天龙八部”的所有人物。\n12345people = PeopleInfo.objects.filter(book__name=&#x27;天龙八部&#x27;)people&lt;QuerySet [&lt;PeopleInfo: 乔峰&gt;, &lt;PeopleInfo: 段誉&gt;, &lt;PeopleInfo: 虚竹&gt;, &lt;PeopleInfo: 王语嫣&gt;]&gt;\n\n查询图书阅读量大于30的所有人物\n1234people = PeopleInfo.objects.filter(book__readcount__gt=30)people&lt;QuerySet [&lt;PeopleInfo: 乔峰&gt;, &lt;PeopleInfo: 段誉&gt;, &lt;PeopleInfo: 虚竹&gt;, &lt;PeopleInfo: 王语嫣&gt;\n\n\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Django 高级查询 F Q查询","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/03/06/Django 高级查询 F Q查询/","content":"F对象之前的查询都是对象的属性与常量值比较，两个属性怎么比较呢？ 答：使用F对象，被定义在django.db.models中。\n语法如下：\n12F(属性名)1\n\n\n例：查询阅读量大于等于评论量的图书。\n12345from django.db.models import FBookInfo.objects.filter(readcount__gt=F(&#x27;commentcount&#x27;))&lt;QuerySet [&lt;BookInfo: 雪山飞狐&gt;]&gt;1234\n\n\n\n可以在F对象上使用算数运算。\n\n例：查询阅读量大于2倍评论量的图书。\n123BookInfo.objects.filter(readcount__gt=F(&#x27;commentcount&#x27;)*2)&lt;QuerySet [&lt;BookInfo: 雪山飞狐&gt;]&gt;12\n\n\n\n\n\nQ对象多个过滤器逐个调用表示逻辑与关系，同sql语句中where部分的and关键字。\n\n例：查询阅读量大于20，并且编号小于3的图书。\n123BookInfo.objects.filter(readcount__gt=20,id__lt=3)&lt;QuerySet [&lt;BookInfo: 天龙八部&gt;]&gt;12\n\n或者\n123BookInfo.objects.filter(readcount__gt=20).filter(id__lt=3)&lt;QuerySet [&lt;BookInfo: 天龙八部&gt;]&gt;12\n\n\n\n如果需要实现逻辑或or 的查询，需要使用Q()对象结合 | 运算符，Q对象被义在django.db.models中。\n语法如下：\n12Q(属性名__运算符=值)1\n\n\n例：查询阅读量大于20的图书，改写为Q对象如下。\n12BookInfo.objects.filter(Q(readcount__gt=20))1\n\n\n\nQ对象可以使用&amp;、|连接，&amp;表示逻辑与，|表示逻辑或。\n\n例：查询阅读量大于20，或编号小于3的图书，只能使用Q对象实现\n123BookInfo.objects.filter(Q(readcount__gt=20)|Q(id__lt=3))&lt;QuerySet [&lt;BookInfo: 射雕英雄传&gt;, &lt;BookInfo: 天龙八部&gt;, &lt;BookInfo: 雪山飞狐&gt;]&gt;12\n\n\n\nQ对象前可以使用~操作符，表示非not。\n\n例：查询编号不等于3的图书。\n12BookInfo.objects.filter(~Q(id=3))&lt;QuerySet [&lt;BookInfo: 射雕英雄传&gt;, &lt;BookInfo: 天龙八部&gt;, &lt;BookInfo: 雪山飞狐&gt;]&gt;\n\n\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Django 中数据库的操作，多种查询方法","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/02/28/Django 中数据库的操作，多种查询方法/","content":"增加增加数据有两种方法。\n\nsave\n通过创建模型类对象，执行对象的save()方法保存到数据库中。\n123456789from book.models import BookInfo,PeopleInfobook = BookInfo(        name=&#x27;python入门&#x27;,        pub_date=&#x27;2010-1-1&#x27;    )book.save()book&lt;BookInfo: python入门&gt;\n\n\n\ncreate\n通过模型类.objects.create()保存。\n1234567PeopleInfo.objects.create(        name=&#x27;itheima&#x27;,        book=book    )&lt;PeopleInfo: itheima&gt;\n\n\n\n修改修改更新有两种方法\n\nsave\n修改模型类对象的属性，然后执行save()方法\n1234567person = PeopleInfo.objects.get(name=&#x27;itheima&#x27;)person.name = &#x27;itcast&#x27;person.save()person&lt;PeopleInfo: itcast&gt;\n\nupdate\n使用模型类.objects.filter().update()，会返回受影响的行数\n12PeopleInfo.objects.filter(name=&#x27;itcast&#x27;).update(name=&#x27;H_sen&#x27;) \n\n\n\n删除删除有两种方法\n\n模型类对象delete\n\n1234person = PeopleInfo.objects.get(name=&#x27;H_sen&#x27;)person.delete()(1, &#123;&#x27;book.PeopleInfo&#x27;: 1&#125;)\n\n\n模型类.objects.filter().delete()\n\n1234BookInfo.objects.filter(name=&#x27;python入门&#x27;).delete()(1, &#123;&#x27;book.BookInfo&#x27;: 1, &#x27;book.PeopleInfo&#x27;: 0&#125;)\n\n\n查询基本查询\nget 查询单一结果，如果不存在会抛出模型类.DoesNotExist异常。\nall 查询多个结果。\ncount 查询结果数量。\n\n123456789101112131415161718192021BookInfo.objects.get(id=1)&lt;BookInfo: 射雕英雄传&gt;BookInfo.objects.get(pk=2)&lt;BookInfo: 天龙八部&gt;BookInfo.objects.get(pk=20)Traceback (most recent call last):  File &quot;&lt;console&gt;&quot;, line 1, in &lt;module&gt;  File &quot;/home/python/.virtualenvs/py3_django_1.11/lib/python3.5/site-packages/django/db/models/manager.py&quot;, line 85, in manager_method    return getattr(self.get_queryset(), name)(*args, **kwargs)  File &quot;/home/python/.virtualenvs/py3_django_1.11/lib/python3.5/site-packages/django/db/models/query.py&quot;, line 380, in get    self.model._meta.object_namebook.models.DoesNotExist: BookInfo matching query does not exist.BookInfo.objects.all()&lt;QuerySet [&lt;BookInfo: 射雕英雄传&gt;, &lt;BookInfo: 天龙八部&gt;, &lt;BookInfo: 笑傲江湖&gt;, &lt;BookInfo: 雪山飞狐&gt;]&gt;BookInfo.objects.count()\n\n过滤查询实现SQL中的where功能，包括\n\nfilter过滤出多个结果\nexclude排除掉符合条件剩下的结果\nget过滤单一结果\n\n对于过滤条件的使用，上述三个方法相同，故仅以filter进行讲解。\n过滤条件的表达语法如下：\n123456789101112属性名称__比较运算符=值# 属性名称和比较运算符间使用两个下划线，所以属性名不能包括多个下划线查询编号为1的图书查询书名包含&#x27;湖&#x27;的图书查询书名以&#x27;部&#x27;结尾的图书查询书名为空的图书查询编号为1或3或5的图书查询编号大于3的图书查询1980年发表的图书查询1990年1月1日后发表的图书\n\n1. 相等exact：表示判等。\n例：查询编号为1的图书。\n12345BookInfo.objects.filter(id__exact=1)可简写为：BookInfo.objects.filter(id=1)\n\n2. 模糊查询contains：是否包含。\n说明：如果要包含%无需转义，直接写即可。\n例：查询书名包含’传’的图书。\n123BookInfo.objects.filter(name__contains=&#x27;传&#x27;)&lt;QuerySet [&lt;BookInfo: 射雕英雄传&gt;]&gt;\n\nstartswith、endswith：以指定值开头或结尾。\n例：查询书名以’部’结尾的图书\n1234567BookInfo.objects.filter(name__endswith=&#x27;部&#x27;)&lt;QuerySet [&lt;BookInfo: 天龙八部&gt;]&gt;以上运算符都区分大小写，在这些运算符前加上i表示不区分大小写，如iexact、icontains、istartswith、iendswith.\n\n3. 空查询isnull：是否为null。\n例：查询书名为空的图书。\n1234BookInfo.objects.filter(name__isnull=True)&lt;QuerySet []&gt;\n\n4. 范围查询in：是否包含在范围内。\n例：查询编号为1或3或5的图书\n1234BookInfo.objects.filter(id__in=[1,3，5])&lt;QuerySet [&lt;BookInfo: 射雕英雄传&gt;, &lt;BookInfo: 笑傲江湖&gt;]&gt;\n\n5. 比较查询\ngt大于 (greater then)\ngte大于等于 (greater then equal)\nlt小于 (less then)\nlte小于等于 (less then equal)\n\n例：查询编号大于3的图书\n12BookInfo.objects.filter(id__gt=3)\n\n不等于的运算符，使用exclude()过滤器。\n例：查询编号不等于3的图书\n123BookInfo.objects.filter(id__gt=3)&lt;QuerySet [&lt;BookInfo: 雪山飞狐&gt;]&gt;\n\n6. 日期查询year、month、day、week_day、hour、minute、second：对日期时间类型的属性进行运算。\n例：查询1980年发表的图书。\n123BookInfo.objects.filter(pub_date__year=1980)&lt;QuerySet [&lt;BookInfo: 射雕英雄传&gt;]&gt;\n\n例：查询1990年1月1日后发表的图书。\n12BookInfo.objects.filter(pub_date__gt=&#x27;1990-1-1&#x27;)&lt;QuerySet [&lt;BookInfo: 笑傲江湖&gt;]&gt;\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Django 中的 shell命令","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/02/24/Django 中的 shell命令/","content":"shell工具Django的manage工具提供了shell命令，帮助我们配置好当前工程的运行环境（如连接好数据库等），以便可以直接在终端中执行测试python语句。\n通过如下命令进入shell\n12python manage.py shell1\n\n导入两个模型类，以便后续使用\n12from book.models import BookInfo,PeopleInfo1\n\n此文的shell 是 manage工具的 一条命令\n还记得之前创建的管理员嘛，用的也是 manage命令，那我们说说这个 manage\nDjango的命令行工具django-admin.py是Django的一个用于管理任务的命令行工具，manage.py是对django-admin.py的简单包装，每个Django Project里面都会包含一个manage.py\n语法：\n123django-admin.py &lt;subcommand&gt; [options]manage.py &lt;subcommand&gt; [options]12\n\nsubcommand是子命令；options是可选的\n1234567891011121314151617181920# 常用子命令：startproject:创建一个项目（*）startapp:创建一个app（*）runserver：运行开发服务器（*）shell：进入django shell（*）dbshell：进入django dbshellcheck：检查django项目完整性flush：清空数据库compilemessages：编译语言文件makemessages：创建语言文件makemigrations：生成数据库同步脚本（*）migrate：同步数据库（*）showmigrations：查看生成的数据库同步脚本（*）sqlflush：查看生成清空数据库的脚本（*）sqlmigrate：查看数据库同步的sql语句（*）dumpdata:导出数据loaddata:导入数据diffsettings:查看你的配置和django默认配置的不同之处12345678910111213141516171819\n\nmanage.py 特有的一些子命令：\n1234createsuperuser:创建超级管理员（*）changepassword:修改密码（*）clearsessions：清除session123\n\n更改开发服务器的端口：\n12python manage.py runserver 80801\n\n查看帮助文档：\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647python manage.py help1D:\\day70class&gt;python manage.py helpType &#x27;manage.py help &lt;subcommand&gt;&#x27; for help on a specific subcommand.Available subcommands:[auth]    changepassword    createsuperuser[contenttypes]    remove_stale_contenttypes[django]    check    compilemessages    createcachetable    dbshell    diffsettings    dumpdata    flush    inspectdb    loaddata    makemessages    makemigrations    migrate    sendtestemail    shell    showmigrations    sqlflush    sqlmigrate    sqlsequencereset    squashmigrations    startapp    startproject    test    testserver[sessions]    clearsessions[staticfiles]    collectstatic    findstatic    runserver\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Django 模型类的使用，超级详细版本","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/02/16/Django 模型类的使用，超级详细版本/","content":"定义模型类\n模型类被定义在”应用/models.py”文件中。\n模型类必须继承自Model类，位于包django.db.models中。\n\n接下来首先以”图书-人物”管理为例进行演示。\n1 定义在models.py 文件中定义模型类。\n1234567891011121314151617181920212223242526272829303132333435363738from django.db import models# Create your models here.# 准备书籍列表信息的模型类class BookInfo(models.Model):    # 创建字段，字段类型...    name = models.CharField(max_length=20, verbose_name=&#x27;名称&#x27;)    pub_date = models.DateField(verbose_name=&#x27;发布日期&#x27;,null=True)    readcount = models.IntegerField(default=0, verbose_name=&#x27;阅读量&#x27;)    commentcount = models.IntegerField(default=0, verbose_name=&#x27;评论量&#x27;)    is_delete = models.BooleanField(default=False, verbose_name=&#x27;逻辑删除&#x27;)    class Meta:        db_table = &#x27;bookinfo&#x27;  # 指明数据库表名        verbose_name = &#x27;图书&#x27;  # 在admin站点中显示的名称    def __str__(self):        &quot;&quot;&quot;定义每个数据对象的显示信息&quot;&quot;&quot;        return self.name# 准备人物列表信息的模型类class PeopleInfo(models.Model):    GENDER_CHOICES = (        (0, &#x27;male&#x27;),        (1, &#x27;female&#x27;)    )    name = models.CharField(max_length=20, verbose_name=&#x27;名称&#x27;)    gender = models.SmallIntegerField(choices=GENDER_CHOICES, default=0, verbose_name=&#x27;性别&#x27;)    description = models.CharField(max_length=200, null=True, verbose_name=&#x27;描述信息&#x27;)    book = models.ForeignKey(BookInfo, on_delete=models.CASCADE, verbose_name=&#x27;图书&#x27;)  # 外键    is_delete = models.BooleanField(default=False, verbose_name=&#x27;逻辑删除&#x27;)    class Meta:        db_table = &#x27;peopleinfo&#x27;        verbose_name = &#x27;人物信息&#x27;    def __str__(self):        return self.name\n\n\n数据库表名\n模型类如果未指明表名，Django默认以小写app应用名_小写模型类名为数据库表名。\n可通过db_table指明数据库表名。\n\n关于主键\ndjango会为表创建自动增长的主键列，每个模型只能有一个主键列，如果使用选项设置某属性为主键列后django不会再创建自动增长的主键列。\n默认创建的主键列属性为id，可以使用pk代替，pk全拼为primary key。\n\n属性命名限制\n不能是python的保留关键字。不允许使用连续的下划线，这是由django的查询方式决定的。\n定义属性时需要指定字段类型，通过字段类型的参数指定选项，语法如下：\n12属性=models.字段类型(选项)1\n\n字段类型\n\n\n\n\n\n类型\n说明\n\n\n\nAutoField\n自动增长的IntegerField，通常不用指定，不指定时Django会自动创建属性名为id的自动增长属性\n\n\nBooleanField\n布尔字段，值为True或False\n\n\nNullBooleanField\n支持Null、True、False三种值\n\n\nCharField\n字符串，参数max_length表示最大字符个数\n\n\nTextField\n大文本字段，一般超过4000个字符时使用\n\n\nIntegerField\n整数\n\n\nDecimalField\n十进制浮点数， 参数max_digits表示总位数， 参数decimal_places表示小数位数\n\n\nFloatField\n浮点数\n\n\nDateField\n日期， 参数auto_now表示每次保存对象时，自动设置该字段为当前时间，用于”最后一次修改”的时间戳，它总是使用当前日期，默认为False； 参数auto_now_add表示当对象第一次被创建时自动设置当前时间，用于创建的时间戳，它总是使用当前日期，默认为False; 参数auto_now_add和auto_now是相互排斥的，组合将会发生错误\n\n\nTimeField\n时间，参数同DateField\n\n\nDateTimeField\n日期时间，参数同DateField\n\n\nFileField\n上传文件字段\n\n\nImageField\n继承于FileField，对上传的内容进行校验，确保是有效的图片\n\n\n\n选项\n\n\n\n\n选项\n说明\n\n\n\nnull\n如果为True，表示允许为空，默认值是False\n\n\nblank\n如果为True，则该字段允许为空白，默认值是False\n\n\ndb_column\n字段的名称，如果未指定，则使用属性的名称\n\n\ndb_index\n若值为True, 则在表中会为此字段创建索引，默认值是False\n\n\ndefault\n默认\n\n\nprimary_key\n若为True，则该字段会成为模型的主键字段，默认值是False，一般作为AutoField的选项使用\n\n\nunique\n如果为True, 这个字段在表中必须有唯一值，默认值是False\n\n\nnull是数据库范畴的概念，blank是表单验证范畴的\n\n外键\n在设置外键时，需要通过on_delete选项指明主表删除数据时，对于外键引用表数据如何处理，在django.db.models中包含了可选常量：\n\n\n\nCASCADE级联，删除主表数据时连通一起删除外键表中数据\nPROTECT保护，通过抛出ProtectedError异常，来阻止删除主表中被外键应用的数据\nSET_NULL设置为NULL，仅在该字段null=True允许为null时可用\nSET_DEFAULT设置为默认值，仅在该字段设置了默认值时可用\nSET()设置为特定值或者调用特定方法\nDO_NOTHING不做任何操作，如果数据库前置指明级联性，此选项会抛出IntegrityError异常\n\n2 迁移将模型类同步到数据库中。\n\n生成迁移文件\n12python manage.py makemigrations\n\n同步到数据库中\n12python manage.py migrate\n\n\n\n3 添加测试数据123456789101112131415161718192021222324insert into bookinfo(name, pub_date, readcount,commentcount, is_delete) values(&#x27;射雕英雄传&#x27;, &#x27;1980-5-1&#x27;, 12, 34, 0),(&#x27;天龙八部&#x27;, &#x27;1986-7-24&#x27;, 36, 40, 0),(&#x27;笑傲江湖&#x27;, &#x27;1995-12-24&#x27;, 20, 80, 0),(&#x27;雪山飞狐&#x27;, &#x27;1987-11-11&#x27;, 58, 24, 0);insert into peopleinfo(name, gender, book_id, description, is_delete)  values    (&#x27;郭靖&#x27;, 1, 1, &#x27;降龙十八掌&#x27;, 0),    (&#x27;黄蓉&#x27;, 0, 1, &#x27;打狗棍法&#x27;, 0),    (&#x27;黄药师&#x27;, 1, 1, &#x27;弹指神通&#x27;, 0),    (&#x27;欧阳锋&#x27;, 1, 1, &#x27;蛤蟆功&#x27;, 0),    (&#x27;梅超风&#x27;, 0, 1, &#x27;九阴白骨爪&#x27;, 0),    (&#x27;乔峰&#x27;, 1, 2, &#x27;降龙十八掌&#x27;, 0),    (&#x27;段誉&#x27;, 1, 2, &#x27;六脉神剑&#x27;, 0),    (&#x27;虚竹&#x27;, 1, 2, &#x27;天山六阳掌&#x27;, 0),    (&#x27;王语嫣&#x27;, 0, 2, &#x27;神仙姐姐&#x27;, 0),    (&#x27;令狐冲&#x27;, 1, 3, &#x27;独孤九剑&#x27;, 0),    (&#x27;任盈盈&#x27;, 0, 3, &#x27;弹琴&#x27;, 0),    (&#x27;岳不群&#x27;, 1, 3, &#x27;华山剑法&#x27;, 0),    (&#x27;东方不败&#x27;, 0, 3, &#x27;葵花宝典&#x27;, 0),    (&#x27;胡斐&#x27;, 1, 4, &#x27;胡家刀法&#x27;, 0),    (&#x27;苗若兰&#x27;, 0, 4, &#x27;黄衣&#x27;, 0),    (&#x27;程灵素&#x27;, 0, 4, &#x27;医术&#x27;, 0),    (&#x27;袁紫衣&#x27;, 0, 4, &#x27;六合拳&#x27;, 0);\n\n考虑到数据库 写入速度的效率，所以此时没有考虑到使用 ORM\n","categories":["djangobook"],"tags":["python"]},{"title":"Django 创建使用 整体流程","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/02/10/Django 创建使用 整体流程/","content":"之前的几篇博客 介绍了 Django中的各个详细使用流程，我们 这命令 全部整合 起来，并且 加入数据进行测试。\n整体流程：创建项目 + 创建应用 + 安装应用 + 配置模板路径 + 本地化 + mysql数据库 + URLconf + 视图\n项目准备\n创建项目\n12django-admin startproject bookmanager\n\n创建应用\n12python manager.py startapp book\n\n更换python解释器：按需选择\n123456789# 进入指定虚拟环境which python# python2/home/python/.virtualenvs/py_django/bin/python# python3/home/python/.virtualenvs/py3_django/bin/python\n\n安装应用\n1234567891011INSTALLED_APPS = [    &#x27;django.contrib.admin&#x27;,    &#x27;django.contrib.auth&#x27;,    &#x27;django.contrib.contenttypes&#x27;,    &#x27;django.contrib.sessions&#x27;,    &#x27;django.contrib.messages&#x27;,    &#x27;django.contrib.staticfiles&#x27;,    #添加子应用    &#x27;book.apps.BookConfig&#x27;]\n\n本地化\n12345#设置中文LANGUAGE_CODE = &#x27;zh-Hans&#x27;#亚洲上海时区TIME_ZONE = &#x27;Asia/Shanghai&#x27;\n\n模板路径\n在应用同级目录下,创建templates 模板文件夹，\n然后创建路径\n123456789101112131415TEMPLATES = [    &#123;        &#x27;BACKEND&#x27;: &#x27;django.template.backends.django.DjangoTemplates&#x27;,        &#x27;DIRS&#x27;: [os.path.join(BASE_DIR,&#x27;templates&#x27;)],        &#x27;APP_DIRS&#x27;: True,        &#x27;OPTIONS&#x27;: &#123;            &#x27;context_processors&#x27;: [                &#x27;django.template.context_processors.debug&#x27;,                &#x27;django.template.context_processors.request&#x27;,                &#x27;django.contrib.auth.context_processors.auth&#x27;,                &#x27;django.contrib.messages.context_processors.messages&#x27;,            ],        &#125;,    &#125;,]\n\n项目中匹配urls\n正则 : 路径只要不是admin/就算匹配成功。并包含到应用中的urls.py\n123456789from django.conf.urls import url,includefrom django.contrib import adminurlpatterns = [    url(r&#x27;^admin/&#x27;, admin.site.urls),    #正则为：只要不是 admin/ 就算匹配成功    url(r&#x27;^&#x27;,include(&#x27;book.urls&#x27;))]\n\n应用中匹配 urls.py\n应用中创建 urls.py\n正则 : 路径中包含 booklist/**，就调用视图中对应的 **bookList 函数\n123456789from django.conf.urls import urlfrom book.views import bookListurlpatterns = [    # 匹配书籍列表信息的URL,调用对应的bookList视图    url(r&#x27;^booklist/$&#x27;,bookList)]\n\n准备视图\n123456# 定义视图：提供书籍列表信息def bookList(request):    return HttpResponse(&#x27;OK!&#x27;)\n\n开启服务器, 测试项目\n1234567# 进入项目文件中, 开启项目对应的服务器python manage.py runserver# 浏览器中输入网址http://127.0.0.1:8000/booklist/\n\n\n\n配置在settings.py中保存了数据库的连接配置信息，Django默认初始配置使用sqlite数据库。\n12345678DATABASES = &#123;    &#x27;default&#x27;: &#123;        &#x27;ENGINE&#x27;: &#x27;django.db.backends.sqlite3&#x27;,        &#x27;NAME&#x27;: os.path.join(BASE_DIR, &#x27;db.sqlite3&#x27;),    &#125;&#125;\n\n\n使用MySQL数据库首先需要安装驱动程序\n123使用MySQL数据库首先需要安装驱动程序\n\n在Django的工程同名子目录的init.py文件中添加如下语句\n12345import pymysqlpymysql.install_as_MySQLdb()\n\n作用是让Django的ORM能以mysqldb的方式来调用PyMySQL。\n\n修改DATABASES配置信息\n1234567891011DATABASES = &#123;    &#x27;default&#x27;: &#123;        &#x27;ENGINE&#x27;: &#x27;django.db.backends.mysql&#x27;,        &#x27;HOST&#x27;: &#x27;127.0.0.1&#x27;,  # 数据库主机        &#x27;PORT&#x27;: 3306,  # 数据库端口        &#x27;USER&#x27;: &#x27;root&#x27;,  # 数据库用户名        &#x27;PASSWORD&#x27;: &#x27;mysql&#x27;,  # 数据库用户密码        &#x27;NAME&#x27;: &#x27;book&#x27;  # 数据库名字    &#125;&#125;\n\n在MySQL中创建数据库\n123create database book charset=utf8;1\n\n\n\n然后就是 定义模型，所以创建的使用流程就OK了，如果想看 接下来的模型类，请看下一篇。\nURLConf\nsettings.py中：指定url配置\n\n123ROOT_URLCONF = &#x27;bookmanager.urls&#x27;\n\n\n项目中urls.py：只要不是admin/就匹配成功，包含到应用中的urls.py\n\n12345678910from django.conf.urls import url,includefrom django.contrib import adminurlpatterns = [     url(r&#x27;^admin/&#x27;, include(admin.site.urls)),     # 只要不是‘admin/’就匹配成功，包含到应用中的urls.py     url(r&#x27;^&#x27;, include(&#x27;book.urls&#x27;)), ]\n\n\n应用中urls.py：匹配 testproject/ 成功就调用 views 中的 testproject 函数，测试项目逻辑\n\n12345678from django.conf.urls import urlimport viewsurlpatterns = [    # 匹配`testproject/`成功就调用`views`中的`testproject`函数    url(r&#x27;^testproject/$&#x27;, views.testproject),]\n\n\n视图：测试项目逻辑\n\n1234567from django.http import HttpResponse  # 测试项目逻辑  def testproject(request):      return HttpResponse(&#x27;测试项目逻辑&#x27;)\n\n\n在models.py 文件中定义模型类\n\n12345678910111213141516171819202122232425262728293031323334353637383940from django.db import models# Create your models here.# 准备书籍列表信息的模型类class BookInfo(models.Model):    # 创建字段，字段类型...    name = models.CharField(max_length=20, verbose_name=&#x27;名称&#x27;)    pub_date = models.DateField(verbose_name=&#x27;发布日期&#x27;,null=True)    readcount = models.IntegerField(default=0, verbose_name=&#x27;阅读量&#x27;)    commentcount = models.IntegerField(default=0, verbose_name=&#x27;评论量&#x27;)    is_delete = models.BooleanField(default=False, verbose_name=&#x27;逻辑删除&#x27;)    class Meta:        db_table = &#x27;bookinfo&#x27;  # 指明数据库表名        verbose_name = &#x27;图书&#x27;  # 在admin站点中显示的名称    def __str__(self):        &quot;&quot;&quot;定义每个数据对象的显示信息&quot;&quot;&quot;        return self.name# 准备人物列表信息的模型类class PeopleInfo(models.Model):    GENDER_CHOICES = (        (0, &#x27;male&#x27;),        (1, &#x27;female&#x27;)    )    name = models.CharField(max_length=20, verbose_name=&#x27;名称&#x27;)    gender = models.SmallIntegerField(choices=GENDER_CHOICES, default=0, verbose_name=&#x27;性别&#x27;)    description = models.CharField(max_length=200, null=True, verbose_name=&#x27;描述信息&#x27;)    book = models.ForeignKey(BookInfo, on_delete=models.CASCADE, verbose_name=&#x27;图书&#x27;)  # 外键    is_delete = models.BooleanField(default=False, verbose_name=&#x27;逻辑删除&#x27;)    class Meta:        db_table = &#x27;peopleinfo&#x27;        verbose_name = &#x27;人物信息&#x27;    def __str__(self):        return self.name1234567891011121314151617181920212223242526272829303132333435363738\n\n\n生成迁移文件\n123python manage.py makemigrations\n\n同步到数据库中\n123python manage.py migrate\n\n添加测试数据\n12345insert into bookinfo(name, pub_date, readcount,commentcount, is_delete) values(&#x27;射雕英雄传&#x27;, &#x27;1980-5-1&#x27;, 12, 34, 0),(&#x27;天龙八部&#x27;, &#x27;1986-7-24&#x27;, 36, 40, 0),(&#x27;笑傲江湖&#x27;, &#x27;1995-12-24&#x27;, 20, 80, 0),(&#x27;雪山飞狐&#x27;, &#x27;1987-11-11&#x27;, 58, 24, 0);\n\n123456789101112131415161718insert into peopleinfo(name, gender, book_id, description, is_delete)  values    (&#x27;郭靖&#x27;, 1, 1, &#x27;降龙十八掌&#x27;, 0),    (&#x27;黄蓉&#x27;, 0, 1, &#x27;打狗棍法&#x27;, 0),    (&#x27;黄药师&#x27;, 1, 1, &#x27;弹指神通&#x27;, 0),    (&#x27;欧阳锋&#x27;, 1, 1, &#x27;蛤蟆功&#x27;, 0),    (&#x27;梅超风&#x27;, 0, 1, &#x27;九阴白骨爪&#x27;, 0),    (&#x27;乔峰&#x27;, 1, 2, &#x27;降龙十八掌&#x27;, 0),    (&#x27;段誉&#x27;, 1, 2, &#x27;六脉神剑&#x27;, 0),    (&#x27;虚竹&#x27;, 1, 2, &#x27;天山六阳掌&#x27;, 0),    (&#x27;王语嫣&#x27;, 0, 2, &#x27;神仙姐姐&#x27;, 0),    (&#x27;令狐冲&#x27;, 1, 3, &#x27;独孤九剑&#x27;, 0),    (&#x27;任盈盈&#x27;, 0, 3, &#x27;弹琴&#x27;, 0),    (&#x27;岳不群&#x27;, 1, 3, &#x27;华山剑法&#x27;, 0),    (&#x27;东方不败&#x27;, 0, 3, &#x27;葵花宝典&#x27;, 0),    (&#x27;胡斐&#x27;, 1, 4, &#x27;胡家刀法&#x27;, 0),    (&#x27;苗若兰&#x27;, 0, 4, &#x27;黄衣&#x27;, 0),    (&#x27;程灵素&#x27;, 0, 4, &#x27;医术&#x27;, 0),    (&#x27;袁紫衣&#x27;, 0, 4, &#x27;六合拳&#x27;, 0);\n\n\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Django 模板，配置文件，静态资源","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/01/30/Django 模板，配置文件，静态资源/","content":"模板 Template1思考 : 网站如何向客户端返回一个漂亮的页面呢？\n\n提示 :\n123漂亮的页面需要html、css、js.可以把这一堆字段串全都写到视图中, 作为HttpResponse()的参数,响应给客户端.\n\n问题 :\n1234视图部分代码臃肿, 耦合度高.这样定义的字符串是不会出任何效果和错误的.效果无法及时查看.有错也不容易及时发现.\n\n设想 :\n12是否可以有一个专门定义前端页面的地方, 效果可以及时展示,错误可以及时发现,并且可以降低模块间耦合度!1\n\n解决问题 :模板\n12MVT设计模式中的T,Template\n\n在Django中, 将前端的内容定义在模板中, 然后再把模板交给视图调用, 各种漂亮、炫酷的效果就出现了.\n模板使用步骤\n创建模板\n设置模板查找路径\n模板接收视图传入的数据\n模板处理数据\n\n1.创建模板\n在应用同级目录下创建模板文件夹templates. 文件夹名称固定写法.\n在templates文件夹下, 创建应用同名文件夹. 例, Book\n在应用同名文件夹下创建网页模板文件. 例 :index.html\n\n\n2.设置模板查找路径\n3.模板接收视图传入的数据\n视图模板加载\n\n\n\n4.模板处理数据\n\n查看模板处理数据成果\n\n\n总结View-Templates流程\n\n配置文件 settings.py1. BASE_DIR12BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))1\n\n当前工程的根目录，Django会依此来定位工程内的相关文件，我们也可以使用该参数来构造文件路径。\n2. DEBUG 调试模式，创建工程后初始值为True，即默认工作在调试模式下。作用：\n\n修改代码文件，程序自动重启\n\nDjango程序出现异常时，向前端显示详细的错误追踪信息，例如\n\n\n而非调试模式下，仅返回Server Error (500)\n\n\n1注意：部署线上运行的Django不要运行在调式模式下，记得修改DEBUG&#x3D;False和ALLOW_HOSTS。\n\n3. 本地语言与时区Django支持本地化处理，即显示语言与时区支持本地化。\n本地化是将显示的语言、时间等使用本地的习惯，这里的本地化就是进行中国化，中国大陆地区使用简体中文，时区使用亚洲/上海时区，注意这里不使用北京时区表示。\n初始化的工程默认语言和时区为英语和UTC标准时区\n12345# settings.pyLANGUAGE_CODE = &#x27;en-us&#x27;  # 语言TIME_ZONE = &#x27;UTC&#x27;  # 时区# 时区\n\n将语言和时区修改为中国大陆信息\n123LANGUAGE_CODE = &#x27;zh-Hans&#x27;TIME_ZONE = &#x27;Asia/Shanghai&#x27;\n\n静态文件项目中的CSS、图片、js都是静态文件。一般会将静态文件放到一个单独的目录中，以方便管理。在html页面中调用时，也需要指定静态文件的路径，Django中提供了一种解析的方式配置静态文件路径。静态文件可以放在项目根目录下，也可以放在应用的目录下，由于有些静态文件在项目中是通用的，所以推荐放在项目的根目录下，方便管理。\n为了提供静态文件，需要配置两个参数：\n\nSTATICFILES_DIRS存放查找静态文件的目录\nSTATIC_URL访问静态文件的URL前缀\n\n示例1） 在项目根目录下创建static目录来保存静态文件。\n2） 在bookmanager/settings.py中修改静态文件的两个参数为\n1234567# settings.pySTATIC_URL = &#x27;/static/&#x27;STATICFILES_DIRS = [    os.path.join(BASE_DIR, &#x27;static&#x27;),]\n\n3）此时在static添加的任何静态文件都可以使用网址 /static/文件在static 中的路径来访问了。\n例如，我们向static目录中添加一个index.html文件，在浏览器中就可以使用 127.0.0.1:8000/static/index.html来访问。\n或者我们在static目录中添加了一个子目录和文件book/detail.html，在浏览器中就可以使用127.0.0.1:8000/static/book/detail.html来访问。\nApp应用配置在每个应用目录中都包含了apps.py文件，用于保存该应用的相关信息。\n在创建应用时，Django会向apps.py文件中写入一个该应用的配置类，如\n12345from django.apps import AppConfigclass BookConfig(AppConfig):    name = &#x27;book&#x27;\n\n我们将此类添加到工程 settings.py 中的 INSTALLED_APPS 列表中，表明注册安装具备此配置属性的应用。\n\nAppConfig.name属性表示这个配置类是加载到哪个应用的，每个配置类必须包含此属性，默认自动生成。\n\nAppConfig.verbose_name属性用于设置该应用的直观可读的名字，此名字在Django提供的Admin管理站点中会显示，如\n123456# admin.pyfrom django.apps import AppConfigclass UsersConfig(AppConfig):    name = &#x27;book&#x27;    verbose_name = &#x27;图书管理&#x27;\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Django站点管理- Admin管理员，视图和URL","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/01/27/Django站点管理- Admin管理员，视图和URL/","content":"站点管理\n站点: 分为内容发布和公共访问两部分\n\n内容发布的部分由网站的管理员负责查看、添加、修改、删除数据\n\nDjango能够根据定义的模型类自动地生成管理模块\n\n使用Django的管理模块, 需要按照如下步骤操作 :\n1.管理界面本地化\n2.创建管理员\n3.注册模型类\n4.发布内容到数据库\n\n\n1.管理界面本地化\n本地化是将显示的语言、时间等使用本地的习惯，这里的本地化就是进行中国化.\n\n中国大陆地区使用简体中文, 时区使用亚洲/上海时区, 注意这里不使用北京时区.\n\n本地化前：\n\n\n本地化后：\n\n\n\n2.创建管理员\n创建管理员的命令 :\n12python manage.py createsuperuser1\n\n按提示输入用户名、邮箱、密码\n\n\n重置密码\n12python manager.py changepassword 用户名1\n\n登陆站点 :http://127.0.0.1:8000/admin\n需要服务器是启动状态\n\n\n登陆站点成功\n站点界面中没有书籍和人物管理入口,因为没有注册模型类\n\n\n\n3.注册模型类\n在应用的admin.py文件中注册模型类\n需要导入模型模块 :from book.models import BookInfo,PeopleInfo\n\n\n注册模型后\n\n\n\n1注册模型成功后, 就可以在站点管理界面方便快速的管理数据.\n\n4.发布内容到数据库\n\n发布内容后，优化模型类展示\n\n# 准备书籍列表信息的模型类\nclass BookInfo(models.Model):\n    # 创建字段，字段类型...\n    name = models.CharField(max_length=10)\n\n    def __str__(self):\n        &quot;&quot;&quot;将模型类以字符串的方式输出&quot;&quot;&quot;\n        return self.name\n12345678\n1234567891011121314151617181920212223242526272829303132333435363738394041![在这里插入图片描述](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20200406233447922.png)------### 视图 和 URL- 站点管理页面做好了, 接下来就要做公共访问的页面了.- 对于Django的设计框架MVT.1. 用户在URL中请求的是视图.2. 视图接收请求后进行处理.3. 并将处理的结果返回给请求者.- 使用视图时需要进行两步操作  1.定义视图  2.配置URLconf#### 1. 定义视图- 视图就是一个Python函数，被定义在应用的views.py中.- 视图的第一个参数是HttpRequest类型的对象reqeust，包含了所有请求信息.- 视图必须返回HttpResponse对象，包含返回给请求者的响应信息.- 需要导入HttpResponse模块 :from django.http import HttpResponse  - 定义视图函数 : 响应字符串OK!给客户端 ![在这里插入图片描述](https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20200406233824628.png?x-oss-process&#x3D;image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY4NTg2OQ&#x3D;&#x3D;,size_16,color_FFFFFF,t_70)&#96;&#96;&#96;python思考 : 如何才能让请求找到视图?1\n\n\n\n2. 配置URLconf\n查找视图的过程 :\n\n\n请求者在浏览器地址栏中输入URL, 请求到网站.\n网站获取URL信息.\n然后与编写好的URLconf逐条匹配.\n如果匹配成功则调用对应的视图.\n如果所有的URLconf都没有匹配成功.则返回404错误.\n\n\n\nURLconf入口\n需要两步完成URLconf配置\n\n\n在项目中定义URLconf\n在应用中定义URLconf\n\n\n在项目中定义URLconf\n\n\n在应用中定义URLconf\n提示：一条URLconf包括URL规则、视图两部分\n\nURL规则使用正则表达式定义.\n\n视图就是在views.py中定义的视图函数.\n\n\nurl匹配过程\n\n\n\n\n3. 测试：请求访问12http://127.0.0.1:8000/1\n\n\n4. 总结视图处理过程如下图：\n123456使用视图时需要进行两步操作，两步操作不分先后\t配置URLconf\t\t在应用/views.py中定义视图12345\n\n\n\n总结View和URL匹配流程\n","categories":["djangobook"],"tags":["python"]},{"title":"Django 项目的创建，模型的使用","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/01/25/Django 项目的创建，模型的使用/","content":"创建Django项目12django-admin startproject name1\n\n创建子应用12python manager.py startapp name1\n\n\n\n创建工程在使用Flask框架时，项目工程目录的组织与创建是需要我们自己手动创建完成的。\n在django中，项目工程目录可以借助django提供的命令帮助我们创建。\n1&gt;. 创建创建工程的命令为：\n12django-admin startproject 工程名称1\n\n\n例如：想要在桌面的code目录中创建一个名为demo的项目工程，可执行如下命令：\n123cd ~/Desktop/Codedjango-admin startproject bookmanager12\n\n\n\n执行后，会多出一个新目录名为bookmanager，此即为新创建的工程目录。\n2&gt;. 工程目录说明查看创建的工程目录，结构如下\n\n与项目同名的目录，此处为bookmanager。\nsettings.py是项目的整体配置文件。\nurls.py是项目的URL配置文件。\nwsgi.py是项目与WSGI兼容的Web服务器入口。\nmanage.py是项目管理文件，通过它管理项目。\n\n3&gt;. 运行开发服务器在开发阶段，为了能够快速预览到开发的效果，django提供了一个纯python编写的轻量级web服务器，仅在开发阶段使用。\n运行服务器命令如下：\n1234567python manage.py runserver ip:端口&quot;或：&quot;python manage.py runserver12345可以不写IP和端口，默认IP是127.0.0.1，默认端口为8000。\n\n启动后可见如下信息：\n在浏览器中输入网址“127.0.0.1:8000”便可看到效果。\n\n\ndjango默认工作在调式Debug模式下，如果增加、修改、删除文件，服务器会自动重启。\n按ctrl+c停止服务器。\n\n创建子应用在Web应用中，通常有一些业务功能模块是在不同的项目中都可以复用的，故在开发中通常将工程项目拆分为不同的子功能模块，各功能模块间可以保持相对的独立，在其他工程项目中需要用到某个特定功能模块时，可以将该模块代码整体复制过去，达到复用。\n在Flask框架中也有类似子功能应用模块的概念，即蓝图Blueprint。\nDjango的视图编写是放在子应用中的。\n1&gt;. 创建在django中，创建子应用模块目录仍然可以通过命令来操作，即：\n12python manage.py startapp 子应用名称1\n\nmanage.py为上述创建工程时自动生成的管理文件。\n例如，在刚才创建的bookmanager工程中，想要创建一个用户book子应用模块，可执行：\n1234cd ~/Desktop/code/bookpython manage.py startapp book123\n\n执行后，可以看到工程目录中多出了一个名为book的子目录。\n2&gt;. 子应用目录说明查看此时的工程目录，结构如下：\n\n\nadmin.py文件跟网站的后台管理站点配置相关。\napps.py文件用于配置当前子应用的相关信息。\nmigrations目录用于存放数据库迁移历史文件。\nmodels.py文件用户保存数据库模型类。\ntests.py文件用于开发测试用例，编写单元测试。\nviews.py文件用于编写Web应用视图。\n\n3&gt;. 注册安装子应用创建出来的子应用目录文件虽然被放到了工程项目目录中，但是django工程并不能立即直接使用该子应用，需要注册安装后才能使用。\n在工程配置文件settings.py中，INSTALLED_APPS项保存了工程中已经注册安装的子应用，初始工程中的INSTALLED_APPS如下：\n\n注册安装一个子应用的方法，即是将子应用的配置信息文件apps.py中的Config类添加到INSTALLED_APPS列表中。\n例如，将刚创建的book子应用添加到工程中，可在INSTALLED_APPS列表中添加’book.apps.BookConfig’。\n\n\n模型使用Django进行数据库开发的提示 ：\nMVT设计模式中的Model, 专门负责和数据库交互.对应(models.py)\n由于Model中内嵌了ORM框架, 所以不需要直接面向数据库编程.\n而是定义模型类, 通过模型类和对象完成数据库表的增删改查.\nORM框架就是把数据库表的行与相应的对象建立关联, 互相转换.使得数据库的操作面向对象.\n\n使用Django进行数据库开发的步骤 ：\n定义模型类\n模型迁移\n操作数据库\n\n1. 定义模型类根据书籍表结构设计模型类:\n\n模型类：BookInfo\n书籍名称字段：name\n\n根据人物表结构设计模型类：\n\n模型类：PeopleInfo\n\n人物姓名字段：name\n\n人物性别字段：gender\n\n外键约束：book\n12外键要指定所属的模型类book &#x3D; models.ForeignKey(BookInfo)1\n\n\n\n说明 :\n\n书籍-人物的关系为一对多. 一本书中可以有多个英雄.\n不需要定义主键字段, 在生成表时会自动添加, 并且值为自增长.\n\n根据数据库表的设计\n\n在models.py中定义模型类,继承自models.Model\n\n# models.py\n\nfrom django.db import models\n# Create your models here.\n# 准备书籍列表信息的模型类\n\nclass BookInfo(models.Model):\n    # 创建字段，字段类型...\n    name = models.CharField(max_length=10)\n\n# 准备人物列表信息的模型类\nclass PeopleInfo(models.Model):\n    name = models.CharField(max_length=10)\n    gender = models.BooleanField()\n    # 外键约束：人物属于哪本书\n    book = models.ForeignKey(BookInfo)\n12345678910111213141516\n\n12345678910111213141516171819202122#### 2&gt;. 模型迁移(数据库迁移) （建表）数据迁移（Data migration）是指在存储类型、格式和计算机系统之间的数据转换。- 数据迁移当一个机构决定使用新的计算系统或与当前的系统不兼容的数据管理系统时是必须的。- 数据迁移通常有计划的执行来完成一个自动的迁移，从单调的任务中解放人力资源。当机构或个人改变计算机系统或升级到新的系统时需要它。&#96;&#96;&#96;python数据库迁移主要分为热迁移和冷迁移：热迁移是将内存数据和硬盘数据同步进行迁移。热迁移的优势在于其对用户业务的影响是非常小的；热迁移对内存数据进行了迁移，用户业务应用对其是无感知的。而缺点是热迁移的过程是不可中断的，整个操作过程相对复杂。冷迁移就是在关机迁移。优势是整个冷迁移过程的操作简单，一般为自动化操作。但其缺点是该方式不支持内存数据的保存，容易导致内存数据的丢失。12345678910\n\n\n\n迁移由两步完成 :\n\n生成迁移文件：根据模型类生成创建表的语句\n12python manage.py makemigrations1\n\n执行迁移：根据第一步生成的语句在数据库中创建表\n12python manage.py migrate1\n\n迁移前\n\n\n迁移后\n\n\n提示：默认采用sqlite3数据库来存储数据\n\n\n","categories":["djangobook"],"tags":["python"]},{"title":"Mysql 演示 脏读，不可重复读，幻读","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/01/12/Mysql 演示 脏读，不可重复读，幻读/","content":"今天要演示的是 Mysql 事务隔离性的 隔离级别。简单介绍一下 事务的四大特性，之前的博客也详细的写过：http://localhost:4000/2020/01/05/%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E5%BA%93%20%E7%9F%A5%E8%AF%86%E7%82%B9%E5%A4%A7%E5%85%A8%20(Mysql%EF%BC%8CRedis%EF%BC%8CMongoDB)/\nInnoDB事务原理\n事务（Transaction）是数据库区别于文件系统的重要特性之一，事务会把数据库从一种一致性状态转换为另一种一致性状态。\n在数据库提交时，可以确保要么所有修改都已保存，要么所有修改都不保存。\n\n事务的（ACID）特征\n原子性(Atomicity)：整个事物的所有操作要么全部提交成功，要么全部失败回滚(不会出现部分执行的情况)。\n一致性(Consistency)：几个并行执行的事务，其执行结果必须与按某一顺序串行执行的结果相一致。\n隔离性(Isolation)：事务的执行不受其他事务的干扰，事务执行的中间结果对其他事务必须是透明的。\n持久性(Durability): 一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。\n\n事物隔离级别\n未提交读: 脏读（READ UNCOMMITTED） 读未提交\n\n提交读: 不可重复读（READ COMMITTED）\n\n可重复读: 幻读（REPEATABLE READ）：这是MySQL的默认事务隔离级别\n\n可串行读（SERIALIZABLE） 序列化&amp;串行读\n在该隔离级别下，可以解决前面出现的脏读、不可重复读和幻读问题，但也会导致大量的超时和锁竞争现象，一般不推荐使用\n\n\n√ 为会发生，×为不会发生\n\n\n\n隔离级别\n脏读\n不可重复读\n幻读\n\n\n\nread uncommitted（未提交读）\n√\n√\n√\n\n\nread committed（提交读）\n×\n√\n√\n\n\nrepeatable read（可重复读）\n×\n×\n√\n\n\nserializable （可串行化）\n×\n×\n×\n\n\n1表格借鉴的Java识堂(https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_44685869&#x2F;article&#x2F;details&#x2F;104105291)\n\n此文 用到的 MySQL 的一些命令\n123456789101112131415161718# 查看 MySQL 版本select version()# 查看 MySQL 隔离级别SELECT @@tx_isolation# MySQL在会话层面设置隔离级别set session transaction isolation level 隔离级别名字# 开启事务start transaction# 提交事务commit# 回滚事务rollback1234567891011121314151617\n\n创建 库、表、查看。\n123456789101112# 创建 demo01 数据库create database demo01use demo01# 创建测试表create table test01(\tid int(3) not null primary key auto_increment,\tname varchar(64) default null,\tprice int(7) default 0         # 这里不许有逗号)ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4;1234567891011\n\n插入初始数据\n123insert into test01(name,price) values(&#x27;张三&#x27;,100),(&#x27;李四&#x27;,0);1一切ok 开始干活\n\n\n\n脏读表中的数据如下，设置隔离级别为未提交读\n\n\n\n时间\n客户端A（Tab A）\n客户端B（Tab B）\n\n\n\nT1\nset session transaction isolation level read uncommitted; start transaction;（开启事务） update test01 set price = price + 100 where id = 1; select * from test01 where id = 1; 设置为未提交读，给张三账号+100，输出为200\n\n\n\nT2\n\nset session transaction isolation level read uncommitted; start transaction; select * from test01 where id = 1; 查询余额输出为200\n\n\nT3\nrollback\n\n\n\nT4\ncommit\n\n\n\nT5\n\nselect * from test01 where id = 1; 查询余额输出为100\n\n\n脏读就是指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。\n再举一个严重的例子，证明一下危害表中的数据如下\n\n\n\n时间\n客户端A（Tab A）\n客户端B（Tab B）\n\n\n\nT1\nset session transaction isolation level read uncommitted; start transaction; update test01 set price = price - 100 where id = 1; update test01 set price = price + 100 where id = 2;\n\n\n\nT2\n\nset session transaction isolation level read uncommitted; start transaction; select price from test01 where id = 2; update test01 set price = price - 100 where id = 2; 更新语句被阻塞\n\n\nT3\nrollback\n\n\n\nT4\n\ncommit\n\n\n执行完成，数据库中的数据如下\n\n\n\n时间\n解释\n\n\n\nT1\n1给2转100\n\n\nT2\n2的余额够100元，购买100元的东西，更新语句被阻塞\n\n\nT3\n1回滚，1的余额变成100，2的余额变成0\n\n\nT4\n2成功扣款，余额0-100=-100\n\n\n现在好了，银行无缘无故损失100元。\n不可重复读表中的数据如下，设置隔离级别为提交读\n\n\n\n\n时间\n客户端A（Tab A）\n客户端B（Tab B）\n\n\n\nT1\nset session transaction isolation level read committed; start transaction; select * from test01 where id = 2; 查询余额输出为0\n\n\n\nT2\n\nset session transaction isolation level read committed; start transaction; update test01 set price = price + 100 where id = 2; select * from test01 where id = 2; commit; 查询余额输出100\n\n\nT3\nselect * from test01 where id = 2; commit; 查询余额输出100\n\n\n\n不可重复读是指在事务1内，读取了一个数据，事务1还没有结束时，事务2也访问了这个数据，修改了这个数据，并提交。紧接着，事务1又读这个数据。由于事务2的修改，那么事务1两次读到的的数据可能是不一样的，因此称为是不可重复读。\n当然你可以在T2时间段客户端B修改完id=2的账户余额但没有commit的时候，在客户端A查询id=2的账户余额，发现账户余额为0，可以证明提交读这个隔离级别不会发生脏读。\n可重复读级别看一下可重复读是个什么过程？表中的数据如下，设置隔离级别为可重复读\n\n\n\n时间\n客户端A（Tab A）\n客户端B（Tab B）\n\n\n\nT1\nset session transaction isolation level repeatable read; start transaction; select * from test01 where id = 2; 查询余额输出为0\n\n\n\nT2\n\nset session transaction isolation level repeatable read; start transaction; update test01 where set price = price + 100 where id = 2; select * from test01 where where id = 2; commit; 查询余额输出100\n\n\nT3\nselect * from test01 where where id = 2; commit; 查询余额输出0\n\n\n\n当我们将当前会话的隔离级别设置为可重复读的时候，当前会话可以重复读，就是每次读取的结果集都相同，而不管其他事务有没有提交。\n但是在可重复读的隔离级别上，会产生幻读的问题。\n幻读设置隔离级别为可重复读\n\n所谓幻读，指的是当某个事务在读取某个范围内的记录时，另外一个事务又在该范围内插入了新的记录，当之前的事务再次读取该范围的记录时，会产生幻行。InnoDB存储引擎通过多版本并发控制（MVCC）解决了幻读的问题。\n用大白话解释一下，就是事务1查询id&lt;10的记录时，返回了2条记录，接着事务2插入了一条id为3的记录，并提交。接着事务1查询id&lt;10的记录时，返回了3条记录，说好的可重复读呢？结果却多了一条数据。\n演示如何解决的幻读，表中的数据如下MySQL通过 MVCC 解决了这种情况下的幻读MVCC 在我理解 就是类似于乐观锁\n\n\n\n时间\n客户端A（Tab A）\n客户端B（Tab B）\n\n\n\nT1\nset session transaction isolation level repeatable read; start transaction; select count(*) from test01 where id &lt;= 10; 输出2\n\n\n\nT2\n\nset session transaction isolation level repeatable read; start transaction; insert into test01 (id, name, price) values (3, “王五”, 0); select count(*) from test01 where id &lt;= 10; commit; 输出3\n\n\nT3\nselect count(*) from test01 where id &lt;= 10; commit; 输出2\n\n\n\n这种情况下的幻读被解决了，我再举一个例子\n表中的数据如下\n\n\n\n\n时间\n客户端A（Tab A）\n客户端B（Tab B）\n\n\n\nT1\nset session transaction isolation level repeatable read; start transaction; select count(*) from account where id = 3; 输出为0\n\n\n\nT2\n\nset session transaction isolation level repeatable read; start transaction; insert into account (id, name, balance) values (3, “王五”, 0); commit;\n\n\nT3\ninsert into account (id, name, balance) values (3, “王五”, 0); 主键重复，插入失败\n\n\n\nT4\nselect count(*) from account where id = 3; 输出为0\n\n\n\nT5\nrollback;\n\n\n\nselect 某记录是否存在，不存在，准备插入此记录，但执行 insert 时发现此记录已存在，无法插入，这个就有问题了。\n很多人容易搞混不可重复读和幻读，确实这两者有些相似。但不可重复读重点在于update和delete，而幻读的重点在于insert。\n总的来说幻读就是事务A对数据进行操作，事务B还是可以用insert插入数据的，因为使用的是行锁，这样导致的各种奇葩问题就是幻读，表现形式很多，就不列举了。\n当隔离级别设置为可串行化，强制事务串行执行，避免了前面说的幻读的问题。\n参考原址：https://blog.csdn.net/zzti_erlie/article/details/88080822\n\n数据库使用锁是为了支持更好的并发，提供数据的完整性和一致性。InnoDB是一个支持行锁的存储引擎，锁的类型有：\n\n共享锁（S）\n排他锁（X）\n意向共享（IS）\n意向排他（IX）\n\n为了提供更好的并发，InnoDB提供了非锁定读：不需要等待访问行上的锁释放，读取行的一个快照。该方法是通过InnoDB的一个特性：MVCC来实现的。\nInnoDB有三种行锁的算法：\nRecord Lock：单个行记录上的锁。\nGap Lock：间隙锁，锁定一个范围，但不包括记录本身。GAP锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况。\nNext-Key Lock：1+2，锁定一个范围，并且锁定记录本身。对于行的查询，都是采用该方法，主要目的是解决幻读的问题。\n\n","categories":["mysql"],"tags":["python"]},{"title":"常用数据库 知识点大全 (Mysql，Redis，MongoDB)","url":"https://github.com/xuMr6/xumr6.github.io.git/2020/01/05/常用数据库 知识点大全 (Mysql，Redis，MongoDB)/","content":"1. Mysql简介：\nMySQL是一个关系型数据库管理系统，由瑞典MySQL AB 公司开发，属于 Oracle 旗下产品。MySQL 是最流行的关系型数据库管理系统之一，在 WEB 应用方面，MySQL是最好的 RDBMS (Relational Database Management System，关系数据库管理系统) 应用软件之一。MySQL是一种关系型数据库管理系统，关系数据库将数据保存在不同的表中，而不是将所有数据放在一个大仓库内，这样就增加了速度并提高了灵活性。MySQL所使用的 SQL 语言是用于访问数据库的最常用标准化语言。MySQL 软件采用了双授权政策，分为社区版和商业版，由于其体积小、速度快、总体拥有成本低，尤其是开放源码这一特点，一般中小型网站的开发都选择 MySQL 作为网站数据库。\n五大引擎1因MyISAM 和 InnoDB 是主要引擎所以多做概述\n\n\nMyISAM：Myisam只支持表级锁，用户在操作 myisam 表时，select、update、delete、insert、语句都会给表自动加锁，如果加锁以后的表满足 insert 并发的情况下，可以在表的尾部插入新的数据。也可以通过 locktable 命令来锁表，这样的操作主要是可以模仿事务，但消耗特别大。\n因为MyISAM表有无法处理事务，所以它只适合在一下几种情况下使用1.选择密集型的表。MyISAM存储引擎在筛选大量数据时非常迅速，这是它最突出的优点。2.插入密集型的表。MyISAM的并发插入特性允许同时选择和插入数据。例如：MyISAM存储引擎很适合管理邮件或Web服务器日志数据。\n\nInnoDB：\n1.更新密集的表。InnoDB存储引擎特别适合处理多重并发的更新请求。2.事务。InnoDB存储引擎是支持事务的标准MySQL存储引擎。3.自动灾难恢复。与其它存储引擎不同，InnoDB表能够自动从灾难中恢复。4.外键约束。MySQL支持外键的存储引擎只有InnoDB。5.支持自动增加列AUTO_INCREMENT属性。\n\nInnodb支持事务和行级锁。\n事物的ACID属性：atomicity、consistent、isolation、durable\nInnodb属于索引组织表。\nInnodb有两种存储方式，共享表空间和多表空间存储\n\n\n\nMyISAM &amp; InnoDB 差异：\n1234567891011121314151617181920212223242526272829303132333435363738- 自增长差异：MyISAM 引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，它可以根据前面几列进行排序后递增。InnoDB 引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列- 主键差异：MyISAM 允许没有任何索引和主键的存在，MyISAM 的索引都是保存行的地址InnoDB 如果没有设定主键 或者 非空唯一索引，就会自动生成一个6字节的主键（用户不可见）InnoDB 的数据是主索引的一部分，附加索引保存的是主索引的值。- count()函数差异：MyISAM 保存有表的总行数，如果 select count(*) from table; 会直接取出该值InnoDB 没有保存表的总行数，如果使用 select count(*) from table; 就会遍历整个表，消耗相当之大，但是加了 where  条件后，MyIASM 和 InnoDB 的处理方式都一样- 全文索引差异：MyISAM 支持 FullText 类型的全文索引InnoDB 不支持 FullText 类型的全文索引，但是 InnoDB 可以使用 \tsphinx 插件支持全文索引，并且效果更好delete from table 使用这条命令时，InnoDB不会从新建立表，而实一条条的删除数据，在InnoDB上如果要清空保存有大量数据的表，最好不要使用这条命令。（推荐使用 truncate table，不过需要用户有 diop 此表的权限）- 索引保留位置差异：MYISAM 的索引以表名 + .MYI 文件分别保存InnoDB 的索引和数据一起保存在 表空间里- 存储疫情选择差异：MyISAM B+treeInnoDB B-tree12345678910111213141516171819202122232425262728293031323334353637\n\n\n\nMEMORY /meməri/ -- 买么瑞：使用MEMORY引擎主要是因为速度，好处就在MEMORY采用的逻辑存储是系统内存,极大的提高了储存数据表的性能； 坏处就是当mysqld守护进程崩溃时，所有的Memory数据都会丢失。一般在以下几种情况下使用Memory存储引擎：\n1.目标数据较小，而且被非常频繁地访问。在内存中存放数据，所以会造成内存的使用，可以通过参数max_heap_table_size控制Memory表的大小，设置此参数，就可以限制Memory表的最大大小。2.如果数据是临时的，而且必须立即使用，那么就可以存放在内存表中。3.存储在Memory表中的数据如果突然丢失，不会对应用服务产生实质的负面影响。\n\nMERGE /mɜːdʒ/ -- 么耳吱：MERGE存储引擎是一组MyISAM表的组合，这些MyISAM表结构必须完全相同，所以就相当于一个集合器。比起其他储存引擎MERGE不是很优秀，但是在某些情况下MERGE还是非常的有用。对于服务器日志这种信息，一般常用的存储策略是将数据分成很多表，每个名称与特定的时间端相关。\n\nARCHIVE /&#39;ɑ:kaiv/ -- 阿尔铠武：Archive是归档的意思，在归档之后很多的高级功能就不再支持了，仅仅支持最基本的插入和查询两种功能。在MySQL 5.5版以前，Archive是不支持索引，但是在MySQL 5.5以后的版本就开始支持索引了。Archive拥有很好的压缩机制，它使用zlib压缩库，在记录被请求时会实时压缩，所以它经常被用来当做仓库使用。\n\nBDB(BerkeleyDB)\n\nEXAMPLE /ɪɡˈzæmpl/ -- 诶个曾伯\n\nFEDERATED /&#39;fɛdə,retɪd/ -- F艾 的瑞der\n\nCSV\n\nBLACKHOLE\n\n\n事务原理InnoDB事务原理\n事务（Transaction）是数据库区别于文件系统的重要特性之一，事务会把数据库从一种一致性状态转换为另一种一致性状态。\n\n在数据库提交时，可以确保要么所有修改都已保存，要么所有修改都不保存。\n123456 MySQL数据库默认采用自动提交(autocommit)模式, 也就是说修改数据(insert、update、delete)的操 作会自动的触发事务,完成事务的提交或者回滚 开启事务使用 begin 或者 start transaction;  回滚事务使用 rollback; pymysql 里面的 conn.commit() 操作就是提交事务 pymysql 里面的 conn.rollback() 操作就是回滚事务12345\n\n\n\n事务的（ACID）特征\n原子性(Atomicity)：整个事物的所有操作要么全部提交成功，要么全部失败回滚(不会出现部分执行的情况)。\n一致性(Consistency)：几个并行执行的事务，其执行结果必须与按某一顺序串行执行的结果相一致。\n隔离性(Isolation)：事务的执行不受其他事务的干扰，事务执行的中间结果对其他事务必须是透明的。\n持久性(Durability): 一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。\n\n事物隔离级别\n未提交读: 脏读（READ UNCOMMITTED） 读未提交\n\n事务2查询到的数据是事务1中修改但未提交的数据，因为事务1回滚了数据\n所以事务2查询的数据是不正确的，因此出现了脏读的问题。\n\n（存在4个问题 脏读、不可重复读、幻读、更新丢失）解决方法:将数据库事务隔离级别调整到 Read_Commited 读已提交\n\n提交读: 不可重复读（READ COMMITTED）\n\n事务2执行update语句但未提交前，事务1的前两个select操作返回结果是相同的。\n但事务2执行commit操作后，事务1的第三个select操作就读取到事务2对数据的改变。\n导致与前两次select操作返回不同的数据，因此出现了不可重复读的问题。\n\n（不可重复读、幻读、丢失更新）解决方法：将数据库事务隔离级别调整到 Repeatable_read 可重复 读\n\n可重复读: 幻读（REPEATABLE READ）：这是MySQL的默认事务隔离级别\n\n事务每开启一个实例，都会分配一个版本号给它，如果读取的数据行正在被其它事务执行DELETE或UPDATE操作（即该行上有排他锁）\n这时该事物的读取操作不会等待行上的锁释放，而是根据版本号去读取行的快照数据（记录在undo log中）\n这样，事务中的查询操作返回的都是同一版本下的数据，解决了不可重复读问题。\n虽然该隔离级别下解决了不可重复读问题，但理论上会导致另一个问题：幻读（Phantom Read）。\n一个事务在执行过程中，另一个事物对已有数据行的更改，MVCC机制可保障该事物读取到的原有数据行的内容相同\n但并不能阻止另一个事务插入新的数据行，这就会导致该事物中凭空多出数据行，像出现了幻读一样，这便是幻读问题。\n\n简易版指 事务A 对一个表中的数据进行了修改，而且该修改涉及到表中的所有数据行；同时另一个 事务B 也在修改表中的数据，该修改像表中插入一条新数据。那么经过操作之后，操作 事务A 的用户就会发现表中还有没修改的数据行，就像发觉了幻觉一样，故 幻读。（存在两个问题 幻读、丢失更新）解决方法将数据库事务隔离级别调整到 Serializable_read 串行读\n\n可串行读（SERIALIZABLE） 序列化&amp;串行读\n\n\n\n这是事务的最高隔离级别，通过强制事务排序，使之不可能相互冲突，就是在每个读的数据行加上共享锁来实现\n在该隔离级别下，可以解决前面出现的脏读、不可重复读和幻读问题，但也会导致大量的超时和锁竞争现象，一般不推荐使用\n\n\n\n\n隔离级别\n脏读\n不可重复读\n幻读\n\n\n\nread uncommitted（未提交读）\n√\n√\n√\n\n\nread committed（提交读）\n×\n√\n√\n\n\nrepeatable read（可重复读）\n×\n×\n√\n\n\nserializable （可串行化）\n×\n×\n×\n\n\n级别高低：脏读 &lt; 不可重复读 &lt; 幻读所以只要是设置了 最高级别 Serializable 就解决了 前三个问题了，当时 他的资源消耗是最致命的。\n锁什么是锁？\n锁是协调多个进程或线程并发访问某一资源的一种机制。在数据库当中，除了传统的计算资源（CPU、RAM、I/O等等）的争用之外，数据也是一种供许多用户共享访问的资源。如何保证数据并发访问的一致性、有效性，是所有数据库必须解决的一个问题，锁的冲突也是影响数据库并发访问性能的一个重要因素。从这一角度来说，锁对于数据库而言就显得尤为重要。\n在 Mysql中 有两种锁：Lock 和 Latch\n\nLatch一般称为闩锁（轻量级的锁），因为其要求锁定的时间必须非常短。若持续的时间长，则应用的性能会非常差，在InnoDB引擎中，Latch又可以分为mutex（互斥量）和 rwlock（读写锁）。其目的是用来保证并发线程操作临界资源的正确性，并且通常没有死锁检测的机制。\nLock的对象是事务，用来锁定的是数据库中的对象，如表、页、行。并且一般lock的对象仅在事务commit或rollback后进行释放（不同事务隔离级别释放的时间可能不同）。\n关于Latch更详细的讲解可以参考：关于MySQL latch争用深入分析与判断，本文主要关注的是Lock锁。\n锁的类型对数据的操作其实只有两种，也就是读和写，而数据库在实现锁时，也会对这两种操作使用不同的锁；InnoDB 实现了标准的行级锁，也就是共享锁（Shared Lock）和互斥锁（Exclusive Lock）。\n\n共享锁（读锁），允许事务读一行数据。\n排他锁（写锁），允许事务删除或更新一行数据。\n\n共享锁之间是兼容的，而互斥锁与其他任意锁都不兼容：\n共享锁代表了读操作、互斥锁代表了写操作，所以我们可以在数据库中并行读，但是只能串行写，只有这样才能保证不会发生线程竞争，实现线程安全。\n锁的粒度Lock锁根据粒度主要分为表锁、页锁和行锁。不同的存储引擎拥有的锁粒度都不同。\n表锁表级别的锁定是MySQL各存储引擎中最大颗粒度的锁定机制。该锁定机制最大的特点是实现逻辑非常简单，带来的系统负面影响最小。所以获取锁和释放锁的速度很快。由于表级锁一次会将整个表锁定，所以可以很好的避免困扰我们的死锁问题。当然，锁定颗粒度大所带来最大的负面影响就是出现锁定资源争用的概率也会最高，致使并发度大打折扣。使用表级锁定的主要是MyISAM，MEMORY，CSV等一些非事务性存储引擎。\n页锁页级锁定是MySQL中比较独特的一种锁定级别，在其他数据库管理软件中也并不是太常见。页级锁定的特点是锁定颗粒度介于行级锁定与表级锁之间，所以获取锁定所需要的资源开销，以及所能提供的并发处理能力也同样是介于上面二者之间。另外，页级锁定和行级锁定一样，会发生死锁。在数据库实现资源锁定的过程中，随着锁定资源颗粒度的减小，锁定相同数据量的数据所需要消耗的内存数量是越来越多的，实现算法也会越来越复杂。不过，随着锁定资源颗粒度的减小，应用程序的访问请求遇到锁等待的可能性也会随之降低，系统整体并发度也随之提升。使用页级锁定的主要是BerkeleyDB存储引擎。\n行锁行级锁定最大的特点就是锁定对象的粒度很小，也是目前各大数据库管理软件所实现的锁定颗粒度最小的。由于锁定颗粒度很小，所以发生锁定资源争用的概率也最小，能够给予应用程序尽可能大的并发处理能力而提高一些需要高并发应用系统的整体性能。虽然能够在并发处理能力上面有较大的优势，但是行级锁定也因此带来了不少弊端。由于锁定资源的颗粒度很小，所以每次获取锁和释放锁需要做的事情也更多，带来的消耗自然也就更大了。此外，行级锁定也最容易发生死锁。使用行级锁定的主要是InnoDB存储引擎。\n总结\n\n表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。\n行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。\n页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。\n\nInnoDB中的锁意向锁\n上节提到InnoDB 支持多种粒度的锁，也就是行锁和表锁。为了支持多粒度锁定，InnoDB 存储引擎引入了意向锁（Intention Lock）。\n那什么是意向锁呢？我们在这里可以举一个例子：如果没有意向锁，当已经有人使用行锁对表中的某一行进行修改时，如果另外一个请求要对全表进行修改，那么就需要对所有的行是否被锁定进行扫描，在这种情况下，效率是非常低的；不过，在引入意向锁之后，当有人使用行锁对表中的某一行进行修改之前，会先为表添加意向互斥锁（IX），再为行记录添加互斥锁（X），在这时如果有人尝试对全表进行修改就不需要判断表中的每一行数据是否被加锁了，只需要通过等待意向互斥锁被释放就可以了。\n与上一节中提到的两种锁的种类相似的是，意向锁也分为两种：\n\n意向共享锁（IS）：事务想要在获得表中某些记录的共享锁，需要在表上先加意向共享锁。\n意向互斥锁（IX）：事务想要在获得表中某些记录的互斥锁，需要在表上先加意向互斥锁。\n\n以上原文出处：https://blog.csdn.net/bruceleenumberone/article/details/81865045\n\n锁分类\n按操作划分：DML锁，DDL锁\n按锁的粒度划分：表级锁、行级锁、页级锁\n按锁级别划分：共享锁、排他锁\n按加锁方式划分：自动锁、显示锁\n按使用方式划分：乐观锁、悲观锁\n\n乐观锁实现方法\n每次获取商品时，（读操作）不对该商品加锁。\n写操作加锁\n在更新数据的时候需要比较程序中的库存量与数据库中的库存量是否相等，如果相等则进行更新\n反之程序重新获取库存量，再次进行比较，直到两个库存量的数值相等才进行数据更新。\n\n举例：我们收藏购物车 就是乐观锁。假如商品只有1件的时候，所有人都会加入购物车，但是购买只会有一个人\n1234567891011#### 乐观锁实现加一操作代码# 我们可以看到，只有当对数量-1操作时才会加锁，只有当程序中值和数据库中的值相等时才正真执行。&#x27;&#x27;&#x27;//不加锁select id,name,stock where id=1;//业务处理begin;update shop set stock=stock-1 where id=1 and stock=stock;commit;&#x27;&#x27;&#x27;12345678910\n\n\n\n悲观锁\n每次获取商品时，对该商品加排他锁。\n也就是在用户A获取获取 id=1 的商品信息时对该行记录加锁，期间其他用户阻塞等待访问该记录。\n\n举例：购买商品的付款时间就是悲观锁，购买商品 30分钟付款时间 就是开的悲观锁，假设商品只有一个，这三个小时我买了没付钱、最后订单取消。这30分钟你都不会购买到次商品\n123456789#### 悲观锁实现加一操作代码# 我们可以看到，首先通过begin开启一个事物，在获得shop信息和修改数据的整个过程中都对数据加锁，保证了数据的一致性。&#x27;&#x27;&#x27;begin;select id,name,stock as old_stock from shop  where id=1 for update;update shop set stock=stock-1 where id=1 and stock=old_stock;commit&#x27;&#x27;&#x27;12345678\n\n\n\n排它锁\n排它锁又叫写锁，如果事务T对A加上排它锁，则其它事务都不能对A加任何类型的锁。获准排它锁的事务既能读数据，又能写数据。\n用法 ： SELECT … FOR UPDATE\n\n举例：\n12345如果我们进入洗手间是为了上厕所，那么就不能允许任何人进来了。这就是排他锁，也叫写锁，就是我们对数据进行写操作的时候，要想获得写锁，获得写锁的事务既可以写数据也可以读数据。当时，如果数据库已经被别人增加了排他锁，那么后面的事务是无法在获得该数据库得任何锁得。也就是说，如果事务A对 数据加上了排他锁后，则其他事务不能在对 数据 加任何类型得封锁。获准排他锁得事务既能读数据，又能修改数据。1234\n\n共享锁(share lock)\n共享锁又叫读锁，如果事务T对A加上共享锁，则其它事务只能对A再加共享锁，不能加其它锁。\n获准共享锁的事务只能读数据，不能写数据。\n用法： SELECT … LOCK IN SHARE MODE;\n\n举例：\n12345如果我们进家里的洗手间只是想洗手，一般不会锁门，其他人也可以来洗手，但是 其他人是不可以进来上厕所的。这就是共享锁，也叫读锁，就是我们对数据进行读取操作的时候，其实是不会改变数据的值的。所以我们可以给数据库增加读锁，获得读锁的事务j就可以读取数据了。当数据库已经被别人增加了读锁的时候，其他新来的事务y也可以读数据，但是不能写。也就是说，如果 事务A 对数据加上了 共享锁后，则 其他事务只能对 数据 再加共享锁，不能加排他锁。获取共享锁的事务只能读数据，不能修改数据。1234\n\n\n\nBtree/B+treeBtreeBtree是一种多路自平衡搜索树，它类似普通的二叉树，但是Btree允许每个节点有更多的子节点。Btree示意图如下：\n由上图可知 Btree 的一些特点:\n\n所有键值分布在整个树中\n任何关键字出现且只出现在一个节点中\n搜索有可能在非叶子节点结束\n在关键字全集内做一次查找，性能逼近二分查找算法\n\nB+treeB+树是B树的变体，也是一种多路平衡查找树，B+树的示意图为：\n由图可看出B+tree的特点 同时也是 Btree 和 B+tree的区别\n\n所有关键字存储在叶子节点，非叶子节点不存储真正的data\n为所有叶子节点增加了一个链指针 只有一个\n\n在数据存储的索引结构上 Btree 更偏向于 纵向深度的存储数据 而 B+tree 更青睐于 横向广度的存储数据。\n参考原址-(写的非常好)\nMySQL主从复制原理什么是mysql的主从复制？MySQL 主从复制是指数据可以从一个MySQL数据库服务器主节点复制到一个或多个从节点。MySQL 默认采用异步复制方式，这样从节点不用一直访问主服务器来更新自己的数据，数据的更新可以在远程连接上进行，从节点可以复制主数据库中的所有数据库或者特定的数据库，或者特定的表。\nmysql复制原理\nmaster服务器将数据的改变记录二进制binlog日志，当master上的数据发生改变时，则将其改变写入二进制日志中；\nslave服务器会在一定时间间隔内对master二进制日志进行探测其是否发生改变，如果发生改变，则开始一个I/OThread请求master二进制事件\n同时主节点为每个I/O线程启动一个dump线程，用于向其发送二进制事件，并保存至从节点本地的中继日志中，从节点将启动SQL线程从中继日志中读取二进制日志，在本地重放，使得其数据和主节点的保持一致，最后I/OThread和SQLThread将进入睡眠状态，等待下一次被唤醒。\n\n\n也就是说\n从库会生成两个线程,一个I/O线程,一个SQL线程;\nI/O线程会去请求主库的binlog,并将得到的binlog写到本地的relay-log(中继日志)文件中;\n主库会生成一个log dump线程,用来给从库I/O线程传binlog;\nSQL线程,会读取relay log文件中的日志,并解析成sql语句逐一执行;\n\n主从复制原址\nbinlog日志简介mysql-binlog是MySQL数据库的二进制日志，用于记录用户对数据库操作的SQL语句（(除了数据查询语句）信息。可以使用mysqlbin命令查看二进制日志的内容。\nMySQL binlog格式binlog的格式也有三种：STATEMENT、ROW、MIXED 。\n\nSTATMENT模式：基于SQL语句的复制(statement-based replication, SBR)，每一条会修改数据的sql语句会记录到binlog中。\n\n\n优点：不需要记录每一条SQL语句与每行的数据变化，这样子binlog的日志也会比较少，减少了磁盘IO，提高性能。\n缺点：在某些情况下会导致master-slave中的数据不一致(如sleep()函数， last_insert_id()，以及user-defined functions(udf)等会出现问题)\n\n\n基于行的复制(row-based replication, RBR)：不记录每一条SQL语句的上下文信息，仅需记录哪条数据被修改了，修改成了什么样子了。\n\n\n优点：不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题。\n缺点：会产生大量的日志，尤其是alter table的时候会让日志暴涨。\n\n\n混合模式复制(mixed-based replication, MBR)：以上两种模式的混合使用，一般的复制使用STATEMENT模式保存binlog，对于STATEMENT模式无法复制的操作使用ROW模式保存binlog，MySQL会根据执行的SQL语句选择日志保存方式。\n\n总结无论是增量备份还是主从复制，都是需要开启mysql-binlog日志，最好跟数据目录设置到不同的磁盘分区，可以降低io等待，提升性能；并且在磁盘故障的时候可以利用mysql-binlog恢复数据。\nmysql优化Mysql优化综合性的问题：\n表的是设计合理化(符合 3范式)\n添加适当的索引(index)[四种：普通索引，主键索引，唯一索引，unique，全文索引]\n分表技术(水平分割，垂直分割)\n读写[写：update/delete/add]分离\n存储过程[模块化编程，可以提高速度]\n对mysql配置优化[配置最大并发数，my.ini调整缓存大小]\nMysql服务器引荐升级\n定时的去清楚不需要的数据，定时进行碎片整理\n\n1. 数据库的表设计\n\n第一范式：1NF是对属性的原子性约束，要求属性(列)具有原子性，不可再分解；(只要是关系型数据库都满足1NF)\n第二范式：2NF是对记录的惟一性约束，要求记录有惟一标识，即实体的惟一性；\n第三范式：3NF是对字段冗余性的约束，它要求字段没有冗余。没有冗余的数据库设计可以做到。\n\n2. 索引\n\n主键索引当把一张表的某列设置为主键的时候，则该列就是主键索引。\n唯一索引当表的某列被指定为unique约束时，这列就是唯一索引\n普通索引一般来说，普通索引是先创建表，然后创建普通索引。\n全文索引全文索引，主要是针对文件，比如文章的索引全文索引针对MyISAM有用，针对innodb没有用\n\n3. 表的分割水平分割：大数据量的表,我们在提供检索的时候，应该根据业务的需求，找到表的标准，并在检索页面约束用户的检索方式，而且要配合分页，案例：大数据量的用户表三张表：A, B, C\n12345678910111213141516171819202122232425262728293031create table A(       id int unsigned not null primary key,/* 这个id不能设置自增长 */       name varchar(32)not null default&#x27;&#x27;，       pwd varchar(32)not null default&#x27;&#x27;)engine = myisam default charset = utf8;create table B(        id int unsigned not null主键，/ *这个id不能设置自增长* /        name varchar(32)not null default&#x27;&#x27;，        pwd varchar(32)not null default&#x27;&#x27;    )engine = myisam default charset = utf8;create table C(        id int unsigned not null主键，/ *这个id不能设置自增长* /        name varchar(32)not null default&#x27;&#x27;，        pwd varchar(32)not null default&#x27;&#x27;    )engine = myisam default charset = utf8;123456789101112131415161718192021222324252627282930\n\n\n\n垂直分割：\n把某个表的某些字段，这些字段，在查询时候并不关系，但是数据量很大，我们建议将这些字段放到一个表中，从而提高效率，\nMysql优化原文链接\n\n\nRedis简介Redis（全称：Remote Dictionary Server 远程字典服务）是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。从2010年3月15日起，Redis的开发工作由VMware主持。从2013年5月开始，Redis的开发由Pivotal赞助。\nRedis介绍Redis 的优点 和 缺点优点:\n\n数据类型多 上面有介绍\n快\\1. 纯内存操作\\2. 单线程避免上下文切换\\3. 非阻塞IO多路复用机制\n\n缺点- 内存限制，不能用作海量数据\nredis的五大数据类型实现原理1231. redis中所有数据结构都以唯一的key字符串作为名称，然后通过这个唯一的key来获取对应的value&#96;&#96;2. 不同的数据类型数据结构差异就在于value的结构不一样\n\n\n\n1. 字符串（string）\n\nvalue的数据结构（数组）\n\n\n字符串value数据结构类似于数组，采用与分配容易空间来减少内存频繁分配\n当字符串长度小于1M时，扩容就是加倍现有空间\n如果字符串长度操作1M时，扩容时最多扩容1M空间，字符串最大长度为 512M\n\n\n字符串的使用场景（缓存）\n\n\n字符串一个常见的用途是缓存用户信息，我们将用户信息使用JSON序列化成字符串\n取用户信息时会经过一次反序列化的过程\n\n2. list（列表）\n\nvalue的数据结构（双向链表）\n\n\n列表的数据结构是双向链表，这意味着插入和删除的时间复杂度是0(1)，索引的时间复杂度位0(n)\n当列表弹出最后一个元素后，该数据结构会被自动删除，内存被回手\n\n\n列表的使用场景（队列、栈）\n\n3. hash（字典）\n\nvalue的数据结构（HashMap）\n\n\nredis中的字典也是HashMap（数组+列表）的二维结构\n不同的是redis的字典的值只能是字符串\n\n\nhash的使用场景（缓存）\n\n\nhash可以用来缓存用户信息，与字符串一次性全部序列化整个对象不同，hash可以对每个字段进行单独存储\n这样可以部分获取用户信息，节约网络流量\nhash也有缺点，hash结构的存储消耗要高于单个字符串\n\n4. set ( 集合对象 )\n\n集合对象 set 是 string 类型（整数也会转换成string类型进行存储）的无序集合。\n集合类型和列表类型的最大的区别是有序性 – &gt; 列表有序唯一性 – &gt; 集合唯一\n数据结构底层数据结构以intset或者hashtable来存储\n最多包含2^32-1元素。\n\n5. Sorted set (有序集合对象)\n\n有序集合类型为集合中的每个元素都关联了一个分数\n数据结构\n\n\n内部是以ziplist或者skiplist+hashtable来实现skiplist，也就是跳跃表\n\n持久化方式前言由于Redis的数据都存放在内存中，如果没有配置持久化，redis重启后数据就全丢失了，于是需要开启redis的持久化功能，将数据保存到磁盘上，当redis重启后，可以从磁盘中恢复数据。redis提供两种方式进行持久化，一种是RDB持久化（原理是将Reids在内存中的数据库记录定时dump到磁盘上的RDB持久化），另外一种是AOF（append only file）持久化（原理是将Reids的操作日志以追加的方式写入文件）。\nRDB：RDB持久化是指在指定的时间间隔内将内存中的数据集快照写入磁盘，实际操作过程是fork一个子进程，先将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储。\nAOF：AOF持久化以日志的形式记录服务器所处理的每一个写、删除操作，查询操作不会记录，以文本的方式记录，可以打开文件看到详细的操作记录。\n二者之间的优缺点\nRDB的优势\n\n一旦采用该方式，那么你的 整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。\n对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。\n性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是 fork 出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。\n相比于AOF机制，如果 数据集很大，RDB的启动效率会更高。\n\nRDB的劣势\n\n如果你想保证数据的高可用性，即最大限度的避免数据丢失，那么RDB将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。\n由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟。\n\nAOF的优势\n\n该机制可以带来更高的数据安全性，即数据持久性。Redis中提供了3种同步策略，即每秒同步、每修改同步和不同步。事实上，每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。而每修改同步，我们可以将其视为同步持久化，即每次发生的数据变化都会被立即记录到磁盘中。可以预见，这种方式在效率上是最低的。至于无同步，无需多言，我想大家都能正确的理解它。\n由于该机制对日志文件的写入操作采用的是append模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。然而如果我们本次操作只是写入了一半数据就出现了系统崩溃问题，不用担心，在Redis下一次启动之前，我们可以通过redis-check-aof工具来帮助我们解决数据一致性的问题。\n如果日志过大，Redis可以自动启用rewrite机制。即Redis以append模式不断的将修改数据写入到老的磁盘文件中，同时Redis还会创建一个新的文件用于记录此期间有哪些修改命令被执行。因此在进行rewrite切换时可以更好的保证数据安全性。\nAOF包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作。事实上，我们也可以通过该文件完成数据的重建。\n\nAOF的劣势\n\n对于相同数量的数据集而言，AOF文件通常要大于RDB文件。RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。\n根据同步策略的不同，AOF在运行效率上往往会慢于RDB。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和RDB一样高效。\n\n二者选择的标准，就是看系统是愿意牺牲一些性能，换取更高的缓存一致性（aof），还是愿意写操作频繁的时候，不启用备份来换取更高的性能，待手动运行save的时候，再做备份（rdb）。rdb这个就更有些 eventually consistent的意思了。\n高级123redis有哪几种集群?&#96;&#96;主从模式 哨兵模式 codis\n\nRedis主从复制主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点(master)，后者称为从节点(slave),数据的复制是单向的，只能由主节点到从节点。\n默认情况下，每台Redis服务器都是主节点；且一个主节点可以有多个从节点(或没有从节点)，但一个从节点只能有一个主节点。\n主从复制的作用\n数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。\n故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。\n负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。\n读写分离：可以用于实现读写分离，主库写、从库读，读写分离不仅可以提高服务器的负载能力，同时可根据需求的变化，改变从库的数量；\n高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。\n\n主从复制原理主从复制过程大体可以分为3个阶段：连接建立阶段（即准备阶段）、数据同步阶段、命令传播阶段。\n在从节点执行 slaveof 命令后，复制过程便开始运作，下面图示大概可以看到，从图中可以看出复制过程大致分为6个过程\n主从复制参考原址\n主从同步CPA原理\nCPA原理是分布式存储理论的基石： C(一致性)； A(可用性)； P(分区容忍性);\n当主从网络无法连通时，修改操作无法同步到节点，所以“一致性”无法满足\n除非我们牺牲“可用性”，也就是暂停分布式节点服务，不再提供修改数据功能，知道网络恢复一句话概括CAP: 当网络分区发生时，一致性 和 可用性 两难全\n\nredis主从同步介绍\n和MySQL主从复制的原因一样，Redis虽然读取写入的速度都特别快，但是也会产生读压力特别大的情况。\n为了分担读压力，Redis支持主从复制，Redis的主从结构可以采用一主多从或者级联结构。\nRedis主从复制可以根据是否是全量分为全量同步和增量同步。注：redis主节点Master挂掉时，运维让从节点Slave接管（redis主从默认无法自动切换，需要运维手动切换）\n\n\nredis主动同步机制全量同步（快照同步）RDB从服务器把有的数据全部丢弃，让主服务把所有数据全部发给他\n12注：Redis全量复制一般发生在Slave初始化阶段，这时Slave需要将Master上的所有数据都复制一份。具体步骤如下：\n\n\n从服务器连接主服务器，发送SYNC命令；\n主服务器接收到SYNC命名后，开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令；\n主服务器BGSAVE执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令；\n从服务器收到快照文件后丢弃所有旧数据，载入收到的快照；\n主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令；\n从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令；\n完成上面几个步骤后就完成了从服务器数据初始化的所有操作，从服务器此时可以接收来自用户的读请求。\n\n\n增量同步 AOF全量同步：从服务器把有的数据全部丢弃，让主服务把所有数据全部发给他\n\n主节点会将那些对自己状态产生修改性影响的指令记录在本地内存buffer中，然后异步将buffer中指令同步到从节点\n从节点一边执行同步指令达到主节点状态，一边向主节点反馈自己同步到哪里（偏移量）\n当网络状态不好时，从节点无法和主节点进行同步，当网络恢复时需要进行快照同步\n\nRedis主从同步策略\n主从刚刚连接的时候，进行全量同步；全同步结束后，进行增量同步。\n当然，如果有需要，slave 在任何时候都可以发起全量同步。\nredis 策略是，无论如何，首先会尝试进行增量同步，如不成功，要求从机进行全量同步。\n\n注意点\n如果多个Slave断线了，需要重启的时候，因为只要Slave启动，就会发送sync请求和主机全量同步，当多个同时出现的时候，可能会导致Master IO剧增宕机。\n不能自动切换master，所以master如果挂掉了，整个集群都不可以写入啦\n\nRedis主从同步参考地址\n哨兵哨兵模式如何解决主从问题？\n当用Redis做主从方案时，假如master宕机，Redis本身无法自动进行主备切换而Redis-sentinel本身也是一个独立运行的进程，它能监控多个master-slave集群，发现master宕机后能进行自动切换。\n哨兵是redis集群架构中非常重要的一个组件，主要功能如下:\n\n集群监控，负责监控redis master 和slave进程是否正常工作。\n消息通知，如果某个redis实例有故障，那么哨兵负责发送消息作为报警通知给管理员。\n故障转移，如果master node挂掉了，会自动转移到slave node上。\n配置中心，如果故障转移发生了，通知client客户端新的master地址。\n\n哨兵本身也是分布式的，作为一个哨兵集群去运行的，相互协同工作\n\n故障转移时，判断一个master node宕机了，需要大部分哨兵都同意才行，涉及到分布式选举问题。\n及时部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身就是单点，那么就不靠谱。\n\n哨兵的核心知识\n\n哨兵至少需要3个实例，来保证自己的健壮性。\n哨兵+redis主从的部署架构，是不会保证数据零丢失的，只能保证redis集群的高可用性\n对于哨兵+redis主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充分的测试和演练。哨兵详解\n\n哨兵模式—-sentinelsentinel作用\n当用Redis做主从方案时，假如master宕机，Redis本身无法自动进行主备切换\n而Redis-sentinel本身也是一个独立运行的进程，它能监控多个master-slave集群，发现master宕机后能进行自动切换。\n\nsentinel原理\nsentinel负责持续监控主节点的健康，当主节挂掉时，自动选择一个最优的从节点切换成主节点\n从节点来连接集群时会首先连接sentinel，通过sentinel来查询主节点的地址\n当主节点发生故障时，sentinel会将最新的主节点地址告诉客户端，可以实现无需重启自动切换redis\n\nSentinel支持集群\n只使用单个sentinel进程来监控redis集群是不可靠的，当sentinel进程宕掉后sentinel本身也有单点问题\n如果有多个sentinel，redis的客户端可以随意地连接任意一个sentinel来获得关于redis集群中的信息。\n\nSentinel版本\nSentinel当前稳定版本称为Sentinel 2，Redis2.8和Redis3.0附带稳定的哨兵版本\n安装完redis-3.2.8后，redis-3.2.8/src/redis-sentinel启动程序 redis-3.2.8/sentinel.conf是配置文件。\n\n运行sentinel两种方式（效果相同）法1：redis-sentinel /path/to/sentinel.conf法2：redis-server /path/to/sentinel.conf –sentinel\n\n以上两种方式，都必须指定一个sentinel的配置文件sentinel.conf，如果不指定，将无法启动sentinel。\nsentinel默认监听26379端口，所以运行前必须确定该端口没有被别的进程占用。\n\nsentinel.conf配置文件说明\n配置文件只需要配置master的信息就好啦，不用配置slave的信息，因为slave能够被自动检测到\n需要注意的是，配置文件在sentinel运行期间是会被动态修改的，例如当发生主备切换时候，配置文件中的master会被修改为另外一个slave。\n这样，之后sentinel如果重启时，就可以根据这个配置来恢复其之前所监控的redis集群的状态。\n\n1234567# sentinel.conf 配置说明sentinel monitor mymaster 127.0.0.1 6379 2sentinel down-after-milliseconds mymaster 60000sentinel failover-timeout mymaster 180000sentinel parallel-syncs mymaster 112345\n\n配置传播\n一旦一个sentinel成功地对一个master进行了failover，它将会把关于master的最新配置通过广播形式通知其它sentinel，其它的sentinel则更新对应master的配置。\n一个faiover要想被成功实行，sentinel必须能够向选为master的slave发送SLAVE OF NO ONE命令，然后能够通过INFO命令看到新master的配置信息。\n当将一个slave选举为master并发送SLAVE OF NO ONE`后，即使其它的slave还没针对新master重新配置自己，failover也被认为是成功了的。\n\n因为每一个配置都有一个版本号，所以以版本号最大的那个为标准：\n\n假设有一个名为mymaster的地址为192.168.1.50:6379。\n一开始，集群中所有的sentinel都知道这个地址，于是为mymaster的配置打上版本号1。\n一段时候后mymaster死了，有一个sentinel被授权用版本号2对其进行failover。\n如果failover成功了，假设地址改为了192.168.1.50:9000，此时配置的版本号为2\n进行failover的sentinel会将新配置广播给其他的sentinel，发现新配置的版本号为2时，版本号变大了，说明配置更新了，于是就会采用最新的版本号为2的配置。\n\nsentinel缺点redis的slave和master数据时完全一样的，但是有个问题，redis数据时存储在内存中内存空间有限，所以哨兵模式不能处理大的数据量\n如何解决？codis\ncodis集群简介\nCodis 是一个分布式 Redis 解决方案, 对于上层的应用来说, 连接到 Codis Proxy 和连接原生的 Redis Server 没有明显的区别 (不支持的命令列表), 上层应用可以像使用单机的 Redis 一样使用, Codis 底层会处理请求的转发, 不停机的数据迁移等工作, 所有后边的一切事情, 对于前面的客户端来说是透明的, 可以简单的认为后边连接的是一个内存无限大的 Redis 服务.\ncodis是redis集群解决方案之一，codis是GO语言开发的代理中间件当客户端向codis发送指令时，codis负责将指令转发给后面的redis实例来执行，并将返回结果转发给客户端\n为什么会出现codis\n\n在大数据高并发场景下，单个redis实例往往会无法应对\n首先redis内存不易过大，内存太大会导致rdb文件过大，导致主从同步时间过长\n其次在CPU利用率中上，单个redis实例只能利用单核，数据量太大，压力就会特别大\n\ncodis部署方案\\4. 单个codis代理支撑的QPS比较有限，通过启动多个codis代理可以显著增加整体QPS\\5. 多codis还能起到容灾功能，挂掉一个codis代理还有很多codis代理可以继续服务codis分片的原理\n\ncodis负责将特定key转发到特定redis实例，codis默认将所有key划分为1024个槽位\n首先会对客户端传来的key进行crc32计算hash值，然后将hash后的整数值对1024进行取模，这个余数就是对应的key槽位\n每个槽位都会唯一映射到后面的多个redis实例之一，codis会在内存中维护槽位和redis实例的映射关系\n这样有了上面key对应的槽位，那么它应该转发到那个redis实例就很明确了\n槽位数量默认是1024，如果集群中节点较多，建议将这个数值大一些，比如2048,4096\n\n不同codis槽位如何同步\n\n如果codis槽位值存在内存中，那么不同的codis实例间的槽位关系得不到同步\n所以codis还需要一个分布式配置存储的数据库专门来持久化槽位关系\ncodis将槽位关系存储在zookeeper中，并且提供一个dashboard可以来观察和修改槽位关系\n\n布隆过滤器布隆过滤器是什么？（判断某个key一定不存在）\n\n本质上布隆过滤器是一种数据结构，比较巧妙的概率型数据结构\n特点是高效地插入和查询，可以用来告诉你 “某样东西一定不存在或者可能存在”。\n相比于传统的 List、Set、Map 等数据结构，它更高效、占用空间更少，但是缺点是其返回的结果是概率性的，而不是确切的。使用：　　1. 布隆过滤器在NoSQL数据库领域中应用的非常广泛　　2. 当用户来查询某一个row时，可以先通过内存中的布隆过滤器过滤掉大量不存在的row请求，然后去再磁盘进行查询　　3. 布隆过滤器说某个值不存在时，那肯定就是不存在，可以显著降低数据库IO请求数量\n\n应用场景\n\n场景1（给用户推荐新闻）\n\n\n当用户看过的新闻，肯定会被过滤掉，对于没有看多的新闻，可能会过滤极少的一部分（误判）。\n这样可以完全保证推送给用户的新闻都是无重复的。\n\n\n场景2（爬虫url去重）\n\n\n在爬虫系统中，我们需要对url去重，已经爬取的页面不再爬取\n当url高达几千万时，如果一个集合去装下这些URL地址非常浪费空间\n使用布隆过滤器可以大幅降低去重存储消耗，只不过也会使爬虫系统错过少量页面\n\n布隆过滤器原理\n\n每个布隆过滤器对应到Redis的数据结构是一个大型的数组和几个不一样的无偏hash函数\n如下图：f、g、h就是这样的hash函数（无偏差指让hash映射到数组的位置比较随机）\n\n添加：值到布隆过滤器\\1. 向布隆过滤器添加key,会使用 f、g、h hash函数对key算出一个整数索引，然后对长度取余\\2. 每个hash函数都会算出一个不同的位置，把算出的位置都设置成1就完成了布隆过滤器添加过程查询：布隆过滤器值\\1. 当查询某个key时，先用hash函数算出一个整数索引，然后对长度取余\\2. 当你有一个不为1时肯定不存在这个key，当全部都为1时可能有这个key\\3. 这样内存中的布隆过滤器过滤掉大量不存在的row请求，然后去再磁盘进行查询，减少IO操作删除：不支持\\1. 目前我们知道布隆过滤器可以支持 add 和 isExist 操作\\2. 如何解决这个问题，答案是计数删除，但是计数删除需要存储一个数值，而不是原先的 bit 位，会增大占用的内存大小。\\3. 增加一个值就是将对应索引槽上存储的值加一，删除则是减一，判断是否存在则是看值是否大于0。\n\nredis事物与分布式锁redis事物\n严格意义来讲,Redis的事务和我们理解的传统数据库(如mysql)的事务是不一样的；\nRedis的事务实质上是命令的集合，在一个事务中要么所有命令都被执行，要么所有命令都不执行。\n\n需要注意的是:　　1.Redis的事务没有关系数据库事务提供的回滚（rollback），所以开发者必须在事务执行失败后进行后续的处理；　　2.如果在一个事务中的命令出现错误，那么所有的命令都不会执行；　　3.如果在一个事务中出现运行错误，那么正确的命令会被执行。\nredis事物介绍\nredis事物是可以一次执行多个命令，本质是一组命令的集合。\n一个事务中的所有命令都会序列化，按顺序串行化的执行而不会被其他命令插入作用：一个队列中，一次性、顺序性、排他性的执行一系列命令\n\nredis事物基本使用\n下面指令演示了一个完整的事物过程，所有指令在exec前不执行，而是缓存在服务器的一个事物队列中\n服务器一旦收到exec指令才开始执行事物队列，执行完毕后一次性返回所有结果\n因为redis是单线程的，所以不必担心自己在执行队列是被打断，可以保证这样的“原子性”\n\n注：redis事物在遇到指令失败后，后面的指令会继续执行mysql的rollback与redis的discard的区别:\n\nmysql回滚为sql全部成功才执行,一条sql失败则全部失败,执行rollback后所有语句造成的影响消失\nredis的discard只是结束本次事务,正确命令造成的影响仍然还在.\n\n123456&gt; multi（开始一个redis事物）incr booksincr books&gt; exec （执行事物）&gt; discard （丢弃事物）12345\n\n\n\nwatch指令\nwatch其实就是redis提供的一种乐观锁，可以解决并发修改问题\n\nwatch会在事物开始前盯住一个或多个关键变量，当服务器收到exec指令要顺序执行缓存中的事物队列时\n\nredis会检查关键变量自watch后是否被修改（包括当前事物所在的客户端）\n\n如果关键变量被人改动过，exec指令就会返回null回复告知客户端事物执行失败，这个时候客户端会选择重试注：redis禁用在multi和exec之间执行watch指令，必须在multi之前盯住关键变量，否则会出错\n\n\nredis分布式锁redis原子操作\n原子操作是指不会被线程调度机制打断的操作\n这种操作一旦开始，就会一直运行到结束，中间不会切换任何进程\n\n分布式锁\n分布式锁本质是占一个坑，当别的进程也要来占坑时发现已经被占，就会放弃或者稍后重试\n占坑一般使用 setnx(set if not exists)指令，只允许一个客户端占坑\n先来先占，用完了在调用del指令释放坑\n\n1234&gt; setnx lock:codehole true.... do something critical ....&gt; del lock:codehole123\n\n\n但是这样有一个问题，如果逻辑执行到中间出现异常，可能导致del指令没有被调用，这样就会陷入死锁，锁永远无法释放\n为了解决死锁问题，我们拿到锁时可以加上一个expire过期时间，这样即使出现异常，当到达过期时间也会自动释放锁\n\n12345&gt; setnx lock:codehole true&gt; expire lock:codehole 5.... do something critical ....&gt; del lock:codehole1234\n\n\n这样又有一个问题，setnx和expire是两条指令而不是原子指令，如果两条指令之间进程挂掉依然会出现死锁\n为了治理上面乱象，在redis 2.8中加入了set指令的扩展参数，使setnx和expire指令可以一起执行\n\n1234&gt; set lock:codehole true ex 5 nx&#x27;&#x27;&#x27; do something &#x27;&#x27;&#x27;&gt; del lock:codehole 123\n\n\n\nredis雪崩&amp;穿透&amp;击穿缓存穿透\n定义\n\n\n缓存穿透是指查询一个一定不存在的数据，由于缓存不命中，接着查询数据库也无法查询出结果，\n虽然也不会写入到缓存中，但是这将会导致每个查询都会去请求数据库，造成缓存穿透；\n\n\n解决方法 ：布隆过滤\n\n\n对所有可能查询的参数以hash形式存储，在控制层先进行校验，不符合则丢弃，从而避免了对底层存储系统的查询压力；\n\n缓存雪崩\n定义\n\n\n缓存雪崩是指，由于缓存层承载着大量请求，有效的保护了存储层，但是如果缓存层由于某些原因整体不能提供服务\n于是所有的请求都会达到存储层，存储层的调用量会暴增，造成存储层也会挂掉的情况。\n\n\n解决方法\n\n\n保证缓存层服务高可用性：比如 Redis Sentinel 和 Redis Cluster 都实现了高可用\n依赖隔离组件为后端限流并降级：比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。\n\n缓存击穿\n定义：\n\n\n缓存击穿，就是说某个 key 非常热点，访问非常频繁，处于集中式高并发访问的情况\n当这个 key 在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库，就像是在一道屏障上凿开了一个洞。\n\n\n解决方法\n\n\n解决方式也很简单，可以将热点数据设置为永远不过期；\n或者基于 redis or zookeeper 实现互斥锁，等待第一个请求构建完缓存之后，再释放锁，进而其它请求才能通过该 key 访问数据\n\nredis项目缓存实现待补充\n\nMongoDB应用场景\n更高的写入负载\n高可用性\n表结构不明确，且数据在不断变大\n\n缺点\n不能保证事务安全性\n消耗内存大\n\nMongoDB为什么快写快：写入数据存在在内存里就返回给应用程序，而保存到硬体的操作则在后台异步完成\n\nRabbitMQRabbitMQ使用场景\n异步处理\n应用解耦\n流量削锋\n日志处理：Kalfka消息中间件\n\nRabbitMq与Redis队列对比\n可靠消费\n可靠发布\n高可用\n持久化\n\n","categories":["python"],"tags":["python"]},{"title":"Vue 父子组件 传值 超详细","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/12/29/Vue 父子组件 传值 超详细/","content":"我们 有两个 Vue页面：\n\nfather.vue\nson.vue\n\nson.vue 作为 子组件 是 被 引入的哪一方的父传子发送是直接在 组件上 进行发送的\n1234567891011121314151617181920// father.vue&lt;template&gt;\t&lt;div id=&quot;father&quot;&gt;\t\t// 父组件 传递子组件 123  FonS 是 key\t\t&lt;son :FonS=&quot;123&quot;&gt;\t\t&lt;/son&gt;\t&lt;/div&gt;\t&lt;/template&gt;&lt;script&gt;import son from &#x27;@/components/son\texport default &#123;        name: &quot;father&quot;,        components:&#123;        \tson:son        \t&#125;        &#125;&lt;/script&gt;12345678910111213141516171819\n\n子组件（页面）props 接收（列表格式）\n123456789// son.vue&lt;script&gt;    export default &#123;        name: &quot;son&quot;,        props: [&#x27;FonS&#x27;],    &#125;&lt;/script&gt;12345678\n\n此时 这个键（Fons）和 data 中的数据一样，可直接 this.Fons使用\n123456mounted()&#123;\talert(this.FonS)&#125;12312345\n\n子传父一般情况下 都是事件对应着的\n1234// son.vue// 此时的 SonF 是 父接收的 事件名 data 是事件名this.$emit(&#x27;SonF&#x27;, 321)123\n\n父接收 在 父页面上的子组件 进行接收\n1234567891011121314151617181920212223242526// father.vue&lt;template&gt;\t&lt;div id=&quot;father&quot;&gt;\t\t// data 作为虚拟名字 被当作方法 他的内部就携带了 321 参数 （这个名字随便起）\t\t&lt;son :FonS=&quot;123&quot; @SonF=&quot;data&quot;&gt;\t\t&lt;/son&gt;\t&lt;/div&gt;\t&lt;/template&gt;&lt;script&gt;import son from &#x27;@/components/son\texport default &#123;        name: &quot;father&quot;,        components:&#123;        \tson:son        \t&#125;,        methods:&#123;        \t// res 就代表了值 （名字随便起 因为内部携带了参数）        \tdata(res)&#123;\t\t\t\talert(res)\t\t\t&#125;        &#125;      &#125;&lt;/script&gt;12345678910111213141516171819202122232425\n\n此时 打印出来的就是 321\n","categories":["vue"],"tags":["python"]},{"title":"Vue 安装 配置axios 后页面一片空白","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/12/15/Vue 安装 配置axios 后页面一片空白/","content":"\n造成 这个问题的原因有很多\n一、javascript Uncaught TypeError: Cannot set property &#39;$axios&#39; of undefined报错信息说 $axios of undefined\n我们打开 main.js 查看\n将箭头所指处 改为小写。\n二1TypeError: setting getter-only property &quot;$axios&quot;\n\nvue-cli2 和 vue-cli3和4 安装方法还不一样，我使用的是 vue-cli4.3.1，应使用npm add axios，vue-cli2.0使用的是npm install axios\n\n查看版本\n12npm -V1\n\n\n\nmain.js 配置如下更改\n12import axios from &#x27;axios&#x27;Vue.prototype.$ajax = axios","categories":["vue"],"tags":["python"]},{"title":"vue 路由传参 query 和 params 的区别","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/11/13/vue 路由传参 query 和 params 的区别/","content":"query使用 path 和 name 传参跳转都可以，而 params 只能使用 name 传参跳转。\nquery传参：\n\n1234567891011121314151617181920212223var router = new VueRouter(&#123;   routes: [     &#123; path: &#x27;/login&#x27;, component: login &#125;,     &#123; name:&#x27;register&#x27;,path: &#x27;/register&#x27;, component: register &#125;    ]&#125;)// 注意：这是 query 两种传参方式 一种是直接跳转把字符串传过去 一种是传描述目标位置的对象&lt;router-link to=&quot;/login?id=10&amp;name=zs&quot;&gt;登录&lt;/router-link&gt;&lt;router-link :to=&quot;&#123;path:&#x27;/register&#x27;,query:&#123;id:5,name:&#x27;lili&#x27;&#125;&#125;&quot;&gt;注册&lt;/router-link&gt;或&lt;router-link :to=&quot;&#123;name:&#x27;register&#x27;,query:&#123;id:5,name:&#x27;lili&#x27;&#125;&#125;&quot;&gt;注册&lt;/router-link&gt;    等同于：this.$router.push(&#x27;/login?id=10&amp;name=zs&#x27;)this.$router.push(&#123;path:&#x27;/register&#x27;,query:&#123;id:5,name:&#x27;lili&#x27;&#125;&#125;)或this.$router.push(&#123;name:&#x27;register&#x27;,query:&#123;id:5,name:&#x27;lili&#x27;&#125;&#125;)接收参数：this.content = this.$route.query;123456789101112131415161718192021注意接收参数的时候，已经是$route而不是$router了哦！！\n\n\nparams传参：\n\n123456789101112131415161718var router = new VueRouter(&#123;  routes: [    &#123; path: &#x27;/login/:id/:name&#x27;, component: login &#125;,// 这里不传入对应的参数（:/id/:name） 刷新页面 参数会消失,页面中就丢失了数据    &#123; name:&#x27;register&#x27;, path: &#x27;/register&#x27;, component: register &#125;  ]&#125;)// 注意：这是 params 两种传参方式 一种是直接跳转把字符串传过去 一种是传描述目标位置的对象&lt;router-link to=&quot;/login/12/ls&quot;&gt;登录&lt;/router-link&gt;&lt;router-link :to=&quot;&#123;name:&#x27;register&#x27;,params:&#123;id:10,name:&#x27;lili&#x27;&#125;&#125;&quot;&gt;注册&lt;/router-link&gt;等同于：this.$router.push(&#x27;/login/12/ls&#x27;)this.$router.push(&#123;name:&#x27;register&#x27;,params:&#123;id:10,name:&#x27;lili&#x27;&#125;&#125;)接收参数：this.content = this.$route.params;1234567891011121314151617\n\n传参跳转页面时，query 不需要再路由上配参数就能在新的页面获取到参数，params 也可以不用配，但是 params 不在路由配参数的话，当用户刷新当前页面的时候，参数就会消失。\n也就是说使用params不在路由配参数跳转，只有第一次进入页面参数有效，刷新页面参数就会消失。\n\n效果的展示query更加类似于我们ajax中get传参，params则类似于post，说的再简单一点，前者在浏览器地址栏中显示参数，后者则不显示\n\nquery:\nparams:注意：这里的12和ls 对应的是/:id/:name 这两个参数可以不写 那么就不会在地址栏上显示 不过刷新页面参数会消失 写上参数刷新页面 参数不会消失\n\n参考地址\n","categories":["vue"],"tags":["python"]},{"title":"Vue.js的基本语法","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/10/31/Vue.js的基本语法/","content":"Vue是一个 MVVM 的框架，数据驱动和 组件化是Vue的核心思想。\n简单的讲MVVM框架就是：我们只需要在数据层做数据操作，显示层会检测到我们每次的数据变化，然后做出相应的改变，监测数据这个工作就是中间的ViewModel。通过这种模式，我们就可以不用再直接操作DOM节点来进行数据的改变。\n插值在模板里可以实现data数据的展示，如果data数据改变，展示的数据也会响应式的改变。响应式的改变意味着我们不需要强制刷新页面就可以实现数据的变化。这种语法为请输入代码Mustache语法\n123456789101112131415&lt;template&gt;    &lt;div class=&quot;main&quot;&gt;         &lt;h3&gt;这里是title的值：&#123;&#123;title&#125;&#125;&lt;/h3&gt;    &lt;/div&gt;&lt;/template&gt;export default &#123;    name:&#x27;phonerisk&#x27;,    data()&#123;        return&#123;            title:&#x27;testTitle&#x27;        &#125;    &#125;&#125;1234567891011121314\n\nv-htmlv-html可以将一段HTML的代码字符串输出成HTML片段而不是普通的文本\n123456789101112131415&lt;template&gt;    &lt;div class=&quot;main&quot;&gt;        &lt;p &gt;这里是&lt;span v-html=&#x27;html&#x27;&gt;&lt;/span&gt;&lt;/p&gt;    &lt;/div&gt;&lt;/template&gt;export default &#123;    name:&#x27;phonerisk&#x27;,    data()&#123;        return&#123;            html:&#x27;&lt;span style=&quot;color:blue;font-size:23px;&quot;&gt;v-if&lt;/span&gt;&#x27;        &#125;    &#125;&#125;1234567891011121314\n\n方法事件v-bind 在事件里绑定方法Mustache·语法不能用于HTML上，所以我们需要绑定一些属之类的需要使用v-bind。v-bind就是将data里面的数据绑定到HTML上面，从而实现属性的变化。\n1234567891011121314151617&quot;v-bind:&quot; 缩写--&gt; &quot;:&quot;&lt;template&gt;    &lt;div class=&quot;main&quot;&gt;        &lt;img :src=&quot;imgUrl&quot; /&gt;    &lt;/div&gt;&lt;/template&gt;export default &#123;    name:&#x27;phonerisk&#x27;,    data()&#123;        return&#123;             imgUrl:&quot;../../static/img/KFC.e66b2f8e.png&quot;        &#125;    &#125;&#125;12345678910111213141516\n\nv-modelv-model是用于表单输入的数据双向绑定。所谓双向绑定就是视图层的数据变化会引起数据层数据的改变，相反的，数据层的变化也会导致视图层展示数据的变化。\n12345678910111213141516171819202122&lt;template&gt;    &lt;div class=&quot;main&quot;&gt;        &lt;input type=&quot;text&quot; v-model=&quot;name&quot;&gt;        &#123;&#123;name&#125;&#125;        &lt;button @click=&#x27;changeName&#x27;&gt;改变名字&lt;/button&gt;    &lt;/div&gt;&lt;/template&gt;export default &#123;    name:&#x27;phonerisk&#x27;,    data()&#123;        return&#123;            name:&#x27;小明&#x27;        &#125;    &#125;,    methods:&#123;        changeName()&#123;            this.name = &quot;小花&quot;;        &#125;    &#125;&#125;123456789101112131415161718192021\n\nv-onv-on 用于监听DOM事件，如按钮的点击事件、双击事件等。v-on 的简写为 @,如下面的 @click 就等价为 v-on:click。\n123456789101112131415161718192021&lt;template&gt;    &lt;div class=&quot;main&quot;&gt;        &lt;button @click=&#x27;yes&#x27;&gt;你敢点我吗？&lt;/button&gt;    &lt;/div&gt;&lt;/template&gt;export default &#123;    name:&#x27;phonerisk&#x27;,    data()&#123;        return&#123;            name:&#x27;&#x27;        &#125;    &#125;,\tmethods:&#123;        yes()&#123;            alert(&quot;我有啥不敢的！！！&quot;);   // alert 是弹出的小窗口        &#125;    &#125;&#125;1234567891011121314151617181920\n\n事件修饰符123456789101112131415161718192021222324252627282930313233343536&lt;template&gt;    &lt;div id=&quot;app&quot;&gt;               &lt;form action=&quot;&quot; @submit.prevent=&#x27;yzbd&#x27;&gt;           &lt;input type=&quot;text&quot; name=&#x27;name&#x27; v-model=&#x27;user&#x27;&gt;           &lt;input type=&quot;password&quot; name=&#x27;pwd&#x27; v-model=&#x27;pwd&#x27;&gt;           &lt;button&gt;登录&lt;/button&gt;        &lt;/form&gt;    &lt;/div&gt;&lt;/template&gt;&lt;script&gt;var vm =new Vue(&#123;    el:&quot;#app&quot;,    data:&#123;        user:&#x27;&#x27;,        pwd:&#x27;&#x27;    &#125;,    methods: &#123;        yzbd:function()&#123;                if(this.user==&#x27;&#x27;)&#123;                  alert(&#x27;你不要拿空的表单来骗人，不输用户不叫你进来&#x27;);                                  &#125;else&#123;                    if(this.user==&#x27;zhangsan&#x27; &amp;&amp; this.pwd==&#x27;12345&#x27;)&#123;                    alert(&#x27;欢迎你回来，zhangsan&#x27;)                    &#125;else&#123;                        alert(&#x27;请输入正确的用户名或密码&#x27;);                    &#125;                &#125;                        &#125;    &#125;,&#125;)&lt;/script&gt;1234567891011121314151617181920212223242526272829303132333435\n\n其他事件修饰符举例\n1234567891011121314151617181920&lt;!-- 阻止单击事件继续传播 --&gt;&lt;a v-on:click.stop=&quot;doThis&quot;&gt;&lt;/a&gt;&lt;!-- 提交事件不再重载页面 --&gt;&lt;form v-on:submit.prevent=&quot;onSubmit&quot;&gt;&lt;/form&gt;&lt;!-- 修饰符可以串联 --&gt;&lt;a v-on:click.stop.prevent=&quot;doThat&quot;&gt;&lt;/a&gt;&lt;!-- 只有修饰符 --&gt;&lt;form v-on:submit.prevent&gt;&lt;/form&gt;&lt;!-- 添加事件监听器时使用事件捕获模式 --&gt;&lt;!-- 即元素自身触发的事件先在此处理，然后才交由内部元素进行处理 --&gt;&lt;div v-on:click.capture=&quot;doThis&quot;&gt;...&lt;/div&gt;&lt;!-- 只当在 event.target 是当前元素自身时触发处理函数 --&gt;&lt;!-- 即事件不是从内部元素触发的 --&gt;&lt;div v-on:click.self=&quot;doThat&quot;&gt;...&lt;/div&gt;12345678910111213141516171819\n\n按键码1234567891011121314151617181920212223&lt;body&gt;    &lt;div id=&quot;app&quot;&gt;               &lt;form action=&quot;&quot; @submit.prevent=&#x27;yzbd&#x27;&gt;            &lt;input type=&quot;text&quot; v-on:keyup.enter=&#x27;yzbd&#x27;&gt;        &lt;/form&gt;    &lt;/div&gt;&lt;/body&gt;&lt;script&gt;var vm =new Vue(&#123;    el:&quot;#app&quot;,    data:&#123;        user:&#x27;&#x27;,        pwd:&#x27;&#x27;    &#125;,    methods: &#123;        yzbd:function()&#123;            alert(&#x27;你按回车了&#x27;);        &#125;    &#125;,&#125;)&lt;/script&gt;12345678910111213141516171819202122\n\n其它按键码：\n12345678910.enter.tab.delete.esc.space.up.down.left.right123456789\n\n\n\nv-if 条件渲染v-if用于做条件化的渲染，当组件的判断条件发生改变，这个组件会被销毁并重建。\n1234567891011121314151617181920&lt;template&gt;    &lt;div class=&quot;main&quot;&gt;    &lt;span v-if=&quot;display&quot;&gt;我叫001&lt;/span&gt;    &lt;span v-if=&quot;!display&quot;&gt;我叫002&lt;/span&gt;    &lt;button @click=&quot;changeShow&quot;&gt;切换&lt;/button&gt;    &lt;/div&gt;&lt;/template&gt;javascript    data()&#123;        return&#123;            display:true        &#125;    &#125;,    methods:&#123;        changeShow()&#123;          this.display = !this.display;        &#125;,    &#125;12345678910111213141516171819\n\nv-if v-else12345678910111213141516171819202122&lt;template&gt;\t&lt;body&gt;\t    &lt;div id=&quot;app&quot;&gt;\t        &lt;div v-if=&#x27;Math.random()&gt;0.5&#x27;&gt;\t            you can see me!\t        &lt;/div&gt;\t        &lt;div v-else&gt;\t            now,you can&#x27;t see me!\t        &lt;/div&gt;\t    &lt;/div&gt;\t&lt;/body&gt;&lt;/template&gt;&lt;script&gt;    var vm = new Vue(&#123;        el: &#x27;#app&#x27;,        data: &#123;         ok:true        &#125;,    &#125;);&lt;/script&gt;123456789101112131415161718192021\n\nv-if v-else-if v-else123456789101112131415161718192021222324252627&lt;template&gt;&lt;body&gt;    &lt;div id=&quot;app&quot;&gt;        &lt;div v-if=&#x27;name==&quot;XXX&quot;&#x27;&gt;            XXX        &lt;/div&gt;        &lt;div v-else-if=&#x27;name==&quot;XXX&quot;&#x27;&gt;            XXX        &lt;/div&gt;        &lt;div v-else-if=&#x27;name==&quot;XXX&quot;&#x27;&gt;           XXX        &lt;/div&gt;        &lt;div v-else&gt;            木有！！！        &lt;/div&gt;    &lt;/div&gt;&lt;/body&gt;&lt;/template&gt;&lt;script&gt;    var vm = new Vue(&#123;        el: &#x27;#app&#x27;,        data: &#123;            name:&#x27;XXX&#x27;        &#125;,    &#125;);&lt;/script&gt;1234567891011121314151617181920212223242526\n\nv-show123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;template&gt;&lt;head&gt;    &lt;style&gt;        .tabcss &#123;            width: 100%;        &#125;        .tabstyle &#123;            display: inline-block;            width: 100px;        &#125;        .cur &#123;            background: cornsilk;        &#125;    &lt;/style&gt;&lt;/head&gt;&lt;body&gt;    &lt;div id=&quot;app&quot;&gt;       &lt;div class=&#x27;tabcss&#x27;&gt;            &lt;div id=&quot;tab1&quot; @click=&#x27;tabid=0&#x27; class=&#x27;tabstyle&#x27; v-bind:class=&quot;&#123;&#x27;cur&#x27;:tabid==0&#125;&quot;&gt;标签1&lt;/div&gt;            &lt;div id=&quot;tab2&quot;@click=&#x27;tabid=1&#x27;  class=&#x27;tabstyle&#x27; v-bind:class=&quot;&#123;&#x27;cur&#x27;:tabid==1&#125;&quot;&gt;标签2&lt;/div&gt;       &lt;/div&gt;       &lt;div class=&#x27;box&#x27;&gt;            &lt;div v-show=&#x27;tabid==0&#x27;&gt;                标签1里的内容                Lorem, ipsum dolor sit amet consectetur adipisicing elit. Sunt eius quo ab, quasi, dolorem asperiores ratione odit placeat iusto nostrum deleniti non. Quis, tenetur! Quam nisi voluptatum ipsa. Dolor, qui!            &lt;/div&gt;            &lt;div v-show=&quot;tabid==1&quot;&gt;                    标签2里的内容                Lorem ipsum dolor sit amet, consectetur adipisicing elit. Dolor eligendi dolorum cum sapiente sed quae accusamus odit! Deleniti ducimus perferendis temporibus sint, consequatur ipsa commodi! Possimus, nobis. Cumque, unde placeat?            &lt;/div&gt;        &lt;/div&gt;    &lt;/div&gt;&lt;/body&gt;&lt;/template&gt;&lt;script&gt;    var vm = new Vue(&#123;        el: &#x27;#app&#x27;,        data: &#123;            tabid:0,            ok:true,            name:&#x27;XXX&#x27;        &#125;,    &#125;);&lt;/script&gt;","categories":["vue"],"tags":["python"]},{"title":"Vue.js 安装 及简单使用，详细介绍","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/10/26/Vue.js 安装 及简单使用，详细介绍/","content":"准备 Vue.js 之前要先有 node.js。\n\nnode.js 安装网址：https://nodejs.org/en/download/。\n具体安装步骤参考：https://www.runoob.com/nodejs/nodejs-install-setup.html\n\nvue.js 有著名的全家桶系列123包含了vue-router，vuex， vue-resource，再加上构建工具vue-cli，就是一个完整的vue项目的核心构成。12\n\n安装好后，就要\n安装 Vue.js了。\n安装node.js，安装完node.js之后，npm也会自动安装\n查询是否安装成功的命令：\n1234node -vnpm -v123\n\n\n\n全局安装脚手架工具vue-cli，命令如下：\n12npm install --global vue-cli1\n\n\n\nvue项目初始化命令如下，若没有安装webpack，则先安装webpack\n123npm install -g webpack   // 安装 webpacknpm cache clean --force    // 清除 npm 缓存12\n\n项目\n1234567891011121314151617181920vue init webpack myVue // myVue 项目名称project name（项目名称）  按回车project description（项目描述） 按回车author（作者） 按回车vue build（解释器） 按回车Install vue-router(路由) 按y按回车use aslantto lint your code(代码检查) 按n按回车set up unit tests(测试模块) 按n按回车setup e2e tests with night watch(是否安装e2e) 按n按回车should we run ’npm install’(是否选择npm方式)选第一个，按回车12345678910111213141516171819\n\n\n\n初始化完成后的vue项目目录如下：\n\n\n进入到myVue目录下，使用npm install 安装package.json包中的依赖\n命令如下：\n1234cd myVuenpm install123\n\n\n\n运行项目：\n12npm run dev1\n\n在浏览器上输入：localhost:8080，将会出现下面的vue初始页面：\n\n\n结束项目运行：\nctrl+c，选择Y即可停止项目的运行\n\n\n\n二、vue项目目录说明\n\n\n\n文件名\n介绍\n\n\n\nbuild\n项目构建(webpack)相关代码\n\n\nconfig\n配置目录，包括端口号等\n\n\nnode_modules\nnpm加载的项目依赖块\n\n\nsrc\n这里是我们要开发的目录，基本上要做的事情都在这个目录里。里面包含了几个目录及文件：\n\n\nassets\n放置一些图片，如logo等\n\n\ncomponents\n该目录里存放的我们的开发文件组件，主要的开发文件都存放在这里了\n\n\nApp.vue\n项目入口文件\n\n\nmain.js\n项目的核心文件\n\n\nrouter\n路由配置目录\n\n\nstatic\n放置一些静态资源文件\n\n\ntest\n初始测试目录，可删除\n\n\n.xxxx文件\n这些是一些配置文件，包括语法配置，git配置等\n\n\nindex.html\n首页入口文件\n\n\npackage.json\n项目配置文件\n\n\nREADME.md\n项目的说明文档，markdown 格式\n\n\n三、vue项目启动流程\n在执行npm run dev的时候，会去在当前文件夹下的项目中找package.json文件,启动开发服务器，默认端口是8080；\n\n\n找到src的main.js文件，在该文件中new Vue的实例，要加载的模板内容App；\n\n\nApp是src目录下的App.vue结尾的文件；\n\n\n在App.vue所对应的模板当中，有一个router-view在src目录下有一个router文件夹，该文件夹有个index.js文件，该文件是配置路由词典，指定了路由地址是空，加载HelloWorld组件\n\n\n\n1注：刚才也说过了，vue运行是基于node环境的，所以 构建vue框架之前，一定要确保node环境安装成功\n\n四、Vue的组件的使用\n在components文件夹下创建.vue结尾的文件\n例如在：src/components/public/ 目录下新建 header.vue 文件\n\nheader.vue文件内容如下：\n\n\n在路由配置文件src/router/index.js中引入组件并配置组件路由\n\n\n\n引入组件\n12import Header from &#x27;@/components/public/header&#x27;1\n\n配置组件路由\n123456&#123;\tpath: &#x27;/header&#x27;,\tname: &#x27;header&#x27;,\tcomponent: Header&#125;12345\n\n\n\n\n\n运行项目：npm run dev\n\n在浏览器中输入：localhost:8080/header ,显示如下页面：\n\n\n\n附：vue生命周期示意图\n","categories":["vue"],"tags":["python"]},{"title":"gitee 分支 命令大全","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/10/14/gitee 分支 命令大全/","content":"在安装好 gitee 之后。我们会有新的一些业务需求。场景：L在公司中再使用 gitee 上传自己的代码，一天上传一次~那其他人的代码怎么办、岂不是乱套了。那全部写完之后再上传，可是项目进度又要每天核实。这个时候就体会到了分支的作用：\n分支管理创建简易的命令行入门教程:Git 全局设置:\n12git config --global user.name &quot;****&quot;git config --global user.email &quot;*******&quot;\n\n创建 git 仓库:1234567mkdir djangcd djanggit inittouch README.mdgit add README.mdgit commit -m &quot;first commit&quot;git remote add origin git@地址\n\n已有仓库?123cd existing_git_repogit remote add origin git@gitee.com:H_sen&#x2F;djang.gitgit push -u origin master\n\n123456789101112# 默认在当前目录下创建和版本库名相同的文件夹并下载版本到该文件夹下git clone &lt;远程仓库的网址&gt;# 指定本地仓库的目录git clone &lt;远程仓库的网址&gt; &lt;本地目录&gt;# -b 指定要克隆的分支，默认是master分支git clone &lt;远程仓库的网址&gt; -b &lt;分支名称&gt; &lt;本地目录&gt;\n12345678910111213# 新建分支 并转移到此位置git checkout -b NewBranch# 查看所有分支git branch# 切换分支 切换到 mastergit checkout master# 删除本地分支 NewBranchgit branch -D NewBranch\n\n\n场景：假设我们现在创建分支 NewBranch 已经成功，如果需要对此传入代码。123git add -A (或文件名)git commit -m &quot;new branch&quot;git push origin NewBrLanch   # 将代码上传到分支\n\n场景：假设项目全部完毕，有多个分支~我们需要将他们合并\n合并分支到主分支1234567891011121314# 回到主分支 mastergit checkout master# 将主分支数据拉到本地git pull# 强行合并 (可能会出错， master很有可能也更新)git merge NewBranch# 假设说上面的 和合并出现问题 那就手动合并vim Readme   # 手动合并标识文件git add -Agit push origin master    # 合并之后 就可以提交到 master 了# 查看从什么地方出现的分支git log -graph\n现在都可以理解 为什么要用分支了。而且再公司中 分支也是有规定的，如 master 不可以轻易的上传啦，要用 dev 开发的分支进行项目开发，而每个组还要有 pyTeam 分支，知道组中的我 H_sen 分支。只有等测试排除所有的问题后才会发布到 master 分支。那么会不会有 BUG 分支呢？ 有！！！\nBug分支场景：假设说所有的拦截手段还是没有挡住 BUG 上传到 master (没错，你写的)，而你这个时候正在开发别的模块。怎么办呢？\n\n停下手头的工作，\n切换 master 分支去解决 master 上的 bug这样会很麻烦\n\ngit stash 脱颖而出如何使用：\n\n找到 bug\n1234# 突然出现的 bugvim Readme # 将 dev 中未保存的代码存放到临时区git stash    # 这样做完全的避免了 将 代码带入到 Bug 分支\n修复 bug\n123456# 切换到 bug 分支git checkout -b bug-100# 切换后 修复 bugvim Readmegit add -Agit commit -m &quot;修复 bug&quot;  # 提交到工作区\n\n将修复后的 bug 和 master 合并\n123456# 切换到 mastergit checkout master# 强行合并git merge bug-100# 推送 到主分支git origin master\n\n\n\nbug 排完了。之前的文件怎么恢复状态?\n\n找回之前的 dev 分支，并恢复 状态\n\n123456789# 切换回开发的分支git checkout dev# 查看状态git statusmore Readme    # 发现之前工作区 未提交的数据都消失了git stash list    # 查看之前使用 git stash 保存的数据git stash apply    # 恢复之前的文件# 查看状态git status    # 此时就可以看到排bug 之前的完美状态了\n\ngit stash 其他操作12345678git stash drop  # 删除最久的那个 使用 git stash 临时保持状态git stash apply stash&#123;&#123;0&#125;&#125;  # 指定恢复到那个临时状态git stash pop    # 恢复并删除上一个临时状态&#39;&#39;&#39;注意！！！ git stash aplpy恢复后 stash的内容并不删除，需要 git stash drop 才能删除git stash pop 就简单许多 恢复的时候直接删除&#39;&#39;&#39;\n","categories":["git"],"tags":["python"]},{"title":"Celery在Django 项目中如何使用","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/09/30/Celery在Django 项目中如何使用/","content":"创建目录celery_pro，并在celery_pro下创建下面两个文件celery.py\n123456789101112131415161718192021222324252627282930313233# celery.py# -*- coding:utf8 -*-from __future__ import absolute_import, unicode_literals#1. absolute_import 可以使导入的celery是python绝对路基的celery模块，不是当前我们创建的celery.py#2. unicode_literals 模块可能是python2和3兼容的，不知道from celery import Celery# from .celery import Celery        #这样才是导入当前目录下的celery\t\t\t# 填写你的项目名app &#x3D; Celery(&#39;project&#39;,             broker&#x3D;&#39;redis:&#x2F;&#x2F;localhost&#39;,             backend&#x3D;&#39;redis:&#x2F;&#x2F;localhost&#39;,             include&#x3D;[&#39;celery_pro.tasks&#39;,                      &#39;celery_pro.tasks2&#39;,                      ])#celery——pro是存放celery文件的文件夹名字#实例化时可以添加下面这个属性app.conf.update(   result_expires&#x3D;3600,        #执行结果放到redis里，一个小时没人取就丢弃)# 配置定时任务：每5秒钟执行 调用一次celery_pro下tasks.py文件中的add函数app.conf.beat_schedule &#x3D; &#123;    &#39;add-every-5-seconds&#39;: &#123;        &#39;task&#39;: &#39;celery_pro.tasks.add&#39;,  # 寻找tasks下面的add函数        &#39;schedule&#39;: 5.0,        &#39;args&#39;: (16, 16)    &#125;,&#125;app.conf.timezone &#x3D; &#39;UTC&#39;   # 配置的时间规范if __name__ &#x3D;&#x3D; &#39;__main__&#39;:   app.start()\ntask.py123456789# task.py# -*- coding:utf8 -*-from __future__ import absolute_import, unicode_literalsfrom .celery import app       #从当前目录导入app#写一个add函数@app.taskdef add(x, y):    return x + y\n\ntask2.py123456789101112# task2.py# -*- coding:utf8 -*-from __future__ import absolute_import, unicode_literalsfrom .celery import appimport time,random@app.taskdef randnum(start,end):    time.sleep(3)    return random.randint(start,end)tasks2.py\ntouch init.py在celery_pro目录下新建init.py文件，否则执行命令时会报错\n执行下面两条命令即可让celery定时执行任务了\n启动一个worker：在celery_pro外层目录下执行celery -A celery_pro worker -l info\n\n启动任务调度器 celery beatcelery -A celery_pro beat -l info\n\n执行效果看到celery运行日志中每5秒回返回一次 add函数执行结果\n\n\n启动celery的worker：每台机器可以启动8个worker\n在pythondir目录下启动 /pythondir/celery_pro/ 目录下的worker\n\ncelery -A celery_pro worker -l info\n\n后台启动worker：/pythondir/celery_pro/目录下执行\n\n123456789celery multi start w1 -A celery_pro -l info             #在后台启动w1这个workercelery multi start w1 w2 -A celery_pro -l info       #一次性启动w1,w2两个workercelery -A celery_pro status                       #查看当前有哪些worker在运行celery multi stop w1 w2 -A celery_pro                   #停止w1,w2两个workercelery multi restart w1 w2 -A celery_pro               #重启w1,w2两个worker\n也可以手动给celery分配任务：在/pythondir/下执行123456789python3from celery_pro import tasks,tasks2t1 &#x3D; tasks.add.delay(34,3)t2 &#x3D; tasks2.randnum.delay(1,10000)             t1.get()t2.get()手动给celery分配任务：在&#x2F;pythondir&#x2F;下执行\n\ncelery与Django项目最佳实践123456789pip3 install Django&#x3D;&#x3D;2.0.4pip3 install celery&#x3D;&#x3D;4.3.0pip3 install redis&#x3D;&#x3D;3.2.1pip3 install ipython&#x3D;&#x3D;7.6.1 find .&#x2F; -type f | xargs sed -i &#39;s&#x2F;\\r$&#x2F;&#x2F;g&#39;     # 批量将当前文件夹下所有文件装换成unix格式celery  multi start celery_test -A celery_test -l debug --autoscale&#x3D;50,5        # celery并发数：最多50个，最少5个http:&#x2F;&#x2F;docs.celeryproject.org&#x2F;en&#x2F;latest&#x2F;reference&#x2F;celery.bin.worker.html#cmdoption-celery-worker-autoscaleps auxww|grep &quot;celery worker&quot;|grep -v grep|awk &#39;&#123;print $2&#125;&#39;|xargs kill -9       # 关闭所有celery进程\n\n在Django中使用celery介绍（celery无法再windows下运行）\n在Django中使用celery时，celery文件必须以tasks.py\n\nDjango会自动到每个APP中找tasks.py文件\n\n\n创建一个Django项目celery_test，和app01在与项目同名的目录下创建celery.py123456789101112131415161718192021# celery.py# -*- coding: utf-8 -*-from __future__ import absolute_importimport osfrom celery import Celery# 只要是想在自己的脚本中访问Django的数据库等文件就必须配置Django的环境变量os.environ.setdefault(&#39;DJANGO_SETTINGS_MODULE&#39;, &#39;celery_test.settings&#39;)# app名字app &#x3D; Celery(&#39;celery_test&#39;)# 配置celeryclass Config:    BROKER_URL &#x3D; &#39;redis:&#x2F;&#x2F;192.168.56.11:6379&#39;    CELERY_RESULT_BACKEND &#x3D; &#39;redis:&#x2F;&#x2F;192.168.56.11:6379&#39;app.config_from_object(Config)# 到各个APP里自动发现tasks.py文件app.autodiscover_tasks()\n\n在与项目同名的目录下的 init.py 文件中添加下面内容12345678# __init__.py # -*- coding:utf8 -*-from __future__ import absolute_import, unicode_literals# 告诉Django在启动时别忘了检测我的celery文件from .celery import app as celery_ap__all__ &#x3D; [&#39;celery_app&#39;]\n\n创建app01/tasks.py文件12345678910# tasks.py# -*- coding:utf8 -*-from __future__ import absolute_import, unicode_literalsfrom celery import shared_task# 这里不再使用@app.task,而是用@shared_task，是指定可以在其他APP中也可以调用这个任务@shared_taskdef add(x, y):   return x + y\n\n在setings.py文件指定redis服务器的配置123# settings.pyCELERY_BROKER_URL &#x3D; &#39;redis:&#x2F;&#x2F;localhost&#39;CELERY_RESULT_BACKEND &#x3D; &#39;redis:&#x2F;&#x2F;localhost&#39;\n\n\n将celery_test这个Django项目拷贝到centos7.3的django_test文件夹中保证启动了redis-server启动一个celery的worker1celery -A celery_test worker -l info\n\n在Linux中启动 Django项目1python3 manage.py runserver 0.0.0.0:9000\n访问http://1.1.1.3:9000/celery_call/ 获取任务id\n根据11中的任务id获取对应的值http://1.1.1.3:9000/celery_result/?id=5065b65b-0c01-430a-a67f-9531fe3e8d90\n基于步骤↑：在django中使用计划任务功能在Django中使用celery的定时任务需要安装django-celery-beat1pip3 install django-celery-beat\n\n在Django的settings中注册django_celery_beat1234INSTALLED_APPS &#x3D; (        ...,        &#39;django_celery_beat&#39;,    )\n\n执行创建表命令123456python3 manage.py makemigrationspython3 manage.py migratepython3 manage.py startsuperuser\n\n运行Django项目123celery -A celery_test worker -l infopython3 manage.py runserver 0.0.0.0:9000\n\n登录 http://1.1.1.3:9000/admin/ 可以看到多了三张表\n在intervals表中添加一条每5秒钟执行一次的任务的时钟![~`](https://img-blog.csdnimg.cn/20200222231733989.png)\n在Periodic tasks表中创建任务\n在/django_test/celery_test/目录下执行下面命令123456789celery -A celery_test worker -l info                                                   #启动一个workerpython manage.py runserver 0.0.0.0:9000                           #运行Django项目celery -A celery_test beat -l info -S django                                                   #启动心跳任务说明：运行上面命令后就可以看到在运行celery -A celery_test worker -l info         窗口中每5秒钟执行一次app01.tasks.add： 2+3&#x3D;5\n关于添加新任务必须重启心跳问题\n每次在Django表中添加一个任务就必须重启一下beat\n\n但是Django中有一个djcelery插件可以帮助我们不必重启\ncdjango+celery+redis实现异步周期任务注：python的celery模块 4.2.0版本， 刚开始安装的未4.1.1版本，但是定时任务居然不执行\n\n在settings.py中配置celery123456789101112131415161718192021222324# settings.py#1、如果在django中需要周期性执行，在这里需要注册 django_celery_beat 中间件INSTALLED_APPS &#x3D; [    &#39;&#39;&#39;    &#39;django_celery_beat&#39;,    &#39;&#39;&#39;]TIME_ZONE &#x3D; &#39;Asia&#x2F;Shanghai&#39;  # 将默认的UTC时区给成中国时区#2、celery：配置celeryBROKER_URL &#x3D; &#39;redis:&#x2F;&#x2F;localhost:6379&#39;CELERY_RESULT_BACKEND &#x3D; &#39;redis:&#x2F;&#x2F;localhost:6379&#39;CELERY_ACCEPT_CONTENT &#x3D; [&#39;application&#x2F;json&#39;]CELERY_TASK_SERIALIZER &#x3D; &#39;json&#39;CELERY_RESULT_SERIALIZER &#x3D; &#39;json&#39;CELERY_TASK_RESULT_EXPIRES &#x3D; 60 * 60CELERY_TIMEZONE &#x3D; &#39;Asia&#x2F;Shanghai&#39;CELERY_ENABLE_UTC&#x3D;FalseCELERY_ANNOTATIONS &#x3D; &#123;&#39;*&#39;: &#123;&#39;rate_limit&#39;: &#39;500&#x2F;s&#39;&#125;&#125;CELERYBEAT_SCHEDULER &#x3D; &#39;djcelery.schedulers.DatabaseScheduler&#39;\n\n在与项目同名的目录下创建celery.py更多定时参考官网：http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html#crontab-schedules\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124# # -*- coding: utf-8 -*-from __future__ import absolute_importimport osfrom celery import Celeryfrom celery.schedules import crontabfrom datetime import timedeltafrom kombu import Queue# set the default Django settings module for the &#39;celery&#39; program.os.environ.setdefault(&#39;DJANGO_SETTINGS_MODULE&#39;, &#39;celery_test.settings&#39;)from django.conf import settingsapp &#x3D; Celery(&#39;celery_test&#39;)# Using a string here means the worker will not have to# pickle the object when using Windows.class Config:    BROKER_URL &#x3D; &#39;redis:&#x2F;&#x2F;1.1.1.3:6379&#39;    CELERY_RESULT_BACKEND &#x3D; &#39;redis:&#x2F;&#x2F;1.1.1.3:6379&#39;    CELERY_ACCEPT_CONTENT &#x3D; [&#39;application&#x2F;json&#39;]    CELERY_TASK_SERIALIZER &#x3D; &#39;json&#39;    CELERY_RESULT_SERIALIZER &#x3D; &#39;json&#39;    CELERY_TIMEZONE &#x3D; &#39;Asia&#x2F;Shanghai&#39;    ENABLE_UTC &#x3D; False    CELERY_TASK_RESULT_EXPIRES &#x3D; 60 * 60    CELERY_ANNOTATIONS &#x3D; &#123;&#39;*&#39;: &#123;&#39;rate_limit&#39;: &#39;500&#x2F;s&#39;&#125;&#125;    # 每次取任务的数量    # CELERYD_PREFETCH_MULTIPLIER &#x3D; 10    # 每个worker执行多少次任务之后就销毁，防止内存泄漏。相当于--maxtasksperchild参数    CELERYD_MAX_TASKS_PER_CHILD &#x3D; 16    # 防止死锁    # CELERYD_FORCE_EXECV &#x3D; True    # 任务发出后，经过一段时间还未收到acknowledge , 就将任务重新交给其他worker执行    # CELERY_DISABLE_RATE_LIMITS &#x3D; True    # CELERYBEAT_SCHEDULER &#x3D; &#39;djcelery.schedulers.DatabaseScheduler&#39;app.config_from_object(Config)app.autodiscover_tasks()#crontab configapp.conf.update(    CELERYBEAT_SCHEDULE &#x3D; &#123;        # 每隔三分钟执行一次add函数        &#39;every-3-min-add&#39;: &#123;            &#39;task&#39;: &#39;app01.tasks.add&#39;,            &#39;schedule&#39;: timedelta(seconds&#x3D;180)        &#125;,        # 每天下午15:420执行        &#39;add-every-day-morning@14:50&#39;: &#123;            &#39;task&#39;: &#39;app01.tasks.minus&#39;,            &#39;schedule&#39;: crontab(hour&#x3D;15, minute&#x3D;20, day_of_week&#x3D;&#39;*&#x2F;1&#39;),        &#125;,    &#125;,)Queue(&#39;transient&#39;, routing_key&#x3D;&#39;transient&#39;,delivery_mode&#x3D;1)celery.py# -*- coding: utf-8 -*-from __future__ import absolute_importimport osfrom celery import Celeryfrom celery.schedules import crontabfrom datetime import timedeltafrom kombu import Queue# set the default Django settings module for the &#39;celery&#39; program.os.environ.setdefault(&#39;DJANGO_SETTINGS_MODULE&#39;, &#39;celery_test.settings&#39;)from django.conf import settingsapp &#x3D; Celery(&#39;celery_test&#39;)# Using a string here means the worker will not have to# pickle the object when using Windows.class Config:    BROKER_URL &#x3D; &#39;redis:&#x2F;&#x2F;1.1.1.3:6379&#39;    CELERY_RESULT_BACKEND &#x3D; &#39;redis:&#x2F;&#x2F;1.1.1.3:6379&#39;    CELERY_ACCEPT_CONTENT &#x3D; [&#39;application&#x2F;json&#39;]    CELERY_TASK_SERIALIZER &#x3D; &#39;json&#39;    CELERY_RESULT_SERIALIZER &#x3D; &#39;json&#39;    CELERY_TIMEZONE &#x3D; &#39;Asia&#x2F;Shanghai&#39;    ENABLE_UTC &#x3D; False    CELERY_TASK_RESULT_EXPIRES &#x3D; 60 * 60    CELERY_ANNOTATIONS &#x3D; &#123;&#39;*&#39;: &#123;&#39;rate_limit&#39;: &#39;500&#x2F;s&#39;&#125;&#125;    # 每次取任务的数量    # CELERYD_PREFETCH_MULTIPLIER &#x3D; 10    # 每个worker执行多少次任务之后就销毁，防止内存泄漏。相当于--maxtasksperchild参数    CELERYD_MAX_TASKS_PER_CHILD &#x3D; 16    # 防止死锁    # CELERYD_FORCE_EXECV &#x3D; True    # 任务发出后，经过一段时间还未收到acknowledge , 就将任务重新交给其他worker执行    # CELERY_DISABLE_RATE_LIMITS &#x3D; True    # CELERYBEAT_SCHEDULER &#x3D; &#39;djcelery.schedulers.DatabaseScheduler&#39;app.config_from_object(Config)app.autodiscover_tasks()#crontab configapp.conf.update(    CELERYBEAT_SCHEDULE &#x3D; &#123;        # 每隔三分钟执行一次add函数        &#39;every-3-min-add&#39;: &#123;            &#39;task&#39;: &#39;app01.tasks.add&#39;,            &#39;schedule&#39;: timedelta(seconds&#x3D;180)        &#125;,        # 每天下午15:420执行        &#39;add-every-day-morning@14:50&#39;: &#123;            &#39;task&#39;: &#39;app01.tasks.minus&#39;,            &#39;schedule&#39;: crontab(hour&#x3D;15, minute&#x3D;20, day_of_week&#x3D;&#39;*&#x2F;1&#39;),        &#125;,    &#125;,)Queue(&#39;transient&#39;, routing_key&#x3D;&#39;transient&#39;,delivery_mode&#x3D;1)\n\n在任意app下创建tasks.py (django会自动到各app中找到此tasks文件)12345678910111213141516# tasks.py# -*- coding:utf8 -*-from __future__ import absolute_import, unicode_literalsfrom celery import shared_task# 这里不再使用@app.task,而是用@shared_task，是指定可以在其他APP中也可以调用这个任务@shared_taskdef add():   print &#39;app01.tasks.add&#39;   return 222 + 333@shared_taskdef minus():   print &#39;app01.tasks.minus&#39;   return 222 - 333\n\n在与项目同名的目录下的 init.py 文件中添加下面内容123456789# __init__.py# -*- coding:utf8 -*-from __future__ import absolute_import, unicode_literals# 告诉Django在启动时别忘了检测我的celery文件from .celery import app as celery_ap__all__ &#x3D; [&#39;celery_app&#39;]\n\n启动脚本（记得开启celery服务）启动django程序1python manage.py runserver 0.0.0.0:8000\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# service.sh#!&#x2F;usr&#x2F;bin&#x2F;env bashsource ..&#x2F;env&#x2F;bin&#x2F;activateexport DJANGO_SETTINGS_MODULE&#x3D;celery_test.settingsbase_dir&#x3D;&#96;pwd&#96;mup_pid() &#123;echo &#96;ps -ef | grep -E &quot;(manage.py)(.*):8000&quot; | grep -v grep| awk &#39;&#123;print $2&#125;&#39;&#96;&#125;start() &#123; python $base_dir&#x2F;manage.py runserver 0.0.0.0:8000 &amp;&gt;&gt; $base_dir&#x2F;django.log 2&gt;&amp;1 &amp; pid&#x3D;$(mup_pid) echo -e &quot;\\e[00;31mmup is running (pid: $pid)\\e[00m&quot;&#125;stop() &#123; pid&#x3D;$(mup_pid) echo -e &quot;\\e[00;31mmup is stop (pid: $pid)\\e[00m&quot; ps -ef | grep -E &quot;(manage.py)(.*):8000&quot; | grep -v grep| awk &#39;&#123;print $2&#125;&#39; | xargs kill -9 &amp;&gt; &#x2F;dev&#x2F;null&#125;restart()&#123;    stop    start&#125;# See how we were called.case &quot;$1&quot; in  start)        start        ;;  stop)        stop        ;;  restart)        restart        ;;  *)        echo $&quot;Usage: $0 &#123;start|stop|restart&#125;&quot;        exit 2esac\n\n启动celery的worker：每台机器可以启动8个worker1celery -A celery_test worker -l info\n12345678910111213141516171819202122232425262728293031323334353637383940414243# start-celery.sh#!&#x2F;bin&#x2F;bashsource ..&#x2F;env&#x2F;bin&#x2F;activateexport C_FORCE_ROOT&#x3D;&quot;true&quot;base_dir&#x3D;&#96;pwd&#96;celery_pid() &#123;    echo &#96;ps -ef | grep -E &quot;celery -A celery_test worker&quot; | grep -v grep| awk &#39;&#123;print $2&#125;&#39;&#96;&#125;start() &#123;    celery  multi start celery_test -A celery_test -l debug --autoscale&#x3D;50,5 --logfile&#x3D;$base_dir&#x2F;var&#x2F;celery-%I.log --pidfile&#x3D;celery_test.pid&#125;restart() &#123;    celery  multi restart celery_test -A celery_test -l debug&#125;stop() &#123;    celery  multi stop celery_test -A celery_test -l debug&#125;#restart()&#123;#    stop#    start#&#125;# See how we were called.case &quot;$1&quot; in  start)        start        ;;  restart)        restart        ;;  stop)        stop        ;;  *)        echo $&quot;Usage: $0 &#123;start|stop|restart&#125;&quot;        exit 2esac#nohup celery -A celery_test worker -l debug --concurrency&#x3D;10 --autoreload  &amp; &gt;&gt;celery.log\n启动celery 定时任务运行1celery -A celery_test beat -l debug\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546# celery-crond.sh#!&#x2F;bin&#x2F;bash#celery 定时任务运行source ..&#x2F;env&#x2F;bin&#x2F;activateexport C_FORCE_ROOT&#x3D;&quot;true&quot;base_dir&#x3D;&#96;pwd&#96;celery_pid() &#123;    echo &#96;ps -ef | grep -E &quot;celery -A celery_test beat&quot; | grep -v grep| awk &#39;&#123;print $2&#125;&#39;&#96;&#125;start() &#123;    #django 调度定时任务    #celery -A celery_test beat -l info -S django &gt;&gt; $base_dir&#x2F;var&#x2F;celery-cron.log 2&gt;&amp;1 &amp;    celery -A celery_test beat -l debug &gt;&gt; $base_dir&#x2F;var&#x2F;Scheduler.log 2&gt;&amp;1 &amp;    sleep 3    pid&#x3D;$(celery_pid)    echo -e &quot;\\e[00;31mcelery is start (pid: $pid)\\e[00m&quot;&#125;restart() &#123;    pid&#x3D;$(celery_pid)    echo -e &quot;\\e[00;31mcelery is restart (pid: $pid)\\e[00m&quot;    ps auxf | grep -E &quot;celery -A celery_test beat&quot; | grep -v grep| awk &#39;&#123;print $2&#125;&#39; | xargs kill -HUP &amp;&gt; &#x2F;dev&#x2F;null&#125;stop() &#123;    pid&#x3D;$(celery_pid)    echo -e &quot;\\e[00;31mcelery is stop (pid: $pid)\\e[00m&quot;    ps -ef | grep -E &quot;celery -A celery_test beat&quot; | grep -v grep| awk &#39;&#123;print $2&#125;&#39; | xargs kill -TERM &amp;&gt; &#x2F;dev&#x2F;null&#125;case &quot;$1&quot; in  start)        start        ;;  restart)        restart        ;;  stop)        stop        ;;  *)        echo $&quot;Usage: $0 &#123;start|stop|restart&#125;&quot;        exit 2esac\nwindows下编写的脚本文件，放到Linux中无法识别格式在Linux中执行.sh脚本，异常/bin/sh^M: bad interpreter: No such file or directory\nset ff=unix\ndos2unix start-celery.shdos2unix celery-crond.sh\n常见报错Received unregistered task of type ‘XXX’ Celery报错（定时任务中无法找到对应tasks.py文件）\napp = Celery(‘opwf’, include=[‘api_workflow.tasks’]) # api_workflow这个app中的tasks文件\n博客参考地址\n","categories":["celery"],"tags":["python"]},{"title":"Celery 的使用","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/09/27/Celery 的使用/","content":"上一篇博客已经详细的介绍了 Celery 和他的工作流程 celery的介绍\n接下来 要演示 Celery 的使用\n安装1python pip install celery\n\n创建 tasks.py文件进行验证\n1234567891011121314151617181920212223242526# tasks.pyfrom celery import Celeryimport timeapp &#x3D; Celery(&#39;TASK&#39;,             broker&#x3D;&#39;redis:&#x2F;&#x2F;localhost&#39;,                     backend&#x3D;&#39;redis:&#x2F;&#x2F;localhost&#39;)@app.taskdef add(x, y):\t&#39;&#39;&#39;\t进行任务的添加\t&#39;&#39;&#39;    print(&quot;running..add.&quot;, x, y)    return x + y@app.taskdef minus(x, y):\t&#39;&#39;&#39;\t任务的减少\t&#39;&#39;&#39;\ttime.sleep(60)    print(&quot;running..minus.&quot;, x, y)    return x - y\n启动Celery Worker来开始监听并执行任务12celery -A tasks worker --loglevel&#x3D;info            # tasks是tasks.py文件：必须在tasks.py所在目录下执行\n调用任务：再打开两个终端，进行命令行模式，调用任务123456789101112&gt;&gt;&gt; import tasks&gt;&gt;&gt; t2 &#x3D; tasks.minus.delay(9,11)# 然后在另一个终端重复上面步骤执行&gt;&gt;&gt; import tasks&gt;&gt;&gt; t1 &#x3D; tasks.add.delay(3,4)&gt;&gt;&gt; t1.get()# 由于t2执行sleep了3s所以t1.get()需要等待\n\ncelery其他命令1234567&gt;&gt;&gt; t.ready()                  #返回true证明可以执行，不必等待&gt;&gt;&gt; t.get(timeout&#x3D;1)           #如果1秒不返回结果就超时,避免一直等待&gt;&gt;&gt; t.get(propagate&#x3D;False)     #如果执行的代码错误只会打印错误信息&gt;&gt;&gt; t.traceback                #打印异常详细结果\nCelery执行异步任务创建项目celerytest\n创建py文件：celery_app_task.py1234567891011121314import celeryimport time# broker&#x3D;&#39;redis:&#x2F;&#x2F;127.0.0.1:6379&#x2F;2&#39; 不加密码backend&#x3D;&#39;redis:&#x2F;&#x2F;:123456@127.0.0.1:6379&#x2F;1&#39;broker&#x3D;&#39;redis:&#x2F;&#x2F;:123456@127.0.0.1:6379&#x2F;2&#39;cel&#x3D;celery.Celery(&#39;test&#39;,backend&#x3D;backend,broker&#x3D;broker)@cel.task&#39;&#39;&#39;添加任务&#39;&#39;&#39;def add(x,y):    return x+y\n创建py文件：add_task.py,添加任务123from celery_app_task import addresult &#x3D; add.delay(4,5)print(result.id)\n创建py文件：run.py，执行任务，或者使用命令执行：celery worker -A celery_app_task -l infowindows下：celery worker -A celery_app_task -l info -P eventlet\n1234from celery_app_task import celif __name__ &#x3D;&#x3D; &#39;__main__&#39;:    cel.worker_main()    # cel.worker_main(argv&#x3D;[&#39;--loglevel&#x3D;info&#39;)\n\n创建py文件：result.py，查看任务执行结果1234567891011121314151617from celery.result import AsyncResultfrom celery_app_task import celasync &#x3D; AsyncResult(id&#x3D;&quot;e919d97d-2938-4d0f-9265-fd8237dc2aa3&quot;, app&#x3D;cel)if async.successful():    result &#x3D; async.get()    print(result)    # result.forget() # 将结果删除elif async.failed():    print(&#39;执行失败&#39;)elif async.status &#x3D;&#x3D; &#39;PENDING&#39;:    print(&#39;任务等待中被执行&#39;)elif async.status &#x3D;&#x3D; &#39;RETRY&#39;:    print(&#39;任务异常后正在重试&#39;)elif async.status &#x3D;&#x3D; &#39;STARTED&#39;:    print(&#39;任务已经开始被执行&#39;)\n执行 add_task.py，添加任务，并获取任务ID\n执行 run.py ，或者执行命令：celery worker -A celery_app_task -l info\n执行 result.py,检查任务状态并获取结果\n多任务结构1234567pro_cel    ├── celery_task      # celery相关文件夹    │   ├── celery.py    # celery连接和配置相关文件,必须叫这个名字    │   └── tasks1.py    #  所有任务函数    │   └── tasks2.py    #  所有任务函数    ├── check_result.py  # 检查结果    └── send_task.py     # 触发任务\ncelery.py1234567891011121314from celery import Celerycel &#x3D; Celery(&#39;celery_demo&#39;,             broker&#x3D;&#39;redis:&#x2F;&#x2F;127.0.0.1:6379&#x2F;1&#39;,             backend&#x3D;&#39;redis:&#x2F;&#x2F;127.0.0.1:6379&#x2F;2&#39;,             # 包含以下两个任务文件，去相应的py文件中找任务，对多个任务做分类             include&#x3D;[&#39;celery_task.tasks1&#39;,                      &#39;celery_task.tasks2&#39;                      ])# 时区cel.conf.timezone &#x3D; &#39;Asia&#x2F;Shanghai&#39;# 是否使用UTCcel.conf.enable_utc &#x3D; False\n\n\ntasks1.py1234567import timefrom celery_task.celery import cel@cel.taskdef test_celery(res):    time.sleep(5)    return &quot;test_celery任务结果:%s&quot;%res\ntasks2.py123456import timefrom celery_task.celery import cel@cel.taskdef test_celery2(res):    time.sleep(5)    return &quot;test_celery2任务结果:%s&quot;%res\n\ncheck_result.py1234567891011121314151617181920from celery.result import AsyncResultfrom celery_task.celery import celasync &#x3D; AsyncResult(id&#x3D;&quot;08eb2778-24e1-44e4-a54b-56990b3519ef&quot;, app&#x3D;cel)if async.successful():    result &#x3D; async.get()    print(result)    # result.forget() # 将结果删除,执行完成，结果不会自动删除    # async.revoke(terminate&#x3D;True)  # 无论现在是什么时候，都要终止    # async.revoke(terminate&#x3D;False) # 如果任务还没有开始执行呢，那么就可以终止。elif async.failed():    print(&#39;执行失败&#39;)elif async.status &#x3D;&#x3D; &#39;PENDING&#39;:    print(&#39;任务等待中被执行&#39;)elif async.status &#x3D;&#x3D; &#39;RETRY&#39;:    print(&#39;任务异常后正在重试&#39;)elif async.status &#x3D;&#x3D; &#39;STARTED&#39;:    print(&#39;任务已经开始被执行&#39;)\nsend_task.py12345678from celery_task.tasks1 import test_celeryfrom celery_task.tasks2 import test_celery2# 立即告知celery去执行test_celery任务，并传入一个参数result &#x3D; test_celery.delay(&#39;第一个的执行&#39;)print(result.id)result &#x3D; test_celery2.delay(&#39;第二个的执行&#39;)print(result.id)\n\n添加任务（执行send_task.py），开启work：celery worker -A celery_task -l info -P eventlet，检查任务执行结果（执行check_result.py）\nCelery执行定时任务设定时间让celery执行一个任务add_task.py1234567891011121314151617181920212223from celery_app_task import addfrom datetime import datetime# 方式一# v1 &#x3D; datetime(2019, 2, 13, 18, 19, 56)# print(v1)# v2 &#x3D; datetime.utcfromtimestamp(v1.timestamp())# print(v2)# result &#x3D; add.apply_async(args&#x3D;[1, 3], eta&#x3D;v2)# print(result.id)# 方式二ctime &#x3D; datetime.now()# 默认用utc时间utc_ctime &#x3D; datetime.utcfromtimestamp(ctime.timestamp())from datetime import timedeltatime_delay &#x3D; timedelta(seconds&#x3D;10)task_time &#x3D; utc_ctime + time_delay# 使用apply_async并设定时间result &#x3D; add.apply_async(args&#x3D;[4, 3], eta&#x3D;task_time)print(result.id)\n\n类似于crontab的定时任务多任务结构中celery.py修改如下12345678910111213141516171819202122232425262728293031from datetime import timedeltafrom celery import Celeryfrom celery.schedules import crontabcel &#x3D; Celery(&#39;tasks&#39;, broker&#x3D;&#39;redis:&#x2F;&#x2F;127.0.0.1:6379&#x2F;1&#39;, backend&#x3D;&#39;redis:&#x2F;&#x2F;127.0.0.1:6379&#x2F;2&#39;, include&#x3D;[    &#39;celery_task.tasks1&#39;,    &#39;celery_task.tasks2&#39;,])cel.conf.timezone &#x3D; &#39;Asia&#x2F;Shanghai&#39;cel.conf.enable_utc &#x3D; Falsecel.conf.beat_schedule &#x3D; &#123;    # 名字随意命名    &#39;add-every-10-seconds&#39;: &#123;        # 执行tasks1下的test_celery函数        &#39;task&#39;: &#39;celery_task.tasks1.test_celery&#39;,        # 每隔2秒执行一次        # &#39;schedule&#39;: 1.0,        # &#39;schedule&#39;: crontab(minute&#x3D;&quot;*&#x2F;1&quot;),        &#39;schedule&#39;: timedelta(seconds&#x3D;2),        # 传递参数        &#39;args&#39;: (&#39;test&#39;,)    &#125;,    # &#39;add-every-12-seconds&#39;: &#123;    #     &#39;task&#39;: &#39;celery_task.tasks1.test_celery&#39;,    #     每年4月11号，8点42分执行    #     &#39;schedule&#39;: crontab(minute&#x3D;42, hour&#x3D;8, day_of_month&#x3D;11, month_of_year&#x3D;4),    #     &#39;schedule&#39;: crontab(minute&#x3D;42, hour&#x3D;8, day_of_month&#x3D;11, month_of_year&#x3D;4),    #     &#39;args&#39;: (16, 16)    # &#125;,&#125;\n启动一个beat：celery beat -A celery_task -l info启动work执行：celery worker -A celery_task -l info -P eventletDjango中使用Celery在项目目录下创建celeryconfig.pydjcelery1234567891011121314djcelery.setup_loader()CELERY_IMPORTS&#x3D;(    &#39;app01.tasks&#39;,)#有些情况可以防止死锁CELERYD_FORCE_EXECV&#x3D;True# 设置并发worker数量CELERYD_CONCURRENCY&#x3D;4#允许重试CELERY_ACKS_LATE&#x3D;True# 每个worker最多执行100个任务被销毁，可以防止内存泄漏CELERYD_MAX_TASKS_PER_CHILD&#x3D;100# 超时时间CELERYD_TASK_TIME_LIMIT&#x3D;12*30\n\n在app01目录下创建tasks.py123456from celery import task@taskdef add(a,b):    with open(&#39;a.text&#39;, &#39;a&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:        f.write(&#39;a&#39;)    print(a+b)\n\n视图函数views.py123456789101112131415from django.shortcuts import render,HttpResponsefrom app01.tasks import addfrom datetime import datetimedef test(request):    # result&#x3D;add.delay(2,3)    ctime &#x3D; datetime.now()    # 默认用utc时间    utc_ctime &#x3D; datetime.utcfromtimestamp(ctime.timestamp())    from datetime import timedelta    time_delay &#x3D; timedelta(seconds&#x3D;5)    task_time &#x3D; utc_ctime + time_delay    result &#x3D; add.apply_async(args&#x3D;[4, 3], eta&#x3D;task_time)    print(result.id)    return HttpResponse(&#39;ok&#39;)\nsettings.py123456789#INSTALLED_APPS &#x3D; [#    &#39;djcelery&#39;,#    &#39;app01&#39;#]from djagocele import celeryconfigBROKER_BACKEND&#x3D;&#39;redis&#39;BOOKER_URL&#x3D;&#39;redis:&#x2F;&#x2F;127.0.0.1:6379&#x2F;1&#39;CELERY_RESULT_BACKEND&#x3D;&#39;redis:&#x2F;&#x2F;127.0.0.1:6379&#x2F;2&#39;\n\n","categories":["celery"],"tags":["python"]},{"title":"Celery 结构，组件","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/09/26/Celery 结构，组件/","content":"什么是 Celery？Celery是一个简单、灵活且可靠的，处理大量消息的分布式系统\n专注于实时处理的异步任务队列\n同时也支持任务调度\ncelery 架构Celery的架构由三部分组成，消息中间件（message broker），任务执行单元（worker）和任务执行结果存储（task result store）组成。\n消息中间件Celery本身不提供消息服务，但是可以方便的和第三方提供的消息中间件集成。包括，RabbitMQ, Redis等等\n任务执行单元Worker是Celery提供的任务执行的单元，worker并发的运行在分布式的系统节点中。\n任务结果存储Task result store用来存储Worker执行的任务的结果，Celery支持以不同方式存储任务的结果，包括AMQP, redis等\nCelery 组件Celery 扮演生产者和消费者的角色\nProducer :任务生产者. 调用 Celery API , 函数或者装饰器, 而产生任务并交给任务队列处理的都是任务生产者。\n\nCelery Beat :任务调度器. Beat 进程会读取配置文件的内容, 周期性的将配置中到期需要执行的任务发送给任务队列。\n\nBroker :消息代理, 队列本身. 也称为消息中间件.。接受任务生产者发送过来的任务消息, 存进队列再按序分发给任务消费方(通常是消息队列或者数据库)。\n\nCelery Worker :执行任务的消费者, 通常会在多台服务器运行多个消费者, 提高运行效率。\n\nResult Backend :任务处理完成之后保存状态信息和结果, 以供查询。\n\n\n使用场景\n异步任务：发邮件、发送消息自动化工单中耗时任务所有需要异步处理的请求都可以\n\n定时任务：工单系统定时获取超时工单进行延时报警对过期会员进行清理\n\n\ncelery应用举例\nCelery 是一个 基于python开发的分布式异步消息任务队列，通过它可以轻松的实现任务的异步处理，如果你的业务场景中需要用到异步任务，就可以考虑使用celery\n\n你想对100台机器执行一条批量命令，可能会花很长时间 ，但你不想让你的程序等着结果返回，而是给你返回 一个任务ID,你过一段时间只需要拿着这个任务id就可以拿到任务执行结果， 在任务执行ing进行时，你可以继续做其它的事情\n\nCelery 在执行任务时需要通过一个消息中间件来接收和发送任务消息，以及存储任务结果， 一般使用rabbitMQ or Redis\n\n\nCelery的优点\n简单：一单熟悉了celery的工作流程后，配置和使用还是比较简单的\n\n高可用：当任务执行失败或执行过程中发生连接中断，celery 会自动尝试重新执行任务\n\n快速：一个单进程的celery每分钟可处理上百万个任务\n\n灵活： 几乎celery的各个组件都可以被扩展及自定制\n\n\nCelery基本工作流程图![~`](https://img-blog.csdnimg.cn/2020022214505926.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY4NTg2OQ==,size_16,color_FFFFFF,t_70)\nuser：用户程序，用于告知celery去执行一个任务。broker: 存放任务（依赖RabbitMQ或Redis，进行存储）worker：执行任务\n\n非常重要版本支持情况123456789101112Celery version 4.0 runs on        Python ❨2.7, 3.4, 3.5❩        PyPy ❨5.4, 5.5❩    This is the last version to support Python 2.7, and from the next version (Celery 5.x) Python 3.5 or newer is required.    If you’re running an older version of Python, you need to be running an older version of Celery:        Python 2.6: Celery series 3.1 or earlier.        Python 2.5: Celery series 3.0 or earlier.        Python 2.4 was Celery series 2.2 or earlier.    Celery is a project with minimal funding, so we don’t support Microsoft Windows. Please don’t open any issues related to that platform.\n\n如果使用python版本不支持 ↓\n解决版本问题：在 celery 官方的提议下，建议将 async 文件的文件名改成 asynchronous\nC:\\Python37\\Lib\\site-packages\\kombu\\async\n12345C:\\Python37\\Lib\\site-packages\\celery\\utils\\timer2.pyC:\\Python37\\lib\\site-packages\\celery\\concurrency\\asynpool.pyC:\\Python37\\lib\\site-packages\\celery\\worker\\components.pyC:\\Python37\\lib\\site-packages\\celery\\worker\\autoscale.pyC:\\Python37\\lib\\site-packages\\celery\\worker\\consumer.py\n\n以下的文件都有 async 的导包，我们将里面的导报都改成 asynchronous\n","categories":["celery"],"tags":["python"]},{"title":"压缩 -压缩实现方法、常用的压缩格式","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/09/19/压缩 -压缩实现方法、常用的压缩格式/","content":"\n压缩是通过 特定的算法来减少机算机对文件的大小机制，可以减少 Bytes有很多的公司 对 存储的数据，都是用压缩包的形式，很少会用到数据库，一朋友 ，新跳了一家公司 分配好项目之后，没想到，发来的 数据都是 压缩包，其中有一个包 里面有着 4000W 的用户信息 （具体啥玩意我也不知道）他的电脑 打都打不开。（ 咱不是程序大佬 小白白 ）\n\n让我突然重视了起来 压缩，因为平时 除了用快压 解压一些小电影剩下的好像什么都没有办，所以 度妈妈 了一些相关知识想着分享一下、也放在这里希望 大佬 能够指点\n言归正传我们大众化的理解就是解压 就是去除空格 其实也不完全对接下来我们来说说 压缩\n\n文件\n\n\n（ 文件 ）压缩技术 简单的来说 就是查找文件内的重复 Bytes，然后建立一个具有相同 Bytes 的 字典 文件，并且用一个代码来 表示。\n\n比如原文件里，有多处重复的 Bytes or word 如：LiEnze 我爱你，这个时候 就会生产出一个代码来表示 如：sb 当然只是举例、真实的操作要麻烦的多\n\n\n\n图片\n\n\n（ 图片 ）计算机处理信息 用的是 二进制表示、在一张图片内 有数不清的 相同颜色点对吧，压缩就会 在某个位置上 有多少个 蓝色点、在通过 公式用 （0， 1）来表示出来\n\n压缩 方法分为：（ 有的时候想追求效率 必然质量会打点折扣 ）\n\n有损压缩：\n\n\n举个例子 在压缩 图片的时候 你图片的左上角 少了一个 像素点 你的肉眼能看出来么？对吧！\n所以有损压缩 非常适用于 压缩 图片 音频 视频 典型的代表格式就是 .mpeg\n\n\n无损压缩：\n\n\n当我们对数据追求完美 不是特别考虑效率的时候 无损压缩就用到了、代表格式就太多了 如：.zip .rar 这些啊其实压缩 最重要的 一点还是去重 也就是 重复压缩\n\n重复压缩 也有两种：\n\n段语句的重复\n\n\nBytes 在重复三个以上就成为短语句\n\n在压缩这种类型时候 zip 用的是 两个数字 一个是 重复位置到当前位置的 距离，另一个是重复的长度\n\n比如：abcddddd 我这个当前重复的位置是第 3个（第一个位置索引是 0），重复的长度是 5，那么我就可以 d(3,5) 来表示重复的 d。\n\n\n不要觉得一个 Bytes 有256个可能 三个字节就是有 256 ^ 3 种可能、这种压缩方法简直就是天方夜谭、\n\n比如：一篇小说中出现的 主人公的名字，和女主角去酒店的名字，极大程度地多次出现、这样就恰恰的符合了 重复压缩的做法，但是重复压缩只适合进行一次压缩\n如果对文件进行第二次重复压缩 意义不大，因为第一次压缩 已经大大的破坏了 源语句的重复倾向。\n\n\n单 Bytes 的重复\n\n\n一个字节有 236 中可能、这样重复的几率岂不是更大？因为他是单字节，所以范围缩小了好多的。\n\n比如在 ASCII 文本文件中 常用的就是 字母 和 数字，据说 E 的使用率是最高的 。\n\n图片就更好理解了 肯定 深色调 和 浅色调 使用的多嘛这里顺便提一下：png图片格式是一种无损压缩，其核心算法就是 zip 算法，它和 zip 格式的文件的主要区别在于：作为一种图片格式，它在文件头处存放了图片的大小、使用的颜色数等信息。\n\n上面提到的短语式压缩的结果也有这种倾向：重复倾向于出现在离当前压缩位置较近的地方，重复长度倾向于比较短（20字节以内）。\n\n\n常见的压缩格式：\n\nJAR– Java Archive File他是 Java 的一种文档格式、你也可以理解为他就是 ZIP文件 ，叫他文件包，他和 ZIP 的最大区别就是 JAR 文件的内容中包含了一个 META-INF/MANIFEST.MF 文件，这个文件是在生成 JAR 文件的时候自动创建的\n\nZIP–zip 是 very常见的一种压缩格式了它不需要单独的一个压缩或者解压缩软件，因为Windows系统已经集成了对 ZIP 压缩格式的支持。\n\nRAR–RAR 的压缩地位仅次于 ZIP ,因为 RAR 的压缩率 要比 ZIP 高很多。有一个后起之秀 叫 7Z 有着 比 RAR 更高的压缩率 但是 没办法 RAR 在压缩领域奠定了一定的基础、不可撼动。\n\nCABCAB 是微软推出的压缩文件格式，主要都是用于安装程序上，所以 CAB 文件包含的文件都是被经过处理的，代价就是咱们自己解压后可能还用不了\n\nISO –ISO 是 一种光盘镜像格式，是吧数据保存到光盘上。你一可以理解 这就是文件提取。\n\nTAR–TAR .tar 为后缀的文件，WinZIP、WinRAR、都可以打开，因为 他们两个都对 TAR 进行了关联，注意说的一点是 TAR是linux 常用的文件格式\n\nUUE–UUE 这个比较牛逼、是压缩遇到邮件编码混合 引起 乱码 的情况下就用压缩格式，可以用WinZIP、WinRAR打开。\n\n\n压缩的操作相比较来说 还是比较麻烦的  要慢慢钻研~\n","categories":["python"],"tags":["python"]},{"title":"递归","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/09/15/递归/","content":"在调用一个函数的过程中，直接或间接地调用了函数本身这个就叫递归。但为了避免出现死循环，必须要有一个结束条件\n在函数中调用函数本身时，相当于你让程序回到函数的第一行重新走一遍而已。、\n\n12345678def foo(S, T):    S &#x3D; T * T - S    if S &gt;&#x3D; 10:        W &#x3D; S + T * T        return W    else:        foo(S, T * 2)\n","categories":["算法"],"tags":["python"]},{"title":"八大查找","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/09/10/八大查找/","content":"顺序查找算法简介 顺序查找又称为线性查找，是一种最简单的查找方法。适用于线性表的顺序存储结构和链式存储结构。该算法的时间复杂度为O(n)。 基本思路 从第一个元素m开始逐个与需要查找的元素x进行比较，当比较到元素值相同(即m=x)时返回元素m的下标，如果比较到最后都没有找到，则返回-1。 优缺点 缺点：是当n 很大时，平均查找长度较大，效率低； 优点：是对表中数据元素的存储没有要求。另外，对于线性链表，只能进行顺序查找。 算法实现\n12345678def sequential_search(lis, key):  length &#x3D; len(lis)  for i in range(length):    if lis[i] &#x3D;&#x3D; key:      return i    else:      return False\n\n折半查找二分查找（Binary Search），是一种在有序数组中查找某一特定元素的查找算法。查找过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则查找过程结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。如果在某一步骤数组为空，则代表找不到。 这种查找算法每一次比较都使查找范围缩小一半。\n算法描述 给予一个包含 个带值元素的数组A 1、 令 L为0 ， R为 n-1 2、 如果L&gt;R，则搜索以失败告终 3、 令 m (中间值元素)为 ⌊(L+R)/2⌋ 4、 如果 AmT，令 R为 m - 1 并回到步骤二 复杂度分析 时间复杂度：折半搜索每次把搜索区域减少一半，时间复杂度为 O(logn) 空间复杂度：O(1)\n1234567891011121314151617def binary_search(lis, key):  low &#x3D; 0  high &#x3D; len(lis) - 1  time &#x3D; 0  while low &lt; high:    time +&#x3D; 1    mid &#x3D; int((low + high) &#x2F; 2)    if key &lt; lis[mid]:      high &#x3D; mid - 1    elif key &gt; lis[mid]:      low &#x3D; mid + 1    else:      # 打印折半的次数      print(&quot;times: %s&quot; % time)      return mid  print(&quot;times: %s&quot; % time)  return False\n\n插值查找算法简介\n插值查找是根据要查找的关键字key与查找表中最大最小记录的关键字比较后的 查找方法，其核心就在于插值的计算公式 (key-a[low])/(a[high]-a[low])*(high-low)。 时间复杂度o(logn)但对于表长较大而关键字分布比较均匀的查找表来说，效率较高。\n算法思想 基于二分查找算法，将查找点的选择改进为自适应选择，可以提高查找效率。当然，差值查找也属于有序查找。 注：对于表长较大，而关键字分布又比较均匀的查找表来说，插值查找算法的平均性能比折半查找要好的多。反之，数组中如果分布非常不均匀，那么插值查找未必是很合适的选择。\n复杂度分析 时间复杂性：如果元素均匀分布，则O（log log n）），在最坏的情况下可能需要O（n）。 空间复杂度：O（1）。\n12345678910111213141516171819202122232425def binary_search(lis, key):  low &#x3D; 0  high &#x3D; len(lis) - 1  time &#x3D; 0  while low &lt; high:    time +&#x3D; 1    # 计算mid值是插值算法的核心代码    mid &#x3D; low + int((high - low) * (key - lis[low])&#x2F;(lis[high] - lis[low]))    print(&quot;mid&#x3D;%s, low&#x3D;%s, high&#x3D;%s&quot; % (mid, low, high))    if key &lt; lis[mid]:      high &#x3D; mid - 1    elif key &gt; lis[mid]:      low &#x3D; mid + 1    else:      # 打印查找的次数      print(&quot;times: %s&quot; % time)      return mid  print(&quot;times: %s&quot; % time)  return Falseif __name__ &#x3D;&#x3D; &#39;__main__&#39;:  LIST &#x3D; [1, 5, 7, 8, 22, 54, 99, 123, 200, 222, 444]  result &#x3D; binary_search(LIST, 444)  print(result)\n","categories":["算法"],"tags":["python"]},{"title":"八大排序","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/09/02/八大排序/","content":"插入排序插入排序：插入排序的基本操作就是将一个数据插入到已经排好序的有序数据中，从而得到一个新的、个数加一的有序数据，算法适用于少量数据的排序；首先将第一个作为已经排好序的，然后每次从后的取出插入到前面并排序；\n时间复杂度：O(n²)\n空间复杂度：O(1)\n稳定性：稳定\n1234567def insert_sort(ilist):    for i in range(len(ilist)):        for j in range(i):            if ilist[i] &lt; ilist[j]:                ilist.insert(j, ilist.pop(i))                break    return ilist\n冒泡排序冒泡排序：它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成\n时间复杂度：O(n²)\n空间复杂度：O(1)\n稳定性：稳定\n12345678910def bubble_sort(blist):    count &#x3D; len(blist)    for i in range(0, count):        for j in range(i + 1, count):            if blist[i] &gt; blist[j]:                blist[i], blist[j] &#x3D; blist[j], blist[i]    return blistblist &#x3D; bubble_sort([4,5,6,7,3,2,6,9,8])print(blist)\n快排快速排序：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列\n时间复杂度：O(nlog₂n)\n空间复杂度：O(nlog₂n)\n稳定性：不稳定\n12345678910def quick_sort(qlist):    if qlist &#x3D;&#x3D; []:        return []    else:        qfirst &#x3D; qlist[0]        qless &#x3D; quick_sort([l for l in qlist[1:] if l &lt; qfirst])        qmore &#x3D; quick_sort([m for m in qlist[1:] if m &gt;&#x3D; qfirst])        return qless + [qfirst] + qmoreqlist &#x3D; quick_sort([4,5,6,7,3,2,6,9,8])\n\n选择排序选择排序：第1趟，在待排序记录r1 ~ r[n]中选出最小的记录，将它与r1交换；第2趟，在待排序记录r2 ~ r[n]中选出最小的记录，将它与r2交换；以此类推，第i趟在待排序记录r[i] ~ r[n]中选出最小的记录，将它与r[i]交换，使有序序列不断增长直到全部排序完毕\n时间复杂度：O(n²)\n空间复杂度：O(1)\n稳定性：不稳定\n1234567891011def select_sort(slist):    for i in range(len(slist)):        x &#x3D; i        for j in range(i, len(slist)):            if slist[j] &lt; slist[x]:                x &#x3D; j        slist[i], slist[x] &#x3D; slist[x], slist[i]    return slistslist &#x3D; select_sort([4,5,6,7,3,2,6,9,8])\n\n归并排序归并排序：采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为二路归并\n时间复杂度：O(nlog₂n)\n空间复杂度：O(1)\n稳定性：稳定\n123456789101112131415161718192021222324def merge_sort(array):    def merge_arr(arr_l, arr_r):        array &#x3D; []        while len(arr_l) and len(arr_r):            if arr_l[0] &lt;&#x3D; arr_r[0]:                array.append(arr_l.pop(0))            elif arr_l[0] &gt; arr_r[0]:                array.append(arr_r.pop(0))        if len(arr_l) !&#x3D; 0:            array +&#x3D; arr_l        elif len(arr_r) !&#x3D; 0:            array +&#x3D; arr_r        return array    def recursive(array):        if len(array) &#x3D;&#x3D; 1:            return array        mid &#x3D; len(array) &#x2F;&#x2F; 2        arr_l &#x3D; recursive(array[:mid])        arr_r &#x3D; recursive(array[mid:])        return merge_arr(arr_l, arr_r)    return recursive(array)","categories":["算法"],"tags":["python"]},{"title":"JWT简介","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/08/17/JWT简介/","content":"\nJWT 特点 体积小，因而传输速度快\n传输方式多样，可以通过URL/POST参数/HTTP头部等方式传输\n严格的结构化。它自身（在 payload 中）就包含了所有与用户相关的验证消息，如用户可访问路由、访问有效期等信息，服务器无需再去连接数据库验证信息的有效性，并且 payload 支持为你的应用而定制化。\n支持跨域验证，可以应用于单点登录。\nJWT是Auth0提出的通过对JSON进行加密签名来实现授权验证的方案，编码之后的JWT看起来是这样的一串字符：\n1eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ\n\n由 . 分为三段，通过解码可以得到：\n\n头部（Header）12345&#x2F;&#x2F; 包括类别（typ）、加密算法（alg）；&#123;  &quot;alg&quot;: &quot;HS256&quot;,  &quot;typ&quot;: &quot;JWT&quot;&#125;\njwt的头部包含两部分信息：\n\n声明类型，这里是jwt\n声明加密的算法 通常直接使用 HMAC SHA256\n\n载荷（payload）载荷（payload） 载荷就是存放有效信息的地方。\n签名（signature)签名的目的：签名实际上是对头部以及载荷内容进行签名。所以，如果有人对头部以及载荷的内容解码之后进行修改，再进行编码的话，那么新的头部和载荷的签名和之前的签名就将是不一样的。而且，如果不知道服务器加密的时候用的密钥的话，得出来的签名也一定会是不一样的。 这样就能保证token不会被篡改。\n\n最后，我们将上面拼接完的字符串用HS256算法进行加密。在加密的时候，我们还需要提供一个密钥（secret）。类似盐\n这里在第三步我们得到 JWT 之后，需要将JWT存放在 client，之后的每次需要认证的请求都要把JWT发送过来。（请求时可以放到 header 的 Authorization ）\n一、使用JSON Web Token的好处？ \n\n性能问题。 JWT方式将用户状态分散到了客户端中，相比于session，可以明显减轻服务端的内存压力。 Session方式存储用户id的最大弊病在于Session是存储在服务器端的，所以需要占用大量服务器内存， 对于较大型应用而言可能还要保存许多的状态，一般还需借助nosql和缓存机制来实现session的存储，如果是分布式应用还需session共享。 \n单点登录。 JWT能轻松的实现单点登录，因为用户的状态已经被传送到了客户端。 token 可保存自定义信息，如用户基本信息，web服务器用key去解析token，就获取到请求用户的信息了。 我们也可以配置它以便包含用户拥有的任何权限。这意味着每个服务不需要与授权服务交互才能授权用户。 \n前后端分离。 以前的传统模式下，后台对应的客户端就是浏览器，就可以使用session+cookies的方式实现登录， 但是在前后分离的情况下，后端只负责通过暴露的RestApi提供数据，而页面的渲染、路由都由前端完成。因为rest是无状态的，因此也就不会有session记录到服务器端。 \n兼容性。 支持移动设备，支持跨程序调用，Cookie 是不允许垮域访问的，而 Token 则不存在这个问题。 \n可拓展性。 jwt是无状态的，特别适用于分布式站点的单点登录（SSO）场景。 比如有3台机器（A、B、C）组成服务器集群，若session存在机器A上，session只能保存在其中一台服务器，此时你便不能访问机器B、C，因为B、C上没有存放该Session， 而使用token就能够验证用户请求合法性，并且我再加几台机器也没事，所以可拓展性好。 \n安全性。因为有签名，所以JWT可以防止被篡改。\n\nJWT是基于token的身份认证的方案。\njson web token全称。可以保证安全传输的前提下传送一些基本的信息，以减轻对外部存储的依赖，减少了分布式组件的依赖，减少了硬件的资源。\n可实现无状态、分布式的Web应用授权，jwt的安全特性保证了token的不可伪造和不可篡改。\n本质上是一个独立的身份验证令牌，可以包含用户标识、用户角色和权限等信息，以及您可以存储任何其他信息（自包含）。任何人都可以轻松读取和解析，并使用密钥来验证真实性。\n缺陷：1). JWT在生成token的时候支持失效时间，但是支持的失效时间是固定的，比如说一天。 但是用户在等出的时候是随机触发的，那么我们jwt token来做这个失效是不可行的，因为jwt在初始化的时候已经定死在什么时候过期了。 采用其他方案，在redis中存储token，设置token的过期时间，每次鉴权的时候都会去延长时间2). jwt不适合存放大量信息，信息越多token越长\n","categories":["jwt"],"tags":["python"]},{"title":"什么是 MongoDB","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/08/06/什么是 MongoDB/","content":"MongoDB 是一个介于关系数据库和非关系数据库之间的开源产品，是最接近于关系型数据库的 NoSQL 数据库。它在轻量级JSON 交换基础之上进行了扩展，即称为 BSON 的方式来描述其无结构化的数据类型。尽管如此它同样可以存储较为复杂的数据类型。它和上一篇文章讲到的Redis有异曲同工之妙。虽然两者均为 NoSQL ，但是 MongoDB 相对于 Redis 而言，MongoDB 更像是传统的数据库。早些年我们是先有了 Relation Database (关系型数据库)，然后出现了很多很复杂的query ，里面用到了很多嵌套，很多 join 操作。所以在设计数据库的时候，我们也考虑到了如何应用他们的关系，使得写 query 可以使 database 效率达到最高。后来人们发现，不是每个系统，都需要如此复杂的关系型数据库。有些简单的网站，比如博客，比如社交网站，完全可以斩断数据库之间的一切关系。这样做带来的好处是，设计数据库变得更加简单，写 query 也变得更加简单。然后，query 消耗的时间可能也会变少。因为 query 简单了，少了许多消耗资源的 join 操作，速度自然会上去。正如所说的， query 简单了，很有以前 MySQL 可以找到的东西，现在关系没了，通过 Mongo 找不到了。我们只能将几组数据都抓到本地，然后在本地做 join ，所以在这点上可能会消耗很多资源。这里我们可以发现。如何选择数据库，完全取决于你所需要处理的数据的模型，即 Data Model 。如果它们之间，关系错综复杂，千丝万缕，这个时候 MySQL 一定是首选。如果他们的关系并不是那么密切，那么， NoSQL 将会是利器。\nMongoDB 和 Redis 一样均为 key-value 存储系统，它具有以下特点：面向集合存储，易存储对象类型的数据。 模式自由。 支持动态查询。 支持完全索引，包含内部对象。 支持查询。 支持复制和故障恢复。 使用高效的二进制数据存储，包括大型对象(如视频等)。 自动处理碎片，以支持云计算层次的扩展性 支持 Python ， PHP ， Ruby ， Java ， C ， C# ， Javascript ，Perl 及 C++ 语言的驱动程序，社区中也提供了对 Erlang 及 .NET 等平台的驱动程序。 文件存储格式为 BSON (一种 JSON 的扩展)。 可通过网络访问。\nMongoDB 与 MySQL 性能比较像 MySQL 一样， MongoDB 提供了丰富的远远超出了简单的键值存储中提供的功能和功能。 MongoDB 具有查询语言，功能强大的辅助索引(包括文本搜索和地理空间)，数据分析功能强大的聚合框架等。相比使用关系数据库而言，使用MongoDB ，您还可以使用如下表所示的这些功能，跨越更多样化的数据类型和数据规模。\nMySQLMongoDB丰富的数据模型否是动态 Schema否是数据类型是是数据本地化否是字段更新是是易于编程否是复杂事务是否审计是是自动分片否是\nMySQL 中的许多概念在 MongoDB 中具有相近的类比。本表概述了每个系统中的一些常见概念。\nMySQLMongoDB表集合行文档列字段joins嵌入文档或者链接\nMongoDB应用范围和限制MongoDB 的主要目标是在 key-value (键/值)存储方式(提供了高性能和高度伸缩性)以及传统的 RDBMS 系统(丰富的功能)架起一座桥梁，集两者的优势于一身。 MongoDB 适用范围如下：\n网站数据： Mongo 非常适合实时的插入，更新与查询，并具备网站实时数据存储所需的复制及高度伸缩性。 缓存：由于性能很高， Mongo 也适合作为信息基础设施的缓存层。在系统重启之后，由 Mongo 搭建的持久化缓存层可以避免下层的数据源过载。 大尺寸，低价值的数据：使用传统的关系型数据库存储一些数据时可能会比较昂贵，在此之前，很多时候程序员往往会选择传统的文件进行存储。 高伸缩性的场景： Mongo 非常适合由数十或数百台服务器组成的数据库。 Mongo 的路线图中已经包含对 MapReduce 引擎的内置支持。 用于对象及 JSON 数据的存储： Mongo 的 BSON 数据格式非常适合文档化格式的存储及查询。 MongoDB 当然也会有以下场景的限制：\n高度事物性的系统：例如银行或会计系统。传统的关系型数据库目前还是更适用于需要大量原子性复杂事务的应用程序。 传统的商业智能应用：针对特定问题的 BI 数据库会对产生高度优化的查询方式。对于此类应用，数据仓库可能是更合适的选择。 需要 SQL 的问题。\n","categories":["mongodb"],"tags":["python"]},{"title":"vue页面结构&语法","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/07/30/vue页面结构&语法/","content":"1.vue页面结构1234567891011121314151617//页面骨架&lt;template&gt;    &lt;div&gt;  &lt;/div&gt;&lt;/template&gt;// 页面行为&lt;script&gt;  export default &#123;&#125;&lt;/script&gt;//页面样式&lt;style&gt;  &lt;/style&gt;\n\n\n2.vue基本语法123456789101112131415v-html:显示（能解析html格式）&#123;&#123; &#125;&#125;：显示（原样显示）v-show：隐藏（true/1显示，fales/0不显示）v-if/v-else-if/v-else:条件判断，多重判断v-for=&#x27;(item,index) in data&#x27;:循环遍历  #item:遍历输出的个体 index：列表的下标 data：循环的列表v-bind:简写为‘：’绑定（标签里使用的绑定）v-on:简写为‘@’双向绑定（标签里使用的绑定）v-model:双向数据绑定（表单里使用的绑定）\n\n\n3.vue基础行为12345678910111213141516171819202122232425262728293031323334//定义数据data:&#123;      return&#123;      &#125;&#125;//钩子函数：样式加载前mounted（）&#123;  &#125;//钩子函数：所有数据加载前 比mounted更快created（）&#123;  &#125;//计算属性（在不改变原始值的情况下进行操作，非必须）computed：&#123;      reverse_msg:function()&#123;        console_log()控制台输出        return this.msg.split(&#x27;&#x27;).reverse().join(&#x27;&#x27;)#反转            &#125;，&#125;//监听属性(监听自定义变量的变化)watch：&#123;          &#125;//自定义函数方法methods:&#123;    &#125;\n123456//vue的各种互动用法console.log(res) //控制台输出this.$Message(res.data.message)  //提示框this.$Notice(res.data.message)  //警示框this.$router.push(&#x27;/&#x27;)  //页面跳转this.$route.query.键 //从url里获取参数值","categories":["vue"],"tags":["python"]},{"title":"vue目录","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/07/24/vue目录/","content":"1.npm相关操作123456789101112131415npm改源：npm set registry https://registry.npm.taobao.orgnpm版本查询：npm -vnpm升级：npm install -g npm安装所有依赖：npm install  #在vue项目目录下进行npm run dev 启动不了：​\t删除node_modules:​\t\t安装npm install rimraf -g​\t\t然后rimraf node_modules\n\n2.vue目录1234567891011121314151617181920212223242526272829303132build：打包文件​config：配置文件​disk:打包后的文件在这个里面，一开始没有​node_modules：根据package.json下载的所有依赖在这里面，不能上传库，解决方法：.gitignore文件中写入不上传的文件名。  #vue和django项目中都有.gitignore​src：所有代码在这个里面​   assets：放样式和静态        bootstrap：手机模式样式​   components：组件文件夹  #所有页面在这里面​   router：放所有路由的        mode：&#x27;history&#x27;  #去掉url中的#        path：url        name：命名空间  #模板跳转使用        component：组件名称   App.vue:渲染组件，绑定vue和页面节点      config:项目配置  #自定义​   main.js:入口文件，实例生成，组件引用static:静态文件夹​package.json：所有模块依赖的集合 npm install 根据这个进行安装.gitignore：文件中写入不上传的文件名\n\n\n","categories":["vue"],"tags":["python"]},{"title":"Vue国际化","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/07/13/Vue国际化/","content":"这里所说的国际化，是根据访问者的浏览器语言来更改页面语言。导入插件:1npm install vue-i18n --save\n\n\n在main的js中注册该组件:1234// 导入import VueI18n from &#x27;vue-i18n&#x27;// 注册Vue.use(VueI18n)\n\n\n1234在src目录下新建lang文件夹，在文件夹中新建zh.js和en.js文件在两个文件中(如果不止是中文和英文的话就再新建对应的文件)输入对应的文本，具体内容按自己需求而定，\n\n这是我自己的zh.js文件内容:1234567// 双语规范的变量(中文)export const m = &#123;  &#x27;welcome&#x27;: &#x27;欢迎您&#x27;,&#125;\n\n这是我的en.js文件内容:12345678// 双语规范的变量(英文)export const m= &#123;    &#x27;welcome&#x27;: &#x27;Welcome!&#x27;&#125;\n\n然后再在main.js中加入以下内容，注意要放在上次写的语句下面:12345678910// 导入语言包const i18n = new VueI18n(&#123;  // 当前默认语言  locale: &#x27;en&#x27;,  // 语言包声明  messages:&#123;    &#x27;zh&#x27;: require(&#x27;./lang/zh&#x27;),    &#x27;en&#x27;: require(&#x27;./lang/en&#x27;),  &#125;&#125;)\n\n\n在new Vue中加入以下内容:1i18n\n\n","categories":["vue"],"tags":["python"]},{"title":"Selenium & PhantomJS","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/07/01/Selenium & PhantomJS/","content":"1.selenium介绍介绍:1.selenium是一个web自动化测试用的框架. 程序员可以通过代码实现对浏览器的控制, 比如打开网页, 点击网页中的元素, 实现鼠标滚动等操作.2.它支持多款浏览器, 如谷歌浏览器, 火狐浏览器等等, 当然也支持无头浏览器.\n目的:在爬取数据的过程中, 经常遇到动态数据加载, 一般动态数据加载有两种, 一种通过ajax请求加载数据, 另一种通过js代码加载动态数据. selenium可以模拟人操作真实浏览器, 获取加载完成的页面数据ajax:    url有规律且未加密, 直接构建url连接请求    url加密过无法破解规律 –&gt; seleniumjs动态数据加载 –&gt;  selenium\n\n2.selenium安装1pip install selenium\n123456789三要素: 浏览器, 驱动程序, selenium框架浏览器: 推荐谷歌浏览器, 标准稳定版本驱动程序:http:&#x2F;&#x2F;chromedriver.storage.googleapis.com&#x2F;index.html# 测试:from selenium import webdriverbrowser &#x3D; webdriver.Chrome(&#39;.&#x2F;webdriver&#39;)   # 将驱动放在脚本所在的文件夹browser.get(&#39;www.baidu.com&#39;)\n\n3.selenium常用操作1234567891011121314151617181920212223242526272829# 实例化浏览器对象:from selenium import webdriverbrowser &#x3D; webdriver.Chrome(&#39;driverpath&#39;)# 发送get请求:browser.get(&#39;https:&#x2F;&#x2F;www.baidu.com&#39;)# 获取页面元素:find_element_by_id:根据元素的idfind_element_by_name:根据元素的namefind_element_by_xpath:根据xpath表达式find_element_by_class_name:根据class的值find_element_by_css_selector:根据css选择器# 节点交互操作:click(): 点击send_keys(): 输入内容clear(): 清空操作execute_script(js): 执行指定的js代码# JS代码: window.scrollTo(0, document.body.scrollHeight)可以模拟鼠标滚动一屏高度quit(): 退出# 获取网页源码:browser.page_source  ---&gt;  str类型# frameswitch_to.frame(&#39;frameid&#39;) \n\n4.qq空间模拟登陆123456789101112131415161718192021222324252627282930313233from selenium import webdriverimport time# 实例化浏览器对象browser &#x3D; webdriver.Chrome(&#39;.&#x2F;chromedriver.exe&#39;)# 打开qq空间登陆页面browser.get(&#39;https:&#x2F;&#x2F;qzone.qq.com&#x2F;&#39;)time.sleep(1)# 转至frame子页面browser.switch_to.frame(&#39;login_frame&#39;)# 获取密码登陆选项并点击a_tag &#x3D; browser.find_element_by_id(&#39;switcher_plogin&#39;)a_tag.click()time.sleep(1)# 获取账号输入框并输入账号browser.find_element_by_id(&#39;u&#39;).clear()user &#x3D; browser.find_element_by_id(&#39;u&#39;)user.send_keys(&#39;3338003899&#39;)time.sleep(1)# 获取密码输入框并输入密码browser.find_element_by_id(&#39;p&#39;).clear()pwd &#x3D; browser.find_element_by_id(&#39;p&#39;)pwd.send_keys(&#39;qq123456&#39;)time.sleep(1)# 获取登陆按钮并单击button &#x3D; browser.find_element_by_id(&#39;login_button&#39;)button.click()\n\n5.PhantomJS浏览器使用PhantomJS下载及配置环境变量- 下载, 直接解压\n- 将解压文件的bin目录添加至环境变量\n\nPhantomJS无界面浏览器12345from selenium import webdriverbrowser &#x3D; webdriver.PhantomJS()browser.get(&#39;https:&#x2F;&#x2F;www.baidu.com&#39;)with open(&#39;baidu_phantomjs.html&#39;, &#39;w&#39;, encoding&#x3D;&quot;utf-8&quot;) as f:    f.write(browser.page_source)\n谷歌无头浏览器1234567891011from selenium import webdriverfrom selenium.webdriver.chrome.options import Optionschrome_options &#x3D; Options()chrome_options.add_argument(&#39;--headless&#39;)chrome_options.add_argument(&#39;--disable-gpu&#39;)browser &#x3D; webdriver.Chrome(chrome_options&#x3D;chrome_options)browser.get(&quot;https:&#x2F;&#x2F;www.baidu.com&quot;)print(browser.page_source)with open(&#39;baidu_headerless.html&#39;, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:    f.write(browser.page_source)\n\n6..图片懒加载1234567891011121314151617import requestsfrom lxml import etreefrom urllib import requestimport urlliburl &#x3D; &#39;http:&#x2F;&#x2F;sc.chinaz.com&#x2F;tupian&#x2F;index.html&#39;headers &#x3D; &#123;    &quot;USer-Agent&quot;:&#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;75.0.3770.142 Safari&#x2F;537.36&#39;&#125;res &#x3D; requests.get(url&#x3D;url, headers&#x3D;headers)res.encoding &#x3D; &#39;utf-8&#39;# print(res.text)tree &#x3D; etree.HTML(res.text)src_list &#x3D; tree.xpath(&#39;&#x2F;&#x2F;div[contains(@class,&quot;box&quot;)]&#x2F;div&#x2F;a&#x2F;img&#x2F;@src2&#39;)for url_img in src_list:    request.urlretrieve(url_img,&#39;%s.jpg&#39;%src_list.index(url_img))    \n\n123456789101112131415161718192021222324from selenium import webdriverimport timebrowser &#x3D; webdriver.Chrome(&#39;.&#x2F;chromedriver.exe&#39;)browser.get(&#39;http:&#x2F;&#x2F;image.baidu.com&#x2F;&#39;)search &#x3D; browser.find_element_by_id(&#39;kw&#39;)search.send_keys(&#39;安琪拉&#39;)button &#x3D; browser.find_element_by_class_name(&#39;s_search&#39;)button.click()for i in range(3):    browser.execute_script(&#39;window.scrollTo(0, document.body.scrollHeight)&#39;)    time.sleep(5)text &#x3D; browser.page_sourcewith open(&#39;baidu_pic.html&#39;, &#39;w&#39;, encoding&#x3D;&quot;utf-8&quot;) as f:    f.write(text)from bs4 import BeautifulSoupsoup &#x3D; BeautifulSoup(open(&#39;.&#x2F;baidu_pic.html&#39;, &#39;r&#39;, encoding&#x3D;&#39;utf-8&#39;), &#39;lxml&#39;)li_list &#x3D; soup.select(&#39;.imgpage ul li&#39;)for url_img in li_list:    url_img &#x3D; url_img[&#39;data-objurl&#39;]    print(url_img)\n\n","categories":["爬虫"],"tags":["python"]},{"title":"Scrapy框架（2）","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/06/22/Scrapy框架（2）/","content":"1.scrapy 多页爬取1# spider编码在原基础之上, 构建其他页面的url地址, 并利用scrapy.Request发起新的请求, 请求的回调函数依然是parse:page &#x3D; 1base_url &#x3D; &#39;http:&#x2F;&#x2F;www.xiaohuar.com&#x2F;list-1-%s.html&#39;if self.page &lt; 4:    page_url &#x3D; base_url%self.page    self.page +&#x3D; 1    yield scrapy.Request(url&#x3D;page_url, callback&#x3D;self.parse)# (其他文件不用改动)\n\n2.scrapy爬取详情页需求: 爬取笑话的标题与详情页连接, 通过详情页链接, 爬取详情页笑话内容1234567# item编码: 定义数据持久化的字段信息import scrapyclass JokeItem(scrapy.Item):    # define the fields for your item here like:    # name &#x3D; scrapy.Field()    title &#x3D; scrapy.Field()    content &#x3D; scrapy.Field()\n123456789101112131415161718192021222324252627# spider的编码:# -*- coding: utf-8 -*-import scrapyfrom ..items import JokeItemclass XhSpider(scrapy.Spider):    name &#x3D; &#39;xh&#39;    # allowed_domains &#x3D; [&#39;www.baidu.com&#39;]    start_urls &#x3D; [&#39;http:&#x2F;&#x2F;www.jokeji.cn&#x2F;list.htm&#39;]    def parse(self, response):        li_list &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;list_title&quot;]&#x2F;ul&#x2F;li&#39;)        for li in li_list:            title &#x3D; li.xpath(&#39;.&#x2F;b&#x2F;a&#x2F;text()&#39;).extract_first()            link &#x3D; &#39;http:&#x2F;&#x2F;www.jokeji.cn&#39; + li.xpath(&#39;.&#x2F;b&#x2F;a&#x2F;@href&#39;).extract_first()            yield scrapy.Request(url&#x3D;link, callback&#x3D;self.datail_parse, meta&#x3D;&#123;&quot;title&quot;:title&#125;)    def datail_parse(self, response):        joke_list &#x3D; response.xpath(&#39;&#x2F;&#x2F;span[@id&#x3D;&quot;text110&quot;]&#x2F;&#x2F;text()&#39;).extract()        title &#x3D; response.meta[&quot;title&quot;]        content &#x3D; &#39;&#39;        for s in joke_list:            content +&#x3D; s        item &#x3D; JokeItem()        item[&quot;title&quot;] &#x3D; title        item[&quot;content&quot;] &#x3D; content        yield item\n1234567891011121314# Pipeline编码: 数据持久化具体操作import pymongoclass JokePipeline(object):    conn &#x3D; pymongo.MongoClient(&#39;localhost&#39;, 27017)    db &#x3D; conn.haha    table &#x3D; db.hahatable    def process_item(self, item, spider):        self.table.insert(dict(item))        return item    def close_spider(self, spider):        self.conn.close()\n\n1234# settings配置编码:UA伪装Robots协议Item_Pipeline\n\n3.scrapy发送post请求12345678910111213141516171819import scrapyimport jsonclass FySpider(scrapy.Spider):    name &#x3D; &#39;fy&#39;    # allowed_domains &#x3D; [&#39;www.baidu.com&#39;]    start_urls &#x3D; [&#39;https:&#x2F;&#x2F;fanyi.baidu.com&#x2F;sug&#39;]    def start_requests(self):        data &#x3D; &#123;            &#39;kw&#39;:&#39;boy&#39;        &#125;        yield scrapy.FormRequest(url&#x3D;self.start_urls[0], callback&#x3D;self.parse, formdata&#x3D;data)    def parse(self, response):        print(~~~~)        print(response.text)        print(json.loads(response.text)) print(~~~~)\n\n4.scrapy中间件12345678910# 中间件分类:\t- 下载中间件: DownloadMiddleware\t- 爬虫中间件: SpiderMiddleware# 中间件的作用:\t- 下载中间件: 拦截请求与响应, 篡改请求与响应\t- 爬虫中间件: 拦截请求与响应, 拦截管道item, 篡改请求与响应, 处理item# 下载中间件的主要方法:process_requestprocess_responseprocess_exception\n\n下载中间件拦截请求, 使用代理ip案例12345678910# spider编码:import scrapyclass DlproxySpider(scrapy.Spider):    name &#x3D; &#39;dlproxy&#39;    # allowed_domains &#x3D; [&#39;www.baidu.com&#39;]    start_urls &#x3D; [&#39;https:&#x2F;&#x2F;www.baidu.com&#x2F;s?wd&#x3D;ip&#39;]    def parse(self, response):        with open(&#39;baiduproxy.html&#39;, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:            f.write(response.text)\n\n1234# Downloadermiddleware编码:def process_request(self, request, spider):    request.meta[&#39;proxy&#39;] &#x3D; &#39;http:&#x2F;&#x2F;111.231.90.122:8888&#39;    return None\n\n\n5.下载中间件实现UA池12345678# spider编码:class DlproxySpider(scrapy.Spider):    name &#x3D; &#39;dlproxy&#39;    # allowed_domains &#x3D; [&#39;www.baidu.com&#39;]    start_urls &#x3D; [&#39;https:&#x2F;&#x2F;www.baidu.com&#x2F;&#39;,&#39;https:&#x2F;&#x2F;www.baidu.com&#x2F;&#39;,&#39;https:&#x2F;&#x2F;www.baidu.com&#x2F;&#39;,&#39;https:&#x2F;&#x2F;www.baidu.com&#x2F;&#39;,&#39;https:&#x2F;&#x2F;www.baidu.com&#x2F;&#39;]        def parse(self, response):        pass\n\n123456789101112131415161718192021# 中间件的编码:from scrapy import signalsfrom fake_useragent import UserAgentimport randomua &#x3D; UserAgent()ua_list &#x3D; []for i in range(100):        ua_chrome &#x3D; ua.Chrome        ua_list.append(ua_chrome)    class ...():        def process_request(self, request, spider):                # request.meta[&#39;proxy&#39;] &#x3D; &#39;http:&#x2F;&#x2F;111.231.90.122:8888&#39;                print(~~~~)                print(self.ua_pool)                print(~~~)                request.headers[&#39;User-Agent&#39;] &#x3D; random.choice(self.ua_pool)                return None       def process_response(self, request, response, spider):      print(~~~~)                 print(request.headers[&quot;User-Agent&quot;])                print(~~~)                 return response\n","categories":["爬虫"],"tags":["python"]},{"title":"scrapy框架（1）","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/06/12/scrapy框架（1）/","content":"1.scrapy安装与环境依赖在安装scrapy前需要安装好相应的依赖库, 再安装scrapy, 具体安装步骤如下:\n(1).安装lxml库1pip install lxml\n(2).安装wheel1pip install wheel\n(3).安装twisted1pip install twisted文件路径\n12(twisted需下载后本地安装,下载地址:http:&#x2F;&#x2F;www.lfd.uci.edu&#x2F;~gohlke&#x2F;pythonlibs&#x2F;#twisted)    (版本选择如下图,版本后面有解释,请根据自己实际选择)\n\n(4).安装pywin321pip install pywin32\n(5).安装scrapy1pip install scrapy\n(6).成功验证1在cmd命令行输入scrapy,显示Scrapy1.6.0-no active project,证明安装成功 \n\n2.创建项目12341.手动创建一个目录test2.在test文件夹下创建爬虫项目为spiderpro: scrapy startproject spiderpro3.进入项目文件夹: cd spiderpro4.创建爬虫文件: scrapy genspider 爬虫名 域名\n\n3.项目目录介绍123456789101112spiderpro　　spiderpro # 项目目录　　　　__init__　　　　spiders:爬虫文件目录　　　　　　__init__　　　　　　tests.py:爬虫文件　　　　items.py:定义爬取数据持久化的数据结构　　　　middlewares.py:定义中间件　　　　pipelines.py:管道,持久化存储相关　　　　settings.py:配置文件　　venv:虚拟环境目录　 scrapy.cfg: scrapy项目配置文件\n说明:123456(1).spiders:其内包含一个个Spider的实现, 每个Spider是一个单独的文件　　(2).items.py:它定义了Item数据结构, 爬取到的数据存储为哪些字段　　(3).pipelines.py:它定义Item Pipeline的实现　　(4).settings.py:项目的全局配置　　(5).middlewares.py:定义中间件, 包括爬虫中间件和下载中间件　　(6).scrapy.cfg:它是scrapy项目的配置文件, 其内定义了项目的配置路径, 部署相关的信息等\n\n4.scrapy框架介绍: 5大核心组件与数据流向\n架构:12345678910111213Scrapy Engine: 这是引擎，负责Spiders、ItemPipeline,Downloader、Scheduler中间的通讯，信号、数据传递等等!　　Scheduler(调度器): 它负责接受引擎发送过来的requests请求，并按照一定的方式进行整理排列，入队、并等待Scrapy Engine(引擎)来请求时，交给引擎。　　Downloader（下载器)：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spiders来处理，　　Spiders：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)，　　Item Pipeline：它负责处理Spiders中获取到的Item，并进行处理，比如去重，持久化存储（存数据库，写入文件，总之就是保存数据用的）　　Downloader Middlewares(下载中间件)：你可以当作是一个可以自定义扩展下载功能的组件　　Spider Middlewares(Spider中间件)：你可以理解为是一个可以自定扩展和操作引擎和Spiders中间‘通信‘的功能组件（比如进入Spiders的Responses;和从Spiders出去的Requests）\n\n工作流:12345678910111.spider将请求发送给引擎, 引擎将request发送给调度器进行请求调度　　2.调度器把接下来要请求的request发送给引擎, 引擎传递给下载器, 中间会途径下载中间件　　3.下载携带request访问服务器, 并将爬取内容response返回给引擎, 引擎将response返回给spider　　4.spider将response传递给自己的parse进行数据解析处理及构建item一系列的工作, 最后将item返回给引擎, 引擎传递个pipeline　　5.pipe获取到item后进行数据持久化　　6.以上过程不断循环直至爬虫程序终止\n\n5.使用scrapy框架爬取糗百需求: 爬取糗事百科热门板块,每一条的标题,好笑,评论条数及作者信息,解析爬取的信息数据,定制item数据存储结构,最终将数据存储于MongoDB数据库中.1234# 创建项目:scrapy startproject qsbk # 创建项目cd qsbk # 切换到项目目录scrapy genspider qsbk_hot www.qiushibaike.com # 创建爬虫文件, qsbk_hot为爬虫名, www...com为爬取范围\n\n1234567# item文件定义数据存储的字段:import scrapyclass QsbkItem(scrapy.Item):    title &#x3D; scrapy.Field()  # 标题    lau &#x3D; scrapy.Field()  # 好笑数    comment &#x3D; scrapy.Field()  # 评论数    auth &#x3D; scrapy.Field()  # 作者\n12345678910111213141516171819202122232425262728293031323334353637# spider文件中定义解析数据的方法class QsbkHotSpider(scrapy.Spider):    name &#x3D;&#39;qsbk_hot&#39;    # allowed_domains &#x3D; [&#39;www.qiushibaike.com&#39;] # 无用, 可注释掉    start_urls &#x3D;[&#39;http:&#x2F;&#x2F;www.qiushibaike.com&#x2F;&#39;]    # 思路:一条热点数据在前端中对应一个li标签, 将一页中的所有li标签取出, 再进一步操作    def parse(self, response):        li_list &#x3D; response.selector.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;recommend-article&quot;]&#x2F;ul&#x2F;li&#39;)        # 循环li标签组成的列表, 先实例化item, 再取需要的字段, 并该item对象的相应属性赋值        for li in li_list:            # 实例化item对象            item &#x3D;QsbkItem()            # 解析获取title(标题), lau(好笑数), comment(评论数), auth(作者)等信息            title &#x3D; li.xpath(&#39;.&#x2F;div[@class&#x3D;&quot;recmd-right&quot;]&#x2F;a&#x2F;text()&#39;).extract_first()            lau &#x3D; li.xpath(&#39;.&#x2F;div[@class&#x3D;&quot;recmd-right&quot;]&#x2F;div[@class&#x3D;&quot;recmd-detail clearfix&quot;]&#x2F;div&#x2F;span[1]&#x2F;text()&#39;).extract_first()            comment &#x3D; li.xpath(&#39;.&#x2F;div[@class&#x3D;&quot;recmd-right&quot;]&#x2F;div[@class&#x3D;&quot;recmd-detail clearfix&quot;]&#x2F;div&#x2F;span[4]&#x2F;text()&#39;).extract_first()            auth &#x3D; li.xpath(&#39;.&#x2F;div[@class&#x3D;&quot;recmd-right&quot;]&#x2F;div[@class&#x3D;&quot;recmd-detail clearfix&quot;]&#x2F;a&#x2F;span&#x2F;text()&#39;).extract_first()            # 因为部分热点数据还没有评论和好笑数, 所以需对数据进行处理            if not lau:                lau &#x3D;None            if not comment:                comment &#x3D;None            # 将字段的值存储在item的属性中            item[&quot;title&quot;]&#x3D; title            item[&quot;lau&quot;]&#x3D; lau            item[&quot;comment&quot;]&#x3D; comment            item[&quot;auth&quot;]&#x3D; auth            # 返回item, 框架会自动将item传送至pipeline中的指定类            yield item\n\n12345678910111213141516171819# 在pipeline中定义管道类进行数据的存储import pymongoclassQsbkPipeline(object):　　# 连接MongoDB数据库\tconn &#x3D; pymongo.MongoClient(&quot;localhost&quot;, 27017)\tdb &#x3D; conn.qiubai  #(数据库名)\ttable &#x3D; db.qb_hot  #(表名)　　def process_item(self, item, spider):　　　　# 向数据库中出入数据　　　　self.table.insert(dict(item))　　　　# 此处return item是为了下一个管道类能够接收到item进行存储　　　　return item　　def close_spider(self):　　　　# 关闭数据库连接　　　　self.conn.close()\n\n123456789101112# 此示例中配置文件中的配置的项, 注意是不是全部的配置, 是针对该项目增加或修改的配置项# 忽略robots协议ROBOTSTXT_OBEY &#x3D;False# UA伪装USER_AGENT &#x3D; &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;76.0.3809.87 Safari&#x2F;537.36&#39;# 管道类的注册配置ITEM_PIPELINES &#x3D;&#123;&#39;qsbk.pipelines.QsbkPipeline&#39;:300,&#125;\n\n6.scrapy爬取校花网人名与图片下载链接需求: 爬取校花网大学校花的默认的第一页的所有图片src和人名, 并通过管道存入mongodb数据库1234# 创建项目:scrapy startproject xiaohuaspider # 创建项目cd xiaohuaspider # 切换到项目目录scrapy genspider hua www.baidu.com # 创建爬虫文件, hua为爬虫名, www.baidu.com为爬取范围\n\n\n12345# 创建item类, 用于存储解析出的数据import scrapyclass XiaohuaspiderItem(scrapy.Item):    name &#x3D; scrapy.Field()    src &#x3D; scrapy.Field()\n\n12345678910111213141516171819# spider中定义爬取的行为与解析数据的操作import scrapyfrom ..items import XiaohuaspiderItemclass HuaSpider(scrapy.Spider):    name &#x3D; &#39;hua&#39;    # allowed_domains &#x3D; [&#39;www.baidu.com&#39;]    start_urls &#x3D; [&#39;http:&#x2F;&#x2F;www.xiaohuar.com&#x2F;hua&#x2F;&#39;]    def parse(self, response):        div_list &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;img&quot;]&#39;)        for div in div_list:            item &#x3D; XiaohuaspiderItem()            name &#x3D; div.xpath(&#39;.&#x2F;&#x2F;span&#x2F;text()&#39;).extract_first()            src &#x3D; div.xpath(&#39;.&#x2F;a&#x2F;img&#x2F;@src&#39;).extract_first()            item[&quot;name&quot;] &#x3D; name            item[&quot;src&quot;] &#x3D; src            yield item\n\n123456789101112# itemPipeline编码, 持久化数据到本地import pymongoclass XiaohuaspiderPipeline(object):    conn &#x3D; pymongo.MongoClient(&#39;localhost&#39;, 27017)    db &#x3D; conn.xiaohua    table &#x3D; db.hua    def process_item(self, item, spider):        self.table.insert(dict(item))        return item    def close_spider(self, spider):        self.conn.close()\n\n1234567891011# 配置项:# UA伪装:USER_AGENT &#x3D; &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;76.0.3809.87 Safari&#x2F;537.36&#39;# 忽略robots协议:ROBOTSTXT_OBEY &#x3D; False# 开启管道类ITEM_PIPELINES &#x3D; &#123;   &#39;xiaohuaspider.pipelines.XiaohuaspiderPipeline&#39;: 300,&#125;\n\n","categories":["爬虫"],"tags":["python"]},{"title":"Scrapy & Django项目","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/05/29/Scrapy & Django项目/","content":"需求: 编写爬虫项目与Django项目详解和, 将爬取到的数据展示到前端页面上爬虫的编写:123456789101112131415161718192021222324252627# spider编写:import scrapyfrom dl.items import DlItemclass PSpider(scrapy.Spider):    name &#x3D; &#39;p&#39;    # allowed_domains &#x3D; [&#39;www.baidu.com&#39;]    start_urls &#x3D; [&#39;https:&#x2F;&#x2F;www.kuaidaili.com&#x2F;free&#x2F;&#39;]    def parse(self, response):        # print(response)        tr_list &#x3D; response.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;list&quot;]&#x2F;table&#x2F;tbody&#x2F;tr&#39;)        # print(tr_list)        for tr in tr_list:            ip &#x3D; tr.xpath(&#39;.&#x2F;td[1]&#x2F;text()&#39;).extract_first()            port &#x3D; tr.xpath(&#39;.&#x2F;td[2]&#x2F;text()&#39;).extract_first()            typ &#x3D; tr.xpath(&#39;.&#x2F;td[3]&#x2F;text()&#39;).extract_first()            protocal &#x3D; tr.xpath(&#39;.&#x2F;td[4]&#x2F;text()&#39;).extract_first()            position &#x3D; tr.xpath(&#39;.&#x2F;td[5]&#x2F;text()&#39;).extract_first()            # print(ip, port, protocal, position)            item &#x3D; DlItem()            item[&#39;ip&#39;] &#x3D; ip            item[&#39;port&#39;] &#x3D; port            item[&#39;typ&#39;] &#x3D; typ            item[&#39;protocal&#39;] &#x3D; protocal            item[&#39;position&#39;] &#x3D; position            print(item)            yield item\n\nitems编码12345678# items编码import scrapyclass DlItem(scrapy.Item):    ip &#x3D; scrapy.Field()    port &#x3D; scrapy.Field()    typ &#x3D; scrapy.Field()    protocal &#x3D; scrapy.Field()    position &#x3D; scrapy.Field()\n\nDjango项目创建与所有配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546# Django项目创建与所有配置:1.models创建:from django.db import models# Create your models here.class Proxy(models.Model):    ip &#x3D; models.CharField(max_length&#x3D;50)    port &#x3D; models.CharField(max_length&#x3D;50)    typ &#x3D; models.CharField(max_length&#x3D;50)    protocal &#x3D; models.CharField(max_length&#x3D;50)    position &#x3D; models.CharField(max_length&#x3D;50)    2.在scrapy框架项目中嵌入djangoimport osimport syssys.path.append(os.path.dirname(os.path.abspath(&#39;.&#39;)))os.environ[&#39;DJANGO_SETTINGS_MODULE&#39;] &#x3D; &#39;proxyscan.settings&#39;# 手动初始化Django：import djangodjango.setup()3.修改爬虫item:import scrapyfrom scrapy_djangoitem import DjangoItemfrom proxy import modelsclass DlItem(DjangoItem):    django_model &#x3D; models.Proxy    4.pipeline编码:class DlPipeline(object):    def process_item(self, item, spider):        print(&#39;开启数据库, 进行数据存储&#39;)        item.save()        print(&#39;关闭数据库&#39;)        return item    5.Django项目迁移数据库与admin后台配置Python manage.py makemigrationspython manage.py migratefrom proxy.models import Proxyadmin.site.register(Proxy)# 创建超级用户:Python manage.py createsuperuser\n\n1234567891011121314151617# 路由:from django.conf.urls import urlfrom django.contrib import adminfrom proxy.views import indexurlpatterns &#x3D; [    url(r&#39;^admin&#x2F;&#39;, admin.site.urls),    url(r&#39;^index&#x2F;&#39;, index),]# 视图函数:from django.shortcuts import renderfrom proxy.models import Proxydef index(requests):    p &#x3D; Proxy.objects.all()    return render(requests, &#39;index.html&#39;, &#123;&quot;p&quot;:p&#125;)\n\n前端代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 前端代码:&lt;!DOCTYPE html&gt;&lt;html lang&#x3D;&quot;en&quot;&gt;&lt;head&gt;    &lt;meta charset&#x3D;&quot;UTF-8&quot;&gt;    &lt;title&gt;Title&lt;&#x2F;title&gt;    &lt;script src&#x3D;&quot;https:&#x2F;&#x2F;cdn.bootcss.com&#x2F;jquery&#x2F;3.4.1&#x2F;jquery.min.js&quot;&gt;&lt;&#x2F;script&gt;    &lt;link href&#x3D;&quot;https:&#x2F;&#x2F;cdn.bootcss.com&#x2F;twitter-bootstrap&#x2F;4.3.1&#x2F;css&#x2F;bootstrap.min.css&quot; rel&#x3D;&quot;stylesheet&quot;&gt;&lt;&#x2F;head&gt;&lt;body&gt;&lt;div class&#x3D;&quot;container&quot;&gt;    &lt;div class&#x3D;&quot;row&quot; &gt;        &lt;div class&#x3D;&quot;col-md-10 col-md-offset-2&quot; style&#x3D;&quot;margin:0 auto&quot;&gt;            &lt;div class&#x3D;&quot;panel panel-primary&quot;&gt;                &lt;div class&#x3D;&quot;panel-heading&quot; style&#x3D;&quot;margin-top:50px&quot;&gt;                    &lt;h3 class&#x3D;&quot;panel-title&quot;&gt;代理IP一览表&lt;&#x2F;h3&gt;                &lt;&#x2F;div&gt;                &lt;div class&#x3D;&quot;panel-body&quot;&gt;                    &lt;table class&#x3D;&quot;table table-striped&quot;&gt;                        &lt;thead&gt;                        &lt;tr&gt;                            &lt;th&gt;IP&lt;&#x2F;th&gt;                            &lt;th&gt;Port&lt;&#x2F;th&gt;                            &lt;th&gt;Type&lt;&#x2F;th&gt;                            &lt;th&gt;Protocal&lt;&#x2F;th&gt;                            &lt;th&gt;Positon&lt;&#x2F;th&gt;                        &lt;&#x2F;tr&gt;                        &lt;&#x2F;thead&gt;                        &lt;tbody&gt;                        &#123;% for i in p %&#125;                            &lt;tr&gt;                                &lt;th&gt;&#123;&#123; i.ip &#125;&#125;&lt;&#x2F;th&gt;                                &lt;td&gt;&#123;&#123; i.port &#125;&#125;&lt;&#x2F;td&gt;                                &lt;td&gt;&#123;&#123; i.typ &#125;&#125;&lt;&#x2F;td&gt;                                &lt;td&gt;&#123;&#123; i.protocal &#125;&#125;&lt;&#x2F;td&gt;                                &lt;td&gt;&#123;&#123; i.position &#125;&#125;&lt;&#x2F;td&gt;                            &lt;&#x2F;tr&gt;                        &#123;% endfor %&#125;                        &lt;&#x2F;tbody&gt;                    &lt;&#x2F;table&gt;                &lt;&#x2F;div&gt;            &lt;&#x2F;div&gt;        &lt;&#x2F;div&gt;    &lt;&#x2F;div&gt;&lt;&#x2F;div&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt;","categories":["爬虫"],"tags":["python"]},{"title":"Redis雪崩&穿透&击穿","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/05/16/Redis雪崩&穿透&击穿/","content":"1. 缓存穿透123451）定义1. 缓存穿透是指查询一个一定不存在的数据，由于缓存不命中，接着查询数据库也无法查询出结果，2. 虽然也不会写入到缓存中，但是这将会导致每个查询都会去请求数据库，造成缓存穿透；2）解决方法 ：布隆过滤1. 对所有可能查询的参数以hash形式存储，在控制层先进行校验，不符合则丢弃，从而避免了对底层存储系统的查询压力；\n\n\n2.缓存雪崩12345671）定义　　　　　　1. 缓存雪崩是指，由于缓存层承载着大量请求，有效的保护了存储层，但是如果缓存层由于某些原因整体不能提供服务2. 于是所有的请求都会达到存储层，存储层的调用量会暴增，造成存储层也会挂掉的情况。2）解决方法1. 保证缓存层服务高可用性：比如 Redis Sentinel 和 Redis Cluster 都实现了高可用2. 依赖隔离组件为后端限流并降级：比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。\n\n\n3. 缓存击穿1234561）定义：1. 缓存击穿，就是说某个 key 非常热点，访问非常频繁，处于集中式高并发访问的情况2. 当这个 key 在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库，就像是在一道屏障上凿开了一个洞。2）解决方法1. 解决方式也很简单，可以将热点数据设置为永远不过期；2. 或者基于 redis or zookeeper 实现互斥锁，等待第一个请求构建完缓存之后，再释放锁，进而其它请求才能通过该 key 访问数据\n","categories":["redis"],"tags":["python"]},{"title":"Redis数据操作","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/05/01/Redis数据操作/","content":"Redis数据结构\nredis是key-value的数据结构，每条数据都是⼀个键值对\n键的类型是字符串\n注意：键不能重复\n\n数据结构\n值的类型分为五种：\n\n字符串string\n哈希hash\n列表list\n集合set\n有序集合zset\n\n数据操作行为\n\n保存\n修改\n获取\n删除中⽂官⽹查看命令⽂档 http://redis.cn/commands.htmlstring类型\n\n字符串类型是Redis中最为基础的数据存储类型，它在Redis中是二进制安全的，这便意味着该类型可以接受任何格式的数据，如JPEG图像数据或Json对象描述信息等。在Redis中字符串类型的Value最多可以容纳的数据长度是512M。\n保存如果设置的键不存在则为添加，如果设置的键已经存在则修改\n设置键值set key value例1：设置键为name值为itcast的数据\n1set name yifchan\n\n\n\n设置键值及过期时间，以秒为单位\n1setex key seconds value\n\n例2：设置键为aa值为aa过期时间为3秒的数据\n1setex aa 3 aa\n\n设置多个键值\n1mset key1 value1 key2 value2 ...\n\n例3：设置键为’a1’值为’python’、键为’a2’值为’java’、键为’a3’值为’c’\n1mset a1 python a2 java a3 c\n\n\n追加值\n1append key value\n\n例4：向键为a1中追加值’ haha’\n1append &#39;a1&#39; &#39;haha&#39;\n\n\n\n获取获取：根据键获取值，如果不存在此键则返回nil\n1get key\n例5：获取键’name’的值\n1get &#39;name&#39;\n\n\n根据多个键获取多个值\n1mget key1 key2 ...\n\n例6：获取键a1、a2、a3’的值\n1mget a1 a2 a3\n\n\n删除详⻅下节键的操作，删除键时会将值删除\n键命令查找键，参数⽀持正则表达式\n1keys pattern\n\n例1：查看所有键\n1keys *\n\n例2：查看名称中包含a的键\n1keys &#39;a*&#39;\n\n\n\n判断键是否存在，如果存在返回1，不存在返回0\n1exists key1\n\n例3：判断键a1是否存在\n1exists a1\n\n\n查看键对应的value的类型\n1type key\n\n例4：查看键a1的值类型，为redis⽀持的五种类型中的⼀种\n1type a1\n\n\n\n删除键及对应的值\n1del key1 key2 ...\n\n例5：删除键a2、a3\n1del a2 a3\n\n\n\n设置过期时间，以秒为单位\n如果没有指定过期时间则⼀直存在，直到使⽤DEL移除\n1expire key seconds\n\n例6：设置键’a1’的过期时间为3秒\n1expire &#39;a1&#39; 3\n\n\n\n查看有效时间，以秒为单位\n1ttl key\n例7：查看键’bb’的有效时间\n1ttl bb\nhash类型\nhash⽤于存储对象，对象的结构为属性、值\n值的类型为string\n增加、修改\n\n设置单个属性\n1hset key field value\n\n例1：设置键 user的属性name为itheima\n1hset user name itheima\n可能报错\n12MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.\nRedis被配置为保存数据库快照，但它目前不能持久化到硬盘。用来修改集合数据的命令不能用\n原因：\n强制关闭Redis快照导致不能持久化。 解决方案：\n运行 config set stop-writes-on-bgsave-error no 命令后，关闭配置项stop-writes-on-bgsave-error解决该问题。设置多个属性\nhmset key field1 value1 field2 value2 …例2：设置键u2的属性name为itcast、属性age为11\nhmset u2 name itcast age 11\n获取获取指定键所有的属性\n1hkeys key\n\n例3：获取键u2的所有属性\n1hkeys u2\n\n\n获取⼀个属性的值\n1hget key field\n\n例4：获取键u2属性’name’的值\n1hget u2 &#39;name&#39;\n\n获取多个属性的值\n1hmget key field1 field2 ...\n\n例5：获取键u2属性’name’、’age的值\n1hmget u2 name age\n\n\n获取所有属性的值\n1hvals key\n\n例6：获取键’u2’所有属性的值\n1hvals u2\n\n\n\n删除删除整个hash键及值，使⽤del命令删除属性，属性对应的值会被⼀起删除\n1hdel key field1 field2 ...\n\n例7：删除键’u2’的属性’age’\n1hdel u2 age\n\n\n\n\nlist类型\n列表的元素类型为string\n按照插⼊顺序排序\n\n增加在左侧插⼊数据\n1lpush key value1 value2 ...\n\n例1：从键为’a1’的列表左侧加⼊数据a 、 b 、c\n1lpush a1 a b c\n\n\n\n在右侧插⼊数据\n1rpush key value1 value2 ...\n\n例2：从键为’a1’的列表右侧加⼊数据0 1\n1rpush a1 0 1\n\n\n\n在指定元素的前或后插⼊新元素\n1linsert key before或after 现有元素 新元素\n\n例3：在键为’a1’的列表中元素’b’前加⼊’3’\n1linsert a1 before b 3\n\n\n获取返回列表⾥指定范围内的元素\n\nstart、stop为元素的下标索引\n索引从左侧开始，第⼀个元素为0\n索引可以是负数，表示从尾部开始计数，如-1表示最后⼀个元素lrange key start stop例4：获取键为’a1’的列表所有元素1lrange a1 0 -1\n\n\n\n\n设置指定索引位置的元素值索引从左侧开始，第⼀个元素为0索引可以是负数，表示尾部开始计数，如-1表示最后⼀个元素\n1lset key index value\n例5：修改键为’a1’的列表中下标为1的元素值为’z’\n1lset a 1 z\n\n删除删除指定元素\n将列表中前count次出现的值为value的元素移除\n\ncount &gt; 0: 从头往尾移除\ncount &lt; 0: 从尾往头移除\ncount = 0: 移除所有1lrem key count value\n\n\n\n例6.1：向列表’a2’中加⼊元素’a’、’b’、’a’、’b’、’a’、’b’\n1lpush a2 a b a b a b\n\n例6.2：从’a2’列表右侧开始删除2个’b’\n1lrem a2 -2 b\n\n例6.3：查看列表’py12’的所有元素\n1lrange a2 0 -1\n\n\nset类型\n⽆序集合\n元素为string类型\n元素具有唯⼀性，不重复\n说明：对于集合没有修改操作\n\n增加添加元素\n1sadd key member1 member2 ...\n例1：向键’a3’的集合中添加元素’zhangsan’、’lisi’、’wangwu’\n1sadd a3 zhangsan sili wangwu\n\n获取返回所有的元素\n1smembers key\n例2：获取键’a3’的集合中所有元素\n1smembers a3\n\n\n删除删除指定元素\n1srem key\n例3：删除键’a3’的集合中元素’wangwu’\n1srem a3 wangwu\n\n\nzset类型\nsorted set，有序集合\n元素为string类型\n元素具有唯⼀性，不重复\n每个元素都会关联⼀个double类型的score，表示权重，通过权重将元素从⼩到⼤排序\n说明：没有修改操作\n\n增加添加\n1zadd key score1 member1 score2 member2 ...\n\n例1：向键’a4’的集合中添加元素’lisi’、’wangwu’、’zhaoliu’、’zhangsan’，权重分别为4、5、6、3\n1zadd a4 4 lisi 5 wangwu 6 zhaoliu 3 zhangsan\n\n\n获取返回指定范围内的元素\n\nstart、stop为元素的下标索引\n索引从左侧开始，第⼀个元素为0\n索引可以是负数，表示从尾部开始计数，如-1表示最后⼀个元素1zrange key start stop\n\n\n\n例2：获取键’a4’的集合中所有元素\n1zrange a4 0 -1\n\n\n\n返回score值在min和max之间的成员\n1zrangebyscore key min max\n例3：获取键’a4’的集合中权限值在5和6之间的成员\n1zrangebyscore a4 5 6\n\n返回成员member的score值\n1zscore key member\n\n例4：获取键’a4’的集合中元素’zhangsan’的权重\n1zscore a4 zhangsan\n\n删除删除指定元素\n1zrem key member1 member2 ...\n\n例5：删除集合’a4’中元素’zhangsan’\n1zrem a4 zhangsan\n\n\n删除权重在指定范围的元素\n1zremrangebyscore key min max\n例6：删除集合’a4’中权限在5、6之间的元素\n1zremrangebyscore a4 5 6\n","categories":["redis"],"tags":["python"]},{"title":"Redis使用教程","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/04/15/Redis使用教程/","content":"1. Redis的安装过程（1） 下载Redis这里就随便下，无论是安装包也好压缩包也好怎么都行\n（2） 配置环境变量配完了之后可以不需要进入Redis的那个目录，然后可以直接敲redis…..把这个服务启动起来。\n（3） 把Redis变成一个服务至少你在服务页面估计能看到他，然后定成自动启动，省的天天手动。\n（4） 设置密码在Redis这个文件夹里面编辑redis.windows-service.conf，类似的conf文件有两个，千万分清楚这个后面带service的。使用这种方式设置是永久性设置。（5） 打开本地客户端如果要查询里面的键值对是什么之类的，就用redis-cli。设置了密码之后，上来输入的就是 auth 123456（根据自己的密码改）\n\n安装、打开、设置密码（就是配Redis环境）这些应该也可以用docker完成\n2. SpringBoot集成Redis（1） 在pom里导入一个jar包（2） yml配置\n123456789101112redis:   host: localhost # Redis服务器地址   database: 0 # Redis数据库索引（默认为0）   port: 6379 # Redis服务器连接端口   password:&quot;test&quot; # Redis服务器连接密码（默认为空，以及这里带引号！！！）   jedis:     pool:       max-active: 8 # 连接池最大连接数（使用负值表示没有限制），能制造出来的最大的连接数       max-wait: -1ms # 连接池最大阻塞等待时间（使用负值表示没有限制）       max-idle: 8 # 连接池中的最大空闲连接，空闲太多可能销毁       min-idle: 0 # 连接池中的最小空闲连接，务必保持几个控线连接   timeout: 3000ms # 连接超时时间（毫秒）\n\n（3） 注入Template开始用经过了上两个步骤之后，其实就已经配置好了，在代码里面注入Template就可以set、get然后启用了。\nTeplate目前已知的有两种，一种是RedisTemplate，一种是StringRedisTemplate（这种其实也就是RedisTemplate&lt;String,String&gt;），就是封装了一下写的高大上了。所以自己用的时候只能用RedisTemplate，如果需要的话自己写泛型。\n序列化器：由于RedisTemplate里面注入的直接就是object，需要被缓存的类实现了Serielizable接口才行。\n简单写一下Serielizable：Java平台允许我们在内存中创建可复用的Java对象，但一般情况下，只有当JVM处于运行状态时，这些对象才能存在，即，这些对象的生命周期不会比JVM更长。但在现实中，就可能要求在JVM运行停止后能够保存（持久化），以便下次运行时重新获取对象，Java对象序列化就能够帮助我们实现该功能。网络上远程传递的时候也需要实现这个功能。\n如果仅仅只是让某个类实现Serializable接口，而没有其他处理的话，则就是使用默认序列化机制。使用默认机制，在序列化对象时，不仅会序列化当前对象本身，还会对该对象引用的其他对象也进行序列化，同样的，这些其他对象引用的另外对象也将被序列化，以此类推。所以，如果一个对象的成员变量是容器类对象，而这些容器含有的元素也是容器类对象，那么这个序列化对象的过程就会很复杂，开销也很大。transient关键字可以忽略掉某些字段。\n3. Redis连接池（1） 为什么要用连接池：连接池定义：连接池是创建和管理一个连接的缓冲池的技术，这些连接准备好被任何需要它们的线程。\n连接池管理的就是连接，每次存取数据的时候，都要new一个连接对象出来，建立一个TCP连接要经历三次握手什么的，这样挺浪费时间的，所以每次用完连接对象之后不释放，存到连接池里面去，下次用连接对象的时候去连接池里面获取。\n（2） 手写Redis连接池：Redis连接池这东西一共就三个函数，一个init，一个getResource，一个release。其中尤其以getResources为核心。\n首先如果要完成连接池保存，要有存储容器，set、list、queue、array、map这些是要选择一下的。频繁的存取，不能用array，不能用array的也不能用list，map不需要key，queue和set两个平分，最终用queue。那就要有两个queue来放置空闲的和被使用的连接。\ngetResource的调用流程：（中间把config都用上了）如果有空闲连接直接返回没有空闲的但是目前总数还没超过max，可以再new一个总数已经到max了，没法new了就只能等如果超过了等待时间，返回报错\nrelease调用流程：把这个连接从使用中队列移动到空闲队列\n","categories":["redis"],"tags":["python"]},{"title":"python基础数据类型的操作","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/03/28/python基础数据类型的操作/","content":"列表和元组1.列表基本操作1.1  列表赋值　12a = [1,2,3,4,5,6,7,8]a[0] = 100        #the result ： [100, 2, 3, 4, 5, 6, 7, 8]\n1.2 元素删除12a = [1,2,3,4,5,6,7,8]del a[0]　　                #the result ： [2, 3, 4, 5, 6, 7, 8]\n1.3 分片赋值1234a = [1,2,3,4,5,6,7,8]a[::2]                  # [1, 3, 5, 7]a[-2:]                  # [7, 8]a[1:1] = [0,0,0]             # the result : [1, 0, 0, 0, 2, 3, 4, 5, 6, 7, 8]\n1.4 使用 for i in range(10,-1,-1) 生成列表12345for i in range(10,-1,-1): # 开始位置（10），结束位置（-1）， 步长（-1）\t\t\tprint i,# 打印结果：10 9 8 7 6 5 4 3 2 1 0# 从10开始，每次向后取一个值，直到遇到结束位置 -1\n2.列表方法2.1 append作用：append用于在列表末尾追加新的对象\n12a = [1,2,3]a.append(4)          　　　　　　　#the result ： [1, 2, 3, 4]\n2.2  count作用：count方法统计某个元素在列表中出现的次数\n1a =[&#x27;aa&#x27;,&#x27;bb&#x27;,&#x27;cc&#x27;,&#x27;aa&#x27;,&#x27;aa&#x27;]print(a.count(&#x27;aa&#x27;))                  #the result ： 3\n2.3   extend作用：extend方法可以在列表的末尾一次性追加另一个序列中的多个值\n123a = [1,2,3]b = [4,5,6]a.extend(b)       #the result ：[1, 2, 3, 4, 5, 6]\n2.4  index作用：index函数用于从列表中找出某个值第一个匹配项的索引位置\n1a = [1,2,3,1]``print(a.index(1))                  #the result ： 0`\n2.5  insert作用： insert方法用于将对象插入到列表中\n1a = [1,2,3]``a.insert(0,``&#x27;aa&#x27;``)      #the result : [``&#x27;aa&#x27;``, 1, 2, 3]`\n2.6  pop作用：pop方法会移除列表中的一个元素（默认是最后一个），并且返回该元素的值\n1a = [1,2,3]``a.pop()                         #the result ： [1, 2]``a.pop(0)`\n2.7   remove作用：remove方法用于移除列表中某个值的第一个匹配项\n12a = [&#x27;aa&#x27;,&#x27;bb&#x27;,&#x27;cc&#x27;,&#x27;aa&#x27;]a.remove(&#x27;aa&#x27;)            #the result ： [&#x27;bb&#x27;, &#x27;cc&#x27;, &#x27;aa&#x27;]\n2.8  reverse作用：reverse方法将列表中的元素反向存放\n1a = [&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;]a.reverse()         #the result ： [&#x27;c&#x27;, &#x27;b&#x27;, &#x27;a&#x27;]\n2.9  sort　　作用：sort方法用于在原位置对列表进行排序，意味着改变原来的列表，让其中的元素按一定顺序排列\n12a = [&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,1,2,3]a.sort()            #the result ：[1, 2, 3, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;]\n2.10  enumrate123li = [11,22,33]for  k,v  in enumerate(li, 1):print(k,v)\n\n2.11 range和xrange 指定范围，生成指定的数字注：python3中的range类似python2中的xrange，比如a = range(1,4) : a返回的不是列表对象而是一个可迭代对象（&lt;class ‘range’&gt;）\n12#1、range根据start与stop指定的范围以及step设定的步长，生成一个序列：range([start,] stop[, step])``#2、xrange 用法与 range 完全相同，所不同的是生成的不是一个list对象，而是一个生成器for i in range(1,10,2):print(i)`\n2.12  列表去空1234567#方法1：filter(None, your_list)#方法2：while &#x27;&#x27; in your_list:\tyour_list.remove(&#x27;&#x27;)#方法3：your_list = [x  for x  in your_list  if x != &#x27;&#x27;]\n\n3.元组定义：元组和列表一样，也是一种序列，唯一的不同是元组不能修改。3.1  创建元组举例1234#1. 创建元组a = (1,2,3,4,5,6)#2. 将列表转换成元组tuple([1,2,3,4])                  #the result ： (1, 2, 3, 4)\n4.列表和元组常用函数com(x,y) 比较两个值len(seq) 返回序列的长度list(seq) 把序列转换成列表max(args) 返回序列或者参数集合中得最大值min(args) 返回序列或者参数集合中的最小值reversed(seq) 对序列进行反向迭代sorted(seq) 返回已经排列的包含seq 所有元素的列表tuple(seq) 把序列转换成元组\n字符串1.字符串格式化1.1 使用百分号（%）字符串格式化123num = 100print(&quot;%d to hex is %x&quot;%(num, num))    #100 to hex is 64 print(&quot;%d to hex is %#x&quot;%(num, num))    #100 to hex is 0x64\n\n1.2  使用format字符串格式化1234#1. 位置参数print(&quot;&#123;0&#125; is &#123;1&#125; years old&quot;.format(&quot;tom&quot;, 28))      #tom ``is` `28 years old``print(``&quot;&#123;&#125; is &#123;&#125; years old&quot;``.format(``&quot;tom&quot;``, 28))       #tom ``is` `28 years old``print(``&quot;Hi, &#123;0&#125;! &#123;0&#125; is &#123;1&#125; years old&quot;``.format(``&quot;tom&quot;``, 28)) #Hi, tom! tom ``is` `28 years old` `#2. 关键字参数``print(``&quot;&#123;name&#125; is &#123;age&#125; years old&quot;``.format(name = ``&quot;tom&quot;``, age = 28))  #tom ``is` `28 years old` `#3. 下标参数``li = [``&quot;tom&quot;``, 28]``print(``&quot;&#123;0[0]&#125; is &#123;0[1]&#125; years old&quot;``.format(li))     #tom ``is` `28 years old`\n2.字符串方法2.1  find方法作用：find方法可以在一个较长的字符串中查找子串，他返回子串所在位置的最左端索引，如果没有找到则返回-1\n1234a=&#x27;abcdefghijk&#x27;print(a.find(&#x27;abc&#x27;))             #the result ： 0print(a.find(&#x27;abc&#x27;,10,100))          #the result ： 11 指定查找的起始和结束查找位置\n2.2  join方法作用：join方法是非常重要的字符串方法，他是split方法的逆方法，用来连接序列中的元素，并且需要被连接的元素都必须是字符串。\n1a = [``&#x27;1&#x27;``,``&#x27;2&#x27;``,``&#x27;3&#x27;``]``print(``&#x27;+&#x27;``.``join``(a))                  #the result ： 1+2+3\n2.3  split方法作用：这是一个非常重要的字符串，它是join的逆方法，用来将字符串分割成序列\n1print(&#96;&#96;&#39;1+2+3+4&#39;&#96;&#96;.split(&#96;&#96;&#39;+&#39;&#96;&#96;))              #the result ： [&#96;&#96;&#39;1&#39;&#96;&#96;, &#96;&#96;&#39;2&#39;&#96;&#96;, &#96;&#96;&#39;3&#39;&#96;&#96;, &#96;&#96;&#39;4&#39;&#96;&#96;]\n2.4  strip作用：strip 方法返回去除首位空格（不包括内部）的字符串\n12print(``&quot;  test  test  &quot;``.strip())        #the result ：“test  test”\n\n2.5 replace作用：replace方法返回某字符串所有匹配项均被替换之后得到字符串\n12print(``&quot;This is a test&quot;``.replace(``&#x27;is&#x27;``,``&#x27;is_test&#x27;``))   #the result ： This_test is_test a test\n2.6 首字母大写1&gt;&gt;&gt; s = ``&#x27;aBdkndfkFFD&#x27;``&gt;&gt;&gt; s.capitalize()``&#x27;Abdkndfkffd&#x27;`\n\n2.7 Pinyin 模块，将汉字转换成拼音1234567891011from xpinyin import Pinyin while True:  p = Pinyin()  fullname =raw_input(&#x27;name：&#x27;).strip()  fullname = fullname.decode(&#x27;utf8&#x27;)  print(fullname)  xin = fullname[0]  ming = fullname[1:]  name = ming + ``&#x27;.&#x27;` `+ xinusername = p.get_pinyin(name, &#x27;&#x27;)  print usernameprint username + &#x27;@yiducloud.cn\n字典1.字典基本使用1.1 键类型：字典的键不一定为整形数据，键可以是任意的不可变类型，比如浮点型（实行），字符串或者元组。 ### 1.2 自动添加：即使键起初在字典中不存在，也可以为他赋值，这样字典就会建立新的项。而（在不适用append方法或者其他类似操作的情况下）不能将值关联到列表之外的索引上。123phonebook = &#123;``&#x27;Tom&#x27;``:8777,``&#x27;Jack&#x27;``:9999,``&#x27;Fly&#x27;``:6666&#125;print(``&quot;Tom&#x27;s Phonenumber is %(Tom)s&quot;` `% phonebook)    #Tom&#x27;s Phonenumber ``is` `8777`\n2.字典常用方法2.1 clear作用：clear方法清除字典中所有的项，这是一个原地操作，所以无返回值（或则说返回None）\n123456d = &#123;&#125;``d[``&#x27;Tom&#x27;``]=8777``d[``&#x27;Jack&#x27;``]=9999``print(d)                #the result : &#123;``&#x27;Jack&#x27;``: 9999, ``&#x27;Tom&#x27;``: 8777&#125;``d.clear()``print(d)                #the result : &#123;&#125;`\n2.2 copy作用：copy方法返回一个具有相同 ”键-值” 对的新字典，而不是副本\n1234567d = &#123;&#x27;Tom&#x27;:8777,&#x27;Fly&#x27;:6666&#125;a = d.copy()a[&#x27;Tom&#x27;] = &#x27;改变后的值&#x27;print(d)           #&#123;``&#x27;Fly&#x27;``: 6666, ``&#x27;Tom&#x27;``: 8777&#125;print(a)           #&#123;``&#x27;Fly&#x27;``: 6666, ``&#x27;Tom&#x27;: &#x27;改变后的值&#x27;&#125;\n\n2.3 fromkeys作用：fromkeys方法使用给定的键建立新的字典，每个键都对应一个默认的值None。\n　　首先建造一个空字典，然后调用它的fromkeys方法，建立另一个字典\n12345678910print(&#123;&#125;.fromkeys([&#x27;name&#x27;,&#x27;age&#x27;]))     #the result　：　&#123;``&#x27;age&#x27;``: None, ``&#x27;name&#x27;``: None&#125;`### 2.4  get作用：get方法是个更宽松的访问字典项的方法，如果试图访问字典中不存在的项时不会报错仅会 返回：None```pythond = &#123;``&#x27;Tom&#x27;``:8777,``&#x27;Jack&#x27;``:8888,``&#x27;Fly&#x27;``:6666&#125;print(d.``get``(``&#x27;Tom&#x27;``))                #the result ：  8777`` print(d.``get``(``&#x27;not_exist&#x27;``))              #the result ：   None`\n\n2.5. for循环字典的三种方法1234567d = &#123;`Tom&#x27;:8777,&#x27;Jack&#x27;:8888,&#x27;Fly&#x27;:6666&#125;for` k,v  in d.items():\tprint(k,v)for` k  in d.values():  \tprint(k) for` k  in d.keys():  print(k)\n\n2.6  pop作用：pop方法用于获得对应与给定键的值，然后将这个”键-值”对从字典中移除\n1234d = &#123;&#x27;Tom&#x27;:8777,&#x27;Jack&#x27;:8888,&#x27;Fly&#x27;:6666&#125;v = d.pop(&#x27;Tom&#x27;)print(v)          #8777\n\n2.7  popitem① popitem方法类似于list.pop，list.pop会弹出列表的最后一个元素，但是popitem仅仅会弹出随机的项，因为字典没有”最后的元素”或则其他有关顺序的概念 　　② 所以想一个接一个的移除并处理字典中的项，popitem是非常有效的（因为不用获取键的列表） 　　③ 尽管popitem和列表的pop方法很类似，但是字典中没有与append等价的方法，因为字典是无序的，类似于append得方法是没有任何意义的\n12d = &#123;&#x27;Tom&#x27;:8777,&#x27;Jack&#x27;:8888,&#x27;Fly&#x27;:6666&#125;d.popitem()\n2.8  setdefault作用：setdefault方法在某种程度上类似于get方法，能够获得与给定键相关联的值，除此之外，setdefault还能在字典中不含有给定键的情况下设定相应的键值\n1234567d = &#123;&#x27;Tom&#x27;:8777,&#x27;Jack&#x27;:8888,&#x27;Fly&#x27;:6666&#125;d.setdefault(``&#x27;Tom&#x27;``)              #the result ： 8777print(d.setdefault(&#x27;Test&#x27;))          #the result ： Noneprint(d)                  #&#123;&#x27;Fly&#x27;: 6666, &#x27;Jack&#x27;: 8888, &#x27;Tom&#x27;: 8777, &#x27;Test&#x27;: None&#125;\n\n2.9  update作用：update方法可以利用一个字典项更新另一个字典，提供的字典中的项会被添加到旧的字典中，如有相同的键则会被覆盖\n12345d = &#123;&#x27;Tom&#x27;:8777,&#x27;Jack&#x27;:8888,&#x27;Fly&#x27;:6666&#125;``a = &#123;&#x27;Tom&#x27;:110,&#x27;Test&#x27;:119&#125;d.update(a)prin t(d)            #the result ：&#123;&#x27;Fly&#x27;: 6666, &#x27;Test&#x27;: 119, &#x27;Jack&#x27;: 8888, &#x27;Tom&#x27;: 110&#125;\n2.10 将两个列表组合成字典1234567keys = [&#x27;a&#x27;, &#x27;b&#x27;]values = [1, 2]#1、zip生成字典print(dict(zip(keys,values)))                   # &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2&#125;#2、for循环推倒字典print(&#123;keys[i]: values[i] for i in range(len(keys))&#125;)       # &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2&#125;\n集合1.集合作用　1.1  去重 　　1.2 取两个列表的交集 　1.3 取两个列表的并集\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566list_1 = [1,2,3,4,5,1,2]#1、去重(去除list_1中重复元素1,2)list_1 = set(list_1)                  #去重： &#123;1, 2, 3, 4, 5&#125;print(list_1)list_2 = set([4,5,6,7,8])#2、交集（在list_1和list_2中都有的元素4,5）print(list_1.intersection(list_2))           #交集： &#123;4, 5&#125;`#3、并集（在list_1和list_2中的元素全部打印出来，重复元素仅打印一次）print(list_1.union(list_2))               #并集： &#123;1, 2, 3, 4, 5, 6, 7, 8&#125;#4、差集print(list_1.difference(list_2))            #差集：在list_1中有在list_2中没有：  &#123;1, 2, 3&#125;print(list_2.difference(list_1))            #差集：在list_1中有在list_2中没有：  &#123;8, 6, 7&#125;#5、子集print(list_1.issubset(list_2))             #子集：  False  List_1中的元素是否全部在list2中#6、父集print(list_1.issuperset(list_2))            #父集：  False  List_1中是否包含list_2中的所有元素#7、交集print(list_1 &amp; list_2)                 #交集  &#123;4, 5&#125;#8、union并集print(list_1 | list_2)                 #并集： &#123;1, 2, 3, 4, 5, 6, 7, 8&#125;#9、difference差集print(list_1 - list_2)                 #差集：  &#123;1, 2, 3&#125;#10、在集合中添加一个元素999list_1.add(999)print(list_1)                      #Add()方法：     &#123;1, 2, 3, 4, 5, 999&#125;#11、删除集合中任意一个元素不会打印删除的值list_1.pop()                      #Pop()方法：      无返回值#12、discard删除集合中的指定元素，如过没有则返回Noneprint(list_1.discard(&quot;ddd&quot;))       #Discard()方法：  删除指定的值，没有返回None","categories":["python"],"tags":["python"]},{"title":"Python常见基础算法题","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/03/01/Python常见基础算法题/","content":"1.判断一个整数是否是回文数。回文数是指正序（从左向右）和倒序（从右向左）读都是一样的整数.示例 1:输入: 121输出: true\n示例 2:输入: -121输出: false解释: 从左向右读, 为 -121 。 从右向左读, 为 121- 。因此它不是一个回文数。\n示例 3:输入: 10输出: false解释: 从右向左读, 为 01 。因此它不是一个回文数。\n12345678910111213def is_num(num):    &#x27;&#x27;&#x27;    判断传递的参数是否是回文数    :param num: 传递的参数    :return: 是回文数返回True 不是则false    &#x27;&#x27;&#x27;    num1 = str(num)    if num1[::-1] == num1:        return True    else:        return Falseif __name__ == &#x27;__main__&#x27;:    print(is_num(121))\n\n2.分别用生成器和迭代器生成斐波那契示例 1:\n输出: 1  1   2   3   5  8  13   \n123456789101112131415161718192021222324252627282930313233343536373839# 迭代器class FibIterator(object):    def __init__(self,n):        &quot;&quot;&quot;实例属性的初始花和赋值&quot;&quot;&quot;        self.n = n      #  数列长度        self.current = 0    #  设置两个初始值        self.num1 = 0        self.num2 = 1   #  初始下标    def __next__(self):        &quot;&quot;&quot;返回迭代器对象的下一位置数据&quot;&quot;&quot;        # 能拿到数据的情况        if self.current &lt; self.n:            num = self.num1            self.num1,self.num2 = self.num2,self.num1+self.num2            self.current+=1            return num        # 拿不到数据的情况        else:            raise  StopIteration    #主动抛出异常    def __iter__(self):        return selfif __name__ == &#x27;__main__&#x27;:    fib = FibIterator(10)    for num in fib:        print(num) # 生成器def fib(n): # 创建一个函数    num1,num2 = 1,1    current = 1    # 初始值    while current &lt;= n:    # i小于等于n，n次数 循环的控制条件        yield num1    # 返回a的值，但不结束函数        num1,num2 = num2 , num1 + num2        current += 1     # 步长值for x in fib(10):    # 以for循环来获取yield每次的值    print(x)\n\n3.字符串相乘：给定两个以字符串形式表示的非负整数 num1 和 num2，返回 num1 和 num2 的乘积，它们的乘积也表示为字符串形式示例 :输入: num1 = “123”, num2 = “456”输出: “56088”\n1234567891011121314def Func(num1,num2):    alist1 = &#x27;&#x27;    alist2 = &#x27;&#x27;    for i in num1:        if i.isdigit():           alist1 += i    for j in num2:        if  j.isdigit():            alist2 += j    str1=str(int(alist1) * int(alist2 ))    return str1if __name__ == &#x27;__main__&#x27;:    print(Func(&#x27;11qq&#x27;,&#x27;zz99xx&#x27;))\n\n4.实现一个算法来实现反转字符数组的功能,反转的要求如下：将字符数组的字符进行反转,例如 [‘b’, ‘ ‘, ‘a’, ‘r’],变成 [‘r’, ‘a’, ‘ ‘, ‘b’] , 将字符数组替换为反转后的数组。解题思路：常规的解法是创建一个相同长度的新数组，然后把第一个数组中的元素，按倒序放入新数组中。但更简单的方法是：利用 Python 交换变量的特性，不需要新建数组，直接在原数组里即可完成\n1234567def reverse(self, chars):    if chars:        size = len(chars)        for i in range(size // 2):            chars[i], chars[size - 1 - i] = chars[size - 1 - i], chars[i]        return charsprint(reverse(&#x27;h&#x27;,&#x27;a&#x27;))\n5.栈：也称下压栈，堆栈，是仅允许在表尾进行插入和删除操作的线性表,特点：先进后出   后进先出\n1234567891011121314151617181920212223242526272829303132333435363738class Stack(object):    def __init__(self):        &quot;&quot;&quot;初始化&quot;&quot;&quot;        self.stack = []    def push(self,item):        &quot;&quot;&quot;push(item)添加一个新的元素item到栈顶&quot;&quot;&quot;        self.stack.append(item)    def pop(self):        &quot;&quot;&quot;pop()弹出栈顶元素&quot;&quot;&quot;        if self.stack == []:            return None        else:            self.stack.pop()    def peek(self):        &quot;&quot;&quot;peek()返回栈顶元素&quot;&quot;&quot;        if self.stack == []:            return None        else:            return self.stack[-1]    def isEmpty(self):        &quot;&quot;&quot;is_empty()判断栈是否为空&quot;&quot;&quot;        return self.stack == []    def size(self):        &quot;&quot;&quot;size()返回栈的元素个数&quot;&quot;&quot;        return len(self.stack)if __name__ == &#x27;__main__&#x27;:    stack = Stack()    stack.push(1)    stack.pop()    print(stack.peek())    print(stack.isEmpty())    print(stack.size())\n6.队列：是只允许在一端进行插入操作，而在另一端进行删除操作的线性表。特点：先进先出   后进后出\n123456789101112131415161718192021222324252627282930313233class Queue(object):    def __init__(self):        &quot;&quot;&quot;实例属性的初始化和赋值创建一个空的队列&quot;&quot;&quot;        self.queue = []    def enqueue(self,item):        &quot;&quot;&quot;往队列中添加一个item元素&quot;&quot;&quot;        self.queue.append(item)    def is_empty(self):        &quot;&quot;&quot;判断一个队列是否为空&quot;&quot;&quot;        return self.queue==[]    def dequeue(self):        &quot;&quot;&quot;从队列头部删除一个元素&quot;&quot;&quot;        if self.queue==[]:            return None        else:            return  self.queue.pop(0)    def size(self):        &quot;&quot;&quot;返回队列大小&quot;&quot;&quot;        return  len(self.queue)if __name__ == &#x27;__main__&#x27;:    q=Queue()    q.enqueue(1)    q.enqueue(2)    q.enqueue(3)    q.enqueue(4)    q.enqueue(5)    print(q.is_empty())    print(&quot;长度为：&quot;,q.size())    print(q.dequeue())\n\n7.去掉空格1234567891011121314def get_num(par_str):    &quot;&quot;&quot;    :param par_str: 字符串    :return: num    &quot;&quot;&quot;    par_list = par_str.split(&#x27; &#x27;)    print([i for i in par_list if i])    for i in par_list:        if &quot;&quot; in par_list:            par_list.remove(&quot;&quot;)    print(par_list)if __name__ == &#x27;__main__&#x27;:    get_num(&#x27;hello, python   hello ,   world&#x27;)\n8.两数之和 方法1： 12345678910111213 def get_num_index(llist,target):        &#x27;&#x27;&#x27;        :param target: 俩个下标值的和        :param llist: 查看下标所用的列表        :return: 返回符合条件的两个下标值        &#x27;&#x27;&#x27;        y = 0        for x,val in enumerate(llist):            y += 1            if llist[x] +llist[y]==target:                return (x,y)if __name__ == &#x27;__main__&#x27;:    print(get_num_index([1,2,3,4,5,6],7))\n方法2：\n12345678910111213141516171819def get_num_index(llist, target):            &#x27;&#x27;&#x27;            :param target: 给定值            :param llist: 查看条件列表            :return: 返回符合条件的下标值            &#x27;&#x27;&#x27;            for i in llist:                y = target - i                if y != 0:                    if y in llist:                        if llist.index(i) == llist.index(y):                            break                        # if llist.index(i) &lt;= llist.index(y):                        return llist.index(y), llist.index(i)                else:                    return llist.index(i)if __name__ == &#x27;__main__&#x27;:    print(get_num_index([1, 2, 3, 4, 5, 6,13],13))\n9.比较：取值第三个大的数1234567891011121314def func(num):    if len(num)&gt;=3:        alist=[]        for i in sorted(num):            try:                alist.index(i)            except:                alist.append(i)        return alist[-3]    else:        return -1if __name__ == &#x27;__main__&#x27;:    num=[8,7,4,2,1,6,2,1,1,8]    print(func(num))\n10.反转字符数组12345678# 反转字符串def reverseString(s):    s[0::] = s[::-1]    print(s)if __name__ == &quot;__main__&quot;:    reverseString([&#x27;b&#x27;, &#x27;&#x27;, &#x27;a&#x27;, &#x27;r&#x27;])","categories":["python"],"tags":["python"]},{"title":"nginx简介","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/02/23/nginx简介/","content":"1. nginx 功能介绍Nginx因为它的稳定性、丰富的模块库、灵活的配置和低系统资源的消耗而闻名．业界一致认为它是Apache2.2＋mod_proxy_balancer的轻量级代替者，不仅是因为响应静态页面的速度非常快，而且它的模块数量达到Apache的近2/3。对proxy和rewrite模块的支持很彻底，还支持mod_fcgi、ssl、vhosts ，适合用来做mongrel clusters的前端HTTP响应。nginx和Apache一样使用模块化设计，nginx模块包括内置模块和第三方模块，其中内置模块中包含主模块和事件模块。\nnginx处理请求逻辑图\n2. nginx可以提供的服务1231. web 服务.2. 负载均衡 （反向代理）3. web cache（web 缓存）\n3. nginx 的优点12345671. 高并发。静态小文件2. 占用资源少。2万并发、10个线程，内存消耗几百M。3. 功能种类比较多。web,cache,proxy。每一个功能都不是特别强。4. 支持epoll模型，使得nginx可以支持高并发。5. nginx 配合动态服务和Apache有区别。（FASTCGI 接口）6. 利用nginx可以对IP限速，可以限制连接数。7. 配置简单，更灵活。\n\n4. nginx应用场合12341. 静态服务器。（图片，视频服务）另一个lighttpd。并发几万，html，js，css，flv，jpg，gif等。2. 动态服务，nginx——fastcgi 的方式运行PHP，jsp。（PHP并发在500-1500，MySQL 并发在300-1500）。3. 反向代理，负载均衡。日pv2000W以下，都可以直接用nginx做代理。4. 缓存服务。类似 SQUID,VARNISH。\n5. 主流web服务产品对比说明5.1 Apache-特性1234561. 2.2版本本身稳定强大，据官方说：其2.4版本性能更强。2. prefork模式取消了进程创建开销，性能很高。3. 处理动态业务数据时，因关联到后端的引擎和数据库，瓶颈不在与Apache本身。4. 高并发时消耗系统资源相对多一些。5. 基于传统的select模型。6. 扩展库，DSO方法。\n5.2 nginx-特性123456781. 基于异步IO模型，（epoll，kqueue），性能强，能够支持上万并发。2. 对小文件支持很好，性能很高（限静态小文件1M）。3. 代码优美，扩展库必须编译进主程序。4. 消耗代码资源比较低。5.  ighttpd（百度贴吧，豆瓣）6. 基于异步IO模式，性能和nginx相近。7. 扩展库是SO模式，比nginx要灵活。8. 通过差距（mod_secdownload）可实现文件URL地址加密。\n5.3 web服务产品性能对比测试5.3.1 静态数据性能对比1231. 处理静态文件Apache性能比nginx和lighttpd要差。2. nginx在处理小文件优势明显。3. 处理静态小文件（小于1M），nginx和lighttpd比Apache更有优势，lighttpd最强。\n5.3.2 动态数据性能对比1231. 处理动态内容三者相差不大，主要取决于PHP和数据库的压力。2. 当处理动态数据时，三者差距不大，从测试结果看，Apache更有优势一点。这是因为处理动态数据能力取决于PHP和后端数据的提供服务能力。也就是说瓶颈不在web服务器上。3. 一般PHP引擎支持的并发参考值300-1000，JAVA引擎并发300-1000，数据库的并发300-1000.\n5.3.3 为什么nginx的总体性能比Apache高。121. nginx使用最新的epoll和kqueue网络IO模型，而Apache使用床头的select模式。2. 目前Linux下能够承受高并发访问的squid、Memcached 都采用的是epoll网络IO模型。\n\n5.3.4 如何选择WEB服务器1234567静态业务：高并发、采用nginx，lighttpd，根据自己的掌握程度或公司的要求。动态业务：采用nginx和Apache均可。既有静态业务又有动态业务：nginx或Apache，不要多选要单选。动态业务可以由前端代理（haproxy）,根据页面元素的类型，向后转发相应的服务器进行处理。思想：我们工作都不要追求一步到位，满足需求的前提下，先用，然后逐步完善。提示：nginx做web（Apache，lighttpd）、反向代理（haproxy,lvs,nat）及缓存服务器（squid）也是不错的。最终建议：对外的业务nginx，对内的业务Apache（yum httpd mysql-server php）。\n\n6. nginx实战过程6.1 安装依赖包\nnginx安装依赖GCC、openssl-devel、pcre-devel和zlib-devel软件库。\nPcre全称（Perl Compatible Regular Expressions），中文perl兼容正则表达式，pcre官方站点。12yum install  pcre pcre-devel -y yum install openssl openssl-devel -y \n\n\n\n6.2 开始编译使用**./configure –help查看各个模块的使用情况，使用–without-http_ssi_module的方式关闭不需要的模块。可以使用–with-http_perl_modules**方式安装需要的模块。\n6.2.1 编译命令1234567tar -zxf nginx-1.10.1.tar.gz cd nginx-1.10.1/./configure --prefix=/data/nginx-1.10.1 --user=nginx --group=nginx  --with-http_ssl_module  --with-http_stub_status_moduleuseradd nginx -M -s /sbin/nologin make &amp;&amp; make install ln -s /data/nginx-1.10.1 /data/nginx\n6.2.2 测试nginx配置文件是否正常123/data/nginx/sbin/nginx -t nginx: the configuration file /data/nginx-1.10.1/conf/nginx.conf syntax is oknginx: configuration file /data/nginx-1.10.1/conf/nginx.conf test is successful\n6.2.3 启动nginx服务器1234/data/nginx/sbin/nginx  -t  ##检查配置文件/data/nginx/sbin/nginx      ##确定nginx服务netstat -lntup |grep nginx      ## 检查进程是否正常curl http://localhost           ## 确认结果\n6.2.4 nginx其他命令1234567nginx -s signalsignal：stop — fast shutdownquit — graceful shutdownreload — reloading the configuration filereopen — reopening the log files用来打开日志文件，这样nginx会把新日志信息写入这个新的文件中\n\n/data/nginx/sbin/nginx -V 查看已经编译的参数。\n使用kill命令操作nginx。格式：kill -信号 PID\n信号名称\n\nTERM,INT 快速关闭\nQUIT 优雅的关闭，保持吸纳有的客户端连接\nHUP 重启应用新的配置文件\nUSR1 重新打开日志文件\nUSR2 升级程序\nWINCH 优雅的关闭工作进程\n\n例子：\n12kill -QUIT  &#96;cat &#x2F;data&#x2F;nginx&#x2F;nginx.pid&#96;kill -HUP &#96;cat &#x2F;data&#x2F;nginx&#x2F;nginx.pid&#96;\n\n7. nginx配置文件配置基础配置文件\n12345678910111213141516171819202122232425262728worker_processes  1;events &#123;    worker_connections  1024;&#125;http &#123;    include       mime.types;    default_type  application&#x2F;octet-stream;    sendfile        on;    keepalive_timeout  65;    server &#123;        listen       80;        server_name  localhost;        location &#x2F; &#123;            root   html;            index  index.html index.htm;        &#125;        error_page   500 502 503 504  &#x2F;50x.html;        location &#x3D; &#x2F;50x.html &#123;            root   html;        &#125;    &#125;&#125;### 测试配置文件是否正常shell&gt; &#x2F;data&#x2F;nginx&#x2F;sbin&#x2F;nginx -t nginx: the configuration file &#x2F;data&#x2F;nginx-1.10.3&#x2F;conf&#x2F;nginx.conf syntax is oknginx: configuration file &#x2F;data&#x2F;nginx-1.10.3&#x2F;conf&#x2F;nginx.conf test is successfulshell&gt; curl -I http:&#x2F;&#x2F;localhostHTTP&#x2F;1.1 200 OK\n8. nginx监控开启nginx的监控服务\n8.1 开启状态页1234567#设定查看Nginx状态的地址   location /status &#123;    stub_status on;   \t#表示开启stubStatus的工作状态统计功能。  access_log off;   \t#access_log off; 关闭access_log 日志记录功能。  #auth_basic &quot;status&quot;;   \t\t\t\t\t\t\t#auth_basic 是nginx的一种认证机制。  #auth_basic_user_file conf/htpasswd;\t#用来指定密码文件的位置。&#125;\n8.2 配置登录密码123yum install -y httpd-tools/usr/local/apache/bin/htpasswd -c /data/nginx/conf/htpasswd biglittleant New password:\n完成后会在/*data/nginx/conf/*目录下生成htpasswd文件。\n8.3 访问URL12345678910curl http://127.0.0.1/statusActive connections:  1server accepts handled requests 16 16 18Reading: 0 Writing: 1 Waiting: 0#active connections – 活跃的连接数量#server accepts handled requests — 总共处理了16个连接 , 成功创建16次握手, 总共处理了18个请求#Reading — 读取客户端的连接数: Writing 响应数据到客户端的数量; Waiting 开启 keep-alive 的情况下,这个值等于 active – (reading+writing), 意思就是 Nginx 已经处理完正在等候下一次请求指令的驻留连接.\n8.4 编写zabbix监控脚本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647nginx_status_fun()&#123;    NGINX_PORT=$1    NGINX_COMMAND=$2    nginx_active()&#123;        /usr/bin/curl &quot;http://127.0.0.1:&quot;$NGINX_PORT&quot;/status/&quot; 2&gt;/dev/null| grep &#x27;Active&#x27; | awk &#x27;&#123;print $NF&#125;&#x27;        &#125;    nginx_reading()&#123;        /usr/bin/curl &quot;http://127.0.0.1:&quot;$NGINX_PORT&quot;/status/&quot; 2&gt;/dev/null| grep &#x27;Reading&#x27; | awk &#x27;&#123;print $2&#125;&#x27;       &#125;    nginx_writing()&#123;        /usr/bin/curl &quot;http://127.0.0.1:&quot;$NGINX_PORT&quot;/status/&quot; 2&gt;/dev/null| grep &#x27;Writing&#x27; | awk &#x27;&#123;print $4&#125;&#x27;       &#125;    nginx_waiting()&#123;        /usr/bin/curl &quot;http://127.0.0.1:&quot;$NGINX_PORT&quot;/status/&quot; 2&gt;/dev/null| grep &#x27;Waiting&#x27; | awk &#x27;&#123;print $6&#125;&#x27;       &#125;    nginx_accepts()&#123;        /usr/bin/curl &quot;http://127.0.0.1:&quot;$NGINX_PORT&quot;/status/&quot; 2&gt;/dev/null| awk NR==3 | awk &#x27;&#123;print $1&#125;&#x27;       &#125;    nginx_handled()&#123;        /usr/bin/curl &quot;http://127.0.0.1:&quot;$NGINX_PORT&quot;/status/&quot; 2&gt;/dev/null| awk NR==3 | awk &#x27;&#123;print $2&#125;&#x27;       &#125;    nginx_requests()&#123;        /usr/bin/curl &quot;http://127.0.0.1:&quot;$NGINX_PORT&quot;/status/&quot; 2&gt;/dev/null| awk NR==3 | awk &#x27;&#123;print $3&#125;&#x27;       &#125;    case $NGINX_COMMAND in        active)            nginx_active;            ;;        reading)            nginx_reading;            ;;        writing)            nginx_writing;            ;;        waiting)            nginx_waiting;            ;;        accepts)            nginx_accepts;            ;;        handled)            nginx_handled;            ;;        requests)            nginx_requests;        esac &#125;\n9. nginx优化9.1 nginx内核优化12345678910111213141516171819202122net.ipv4.tcp_fin_timeout = 2net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_syncookies = 1net.ipv4.tcp_keepalive_time = 600net.ipv4.ip_local_port_range = 4000    65000net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.tcp_max_tw_buckets = 36000net.ipv4.route.gc_timeout = 100net.ipv4.tcp_syn_retries = 1net.ipv4.tcp_synack_retries = 1net.core.somaxconn = 16384net.core.netdev_max_backlog = 16384net.ipv4.tcp_max_orphans = 16384#以下参数是对iptables防火墙的优化，防火墙不开会提示，可以忽略不理。net.ipv4.ip_conntrack_max = 25000000net.ipv4.netfilter.ip_conntrack_max=25000000net.ipv4.netfilter.ip_conntrack_tcp_timeout_established=180net.ipv4.netfilter.ip_conntrack_tcp_timeout_time_wait=120net.ipv4.netfilter.ip_conntrack_tcp_timeout_close_wait=60net.ipv4.netfilter.ip_conntrack_tcp_timeout_fin_wait=120\n10. 扩展一：10.1  nginx全局变量123456789101112131415161718192021222324$args：这个变量等于请求行中的参数，同$query_string。$is_args: 如果已经设置$args，则该变量的值为&quot;?&quot;，否则为&quot;&quot;。$content_length： 请求头中的Content-length字段。$content_type： 请求头中的Content-Type字段。$document_uri： 与$uri相同。$document_root： 当前请求在root指令中指定的值。$host： 请求主机头字段，否则为服务器名称。$http_user_agent： 客户端agent信息。$http_cookie： 客户端cookie信息。$limit_rate： 这个变量可以限制连接速率。$request_method： 客户端请求的动作，通常为GET或POST。$remote_addr： 客户端的IP地址。$remote_port： 客户端的端口。$remote_user： 已经经过Auth Basic Module验证的用户名。$request_body_file&#96;: 客户端请求主体的临时文件名。$request_uri: 请求的URI，带参数$request_filename： 当前请求的文件路径，由root或alias指令与URI请求生成。$scheme： 所用的协议，比如http或者是https，比如rewrite ^(.+)$ $scheme:&#x2F;&#x2F;example.com$1 redirect;。$server_protocol： 请求使用的协议，通常是HTTP&#x2F;1.0或HTTP&#x2F;1.1。$server_addr： 服务器地址，在完成一次系统调用后可以确定这个值。$server_name： 服务器名称。$server_port： 请求到达服务器的端口号。$request_uri： 包含请求参数的原始URI，不包含主机名，如：&#x2F;foo&#x2F;bar.php?arg&#x3D;baz，它无法修改。$uri： 不带请求参数的当前URI，$uri不包含主机名，如&#x2F;foo&#x2F;bar.html可能和最初的值有不同，比如经过重定向之类的。它可以通过内部重定向，或者使用index指令进行修改。不包括协议和主机名，例如&#x2F;foo&#x2F;bar.html。\n\n例子：\n12345678910访问链接是：http://localhost:88/test1/test.php 网站路径是：/var/www/html$host：localhost$server_port：88$request_uri：http://localhost:88/test1/test.php$document_uri：/test1/test.php$document_root：/var/www/html$request_filename：/var/www/html/test1/test.php\nnginx plus – ngx_http_status_module\n商业版的 nginx plus 通过他的 ngx_http_status_module 提供了比 nginx 更多的监控指标，可以参看 http://demo.nginx.com/status.html\nnginx access log 分析nginx 的 access log 中可以记录很多有价值的信息，通过分析 access log，可以收集到很多指标。python 编写的 linux 工具 ngxtop 就实现了对 access log 的分析功能。\nNDK – ngx_devel_kitNDK 是一个拓展nginx服务器核心功能的模块，第三方模块开发可以基于它来快速实现。NDK提供函数和宏处理一些基本任务，减轻第三方模块开发的代码量。\nnginx lua – lua-nginx-modulenginx的lua模块，通过这个模块，可以对nginx做定制开发\n10.2  web服务器事件处理模型selectselect最早于1983年出现在4.2BSD中，它通过一个select()系统调用来监视多个文件描述符的数组，当select()返回后，该数组中就绪的文件描述符便会被内核修改标志位，使得进程可以获得这些文件描述符从而进行后续的读写操作。select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点，事实上从现在看来，这也是它所剩不多的优点之一。select的一个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，不过可以通过修改宏定义甚至重新编译内核的方式提升这一限制。另外，select()所维护的存储大量文件描述符的数据结构，随着文件描述符数量的增大，其复制的开销也线性增长。同时，由于网络响应时间的延迟使得大量TCP连接处于非活跃状态，但调用select()会对所有socket进行一次线性扫描，所以这也浪费了一定的开销。\npollpoll在1986年诞生于System V Release 3，它和select在本质上没有多大差别，但是poll没有最大文件描述符数量的限制。poll和select同样存在一个缺点就是，包含大量文件描述符的数组被整体复制于用户态和内核的地址空间之间，而不论这些文件描述符是否就绪，它的开销随着文件描述符数量的增加而线性增大。另外，select()和poll()将就绪的文件描述符告诉进程后，如果进程没有对其进行IO操作，那么下次调用select()和poll()的时候将再次报告这些文件描述符，所以它们一般不会丢失就绪的消息，这种方式称为水平触发（Level Triggered）。\nepoll直到Linux2.6才出现了由内核直接支持的实现方法，那就是epoll，它几乎具备了之前所说的一切优点，被公认为Linux2.6下性能最好的多路I/O就绪通知方法。epoll可以同时支持水平触发和边缘触发（Edge Triggered，只告诉进程哪些文件描述符刚刚变为就绪状态，它只说一遍，如果我们没有采取行动，那么它将不会再次告知，这种方式称为边缘触发），理论上边缘触发的性能要更高一些，但是代码实现相当复杂。epoll同样只告知那些就绪的文件描述符，而且当我们调用epoll_wait()获得就绪文件描述符时，返回的不是实际的描述符，而是一个代表就绪描述符数量的值，你只需要去epoll指定的一个数组中依次取得相应数量的文件描述符即可，这里也使用了内存映射（mmap）技术，这样便彻底省掉了这些文件描述符在系统调用时复制的开销。另一个本质的改进在于epoll采用基于事件的就绪通知方式。在select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait()时便得到通知。\nnginx -s reload 过程nginx主进程读取配置文件，如果发现配置文件变更，会创建一个新的主进程，然后同时旧的进程，及旧的子进程关闭，旧进程会拒绝新的连接，服务到自己的连接结束，然后关闭。\nApache select模型和 nginx epoll 模型对比讲解\nNginx的高并发得益于其采用了epoll模型，与传统的服务器程序架构不同，epoll是linux内核2.6以后才出现的。下面通过比较Apache和Nginx工作原理来比较。\n传统Apache都是多进程或者多线程来工作，假设是多进程工作（prefork），apache会先生成几个进程，类似进程池的工作原理，只不过这里的进程池会随着请求数目的增加而增加。对于每一个连接，apache都是在一个进程内处理完毕。具体是 recv（），以及根据 URI 去进行磁盘I/O来寻找文件，还有 send（）都是阻塞的。其实说白了都是 apche 对于套接字的I/O，读或者写，但是读或者写都是阻塞的，阻塞意味着进程就得挂起进入sleep状态，那么一旦连接数很多，Apache必然要生成更多的进程来响应请求，一旦进程多了，CPU对于进程的切换就频繁了，很耗资源和时间，所以就导致apache性能下降了，说白了就是处理不过来这么多进程了。其实仔细想想，如果对于进程每个请求都没有阻塞，那么效率肯定会提高很多。\nNginx采用epoll模型，异步非阻塞。对于Nginx来说，把一个完整的连接请求处理都划分成了事件，一个一个的事件。比如accept（）， recv（），磁盘I/O，send（）等，每部分都有相应的模块去处理，一个完整的请求可能是由几百个模块去处理。真正核心的就是事件收集和分发模块，这就是管理所有模块的核心。只有核心模块的调度才能让对应的模块占用CPU资源，从而处理请求。拿一个HTTP请求来说，首先在事件收集分发模块注册感兴趣的监听事件，注册好之后不阻塞直接返回，接下来就不需要再管了，等待有连接来了内核会通知你(epoll的轮询会告诉进程)，cpu就可以处理其他事情去了。一旦有请求来，那么对整个请求分配相应的上下文（其实已经预先分配好），这时候再注册新的感兴趣的事件(read函数)，同样客户端数据来了内核会自动通知进程可以去读数据了，读了数据之后就是解析，解析完后去磁盘找资源（I/O），一旦I/O完成会通知进程，进程开始给客户端发回数据send()，这时候也不是阻塞的，调用后就等内核发回通知发送的结果就行。整个下来把一个请求分成了很多个阶段，每个阶段都到很多模块去注册，然后处理，都是异步非阻塞。异步这里指的就是做一个事情，不需要等返回结果，做好了会自动通知你。\nselect/epoll的特点\nselect的特点：select 选择句柄的时候，是遍历所有句柄，也就是说句柄有事件响应时，select需要遍历所有句柄才能获取到哪些句柄有事件通知，因此效率是非常低。但是如果连接很少的情况下， select和epoll的LT触发模式相比， 性能上差别不大。这里要多说一句，select支持的句柄数是有限制的， 同时只支持1024个，这个是句柄集合限制的，如果超过这个限制，很可能导致溢出，而且非常不容易发现问题， 当然可以通过修改linux的socket内核调整这个参数。epoll的特点：epoll对于句柄事件的选择不是遍历的，是事件响应的，就是句柄上事件来就马上选择出来，不需要遍历整个句柄链表，因此效率非常高，内核将句柄用红黑树保存的。对于epoll而言还有ET和LT的区别，LT表示水平触发，ET表示边缘触发，两者在性能以及代码实现上差别也是非常大的。\n不管是Nginx还是Squid这种反向代理，其网络模式都是事件驱动。事件驱动其实是很老的技术，早期的select、poll都是如此。后来基于内核通知的更高级事件机制出现，如libevent里的epoll，使事件驱动性能得以提高。事件驱动的本质还是IO事件，应用程序在多个IO句柄间快速切换，实现所谓的异步IO。事件驱动服务器，最适合做的就是这种IO密集型工作，如反向代理，它在客户端与WEB服务器之间起一个数据中转作用，纯粹是IO操作，自身并不涉及到复杂计算。反向代理用事件驱动来做，显然更好，一个工作进程就可以run了，没有进程、线程管理的开销，CPU、内存消耗都小。\n所以Nginx、Squid都是这样做的。当然，Nginx也可以是多进程 + 事件驱动的模式，几个进程跑libevent，不需要Apache那样动辄数百的进程数。Nginx处理静态文件效果也很好，那是因为静态文件本身也是磁盘IO操作，处理过程一样。至于说多少万的并发连接，这个毫无意义。随手写个网络程序都能处理几万的并发，但如果大部分客户端阻塞在那里，就没什么价值。\n再看看Apache或者Resin这类应用服务器，之所以称他们为应用服务器，是因为他们真的要跑具体的业务应用，如科学计算、图形图像、数据库读写等。它们很可能是CPU密集型的服务，事件驱动并不合适。例如一个计算耗时2秒，那么这2秒就是完全阻塞的，什么event都没用。想想MySQL如果改成事件驱动会怎么样，一个大型的join或sort就会阻塞住所有客户端。这个时候多进程或线程就体现出优势，每个进程各干各的事，互不阻塞和干扰。当然，现代CPU越来越快，单个计算阻塞的时间可能很小，但只要有阻塞，事件编程就毫无优势。所以进程、线程这类技术，并不会消失，而是与事件机制相辅相成，长期存在。\n总言之，事件驱动适合于IO密集型服务，多进程或线程适合于CPU密集型服务，它们各有各的优势，并不存在谁取代谁的倾向。\n","categories":["nginx"],"tags":["python"]},{"title":"Mysql锁","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/02/14/Mysql锁/","content":"锁是计算机协调多个进程或线程并发访问某一资源的机制。锁保证数据并发访问的一致性、有效性；锁冲突也是影响数据库并发访问性能的一个重要因素。锁是Mysql在服务器层和存储引擎层的的并发控制。\n加锁是消耗资源的，锁的各种操作，包括获得锁、检测锁是否是否已解除、释放锁等。\n锁机制共享锁与排他锁\n共享锁（读锁）：其他事务可以读，但不能写。\n\n排他锁（写锁） ：其他事务不能读取，也不能写。\n粒度锁MySQL 不同的存储引擎支持不同的锁机制，所有的存储引擎都以自己的方式显现了锁机制，服务器层完全不了解存储引擎中的锁实现：\n\nMyISAM 和 MEMORY 存储引擎采用的是表级锁（table-level locking）\n\nBDB 存储引擎采用的是页面锁（page-level locking），但也支持表级锁\n\nInnoDB 存储引擎既支持行级锁（row-level locking），也支持表级锁，但默认情况下是采用行级锁。\n\n\n默认情况下，表锁和行锁都是自动获得的， 不需要额外的命令。\n但是在有的情况下， 用户需要明确地进行锁表或者进行事务的控制， 以便确保整个事务的完整性，这样就需要使用事务控制和锁定语句来完成。\n不同粒度锁的比较：\n\n表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。\n这些存储引擎通过总是一次性同时获取所有需要的锁以及总是按相同的顺序获取表锁来避免死锁。\n表级锁更适合于以查询为主，并发用户少，只有少量按索引条件更新数据的应用，如Web 应用\n\n\n行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。\n最大程度的支持并发，同时也带来了最大的锁开销。\n在 InnoDB 中，除单个 SQL 组成的事务外，锁是逐步获得的，这就决定了在 InnoDB 中发生死锁是可能的。\n行级锁只在存储引擎层实现，而Mysql服务器层没有实现。 行级锁更适合于有大量按索引条件并发更新少量不同数据，同时又有并发查询的应用，如一些在线事务处理（OLTP）系统\n\n\n页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。MyISAM 表锁MyISAM表级锁模式：\n表共享读锁 （Table Read Lock）：不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求；\n表独占写锁 （Table Write Lock）：会阻塞其他用户对同一表的读和写操作；\n\nMyISAM 表的读操作与写操作之间，以及写操作之间是串行的。当一个线程获得对一个表的写锁后， 只有持有锁的线程可以对表进行更新操作。 其他线程的读、 写操作都会等待，直到锁被释放为止。\n默认情况下，写锁比读锁具有更高的优先级：当一个锁释放时，这个锁会优先给写锁队列中等候的获取锁请求，然后再给读锁队列中等候的获取锁请求。 （This ensures that updates to a table are not “starved” even when there is heavy SELECT activity for the table. However, if there are many updates for a table, SELECT statements wait until there are no more updates.）。\n这也正是 MyISAM 表不太适合于有大量更新操作和查询操作应用的原因，因为，大量的更新操作会造成查询操作很难获得读锁，从而可能永远阻塞。同时，一些需要长时间运行的查询操作，也会使写线程“饿死” ，应用中应尽量避免出现长时间运行的查询操作（在可能的情况下可以通过使用中间表等措施对SQL语句做一定的“分解” ，使每一步查询都能在较短时间完成，从而减少锁冲突。如果复杂查询不可避免，应尽量安排在数据库空闲时段执行，比如一些定期统计可以安排在夜间执行）。\n可以设置改变读锁和写锁的优先级：\n\n通过指定启动参数low-priority-updates，使MyISAM引擎默认给予读请求以优先的权利。\n通过执行命令SET LOW_PRIORITY_UPDATES=1，使该连接发出的更新请求优先级降低。\n通过指定INSERT、UPDATE、DELETE语句的LOW_PRIORITY属性，降低该语句的优先级。\n给系统参数max_write_lock_count设置一个合适的值，当一个表的读锁达到这个值后，MySQL就暂时将写请求的优先级降低，给读进程一定获得锁的机会。MyISAM加表锁方法：MyISAM 在执行查询语句（SELECT）前，会自动给涉及的表加读锁，在执行更新操作（UPDATE、DELETE、INSERT 等）前，会自动给涉及的表加写锁，这个过程并不需要用户干预，因此，用户一般不需要直接用 LOCK TABLE 命令给 MyISAM 表显式加锁。\n\n在自动加锁的情况下，MyISAM 总是一次获得 SQL 语句所需要的全部锁，这也正是 MyISAM 表不会出现死锁（Deadlock Free）的原因。\nMyISAM存储引擎支持并发插入，以减少给定表的读和写操作之间的争用：\n如果MyISAM表在数据文件中间没有空闲块，则行始终插入数据文件的末尾。 在这种情况下，你可以自由混合并发使用MyISAM表的INSERT和SELECT语句而不需要加锁——你可以在其他线程进行读操作的时候，同时将行插入到MyISAM表中。 文件中间的空闲块可能是从表格中间删除或更新的行而产生的。 如果文件中间有空闲快，则并发插入会被禁用，但是当所有空闲块都填充有新数据时，它又会自动重新启用。 要控制此行为，可以使用MySQL的concurrent_insert系统变量。\n如果你使用LOCK TABLES显式获取表锁，则可以请求READ LOCAL锁而不是READ锁，以便在锁定表时，其他会话可以使用并发插入。\n\n当concurrent_insert设置为0时，不允许并发插入。\n当concurrent_insert设置为1时，如果MyISAM表中没有空洞（即表的中间没有被删除的行），MyISAM允许在一个线程读表的同时，另一个线程从表尾插入记录。这也是MySQL的默认设置。\n当concurrent_insert设置为2时，无论MyISAM表中有没有空洞，都允许在表尾并发插入记录。\n\n查询表级锁争用情况：可以通过检查 table_locks_waited 和 table_locks_immediate 状态变量来分析系统上的表锁的争夺，如果 Table_locks_waited 的值比较高，则说明存在着较严重的表级锁争用情况：\n1234567mysql&gt; SHOW STATUS LIKE &#39;Table%&#39;;+-----------------------+---------+| Variable_name | Value |+-----------------------+---------+| Table_locks_immediate | 1151552 || Table_locks_waited | 15324 |+-----------------------+---------+\nInnoDB行级锁和表级锁InnoDB锁模式：InnoDB 实现了以下两种类型的行锁：\n\n共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。\n\n排他锁（X）：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB 还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁：\n\n意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的 IS 锁。\n\n意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的 IX 锁。\n\n\n锁模式的兼容情况：\n\n（如果一个事务请求的锁模式与当前的锁兼容， InnoDB 就将请求的锁授予该事务； 反之， 如果两者不兼容，该事务就要等待锁释放。）\nInnoDB加锁方法：\n意向锁是 InnoDB 自动加的， 不需用户干预。\n对于 UPDATE、 DELETE 和 INSERT 语句， InnoDB会自动给涉及数据集加排他锁（X)；\n对于普通 SELECT 语句，InnoDB 不会加任何锁；事务可以通过以下语句显式给记录集加共享锁或排他锁：\n共享锁（S）：SELECT * FROM table_name WHERE … LOCK IN SHARE MODE。 其他 session 仍然可以查询记录，并也可以对该记录加 share mode 的共享锁。但是如果当前事务需要对该记录进行更新操作，则很有可能造成死锁。\n排他锁（X)：SELECT * FROM table_name WHERE … FOR UPDATE。其他 session 可以查询该记录，但是不能对该记录加共享锁或排他锁，而是等待获得锁\n\n\n\n\n隐式锁定：InnoDB在事务执行过程中，使用两阶段锁协议：\n\n随时都可以执行锁定，InnoDB会根据隔离级别在需要的时候自动加锁；\n锁只有在执行commit或者rollback的时候才会释放，并且所有的锁都是在同一时刻被释放。\n\n显式锁定 ：12select ... lock in share mode &#x2F;&#x2F;共享锁 select ... for update &#x2F;&#x2F;排他锁 \n\n\n\nselect for update：\n在执行这个 select 查询语句的时候，会将对应的索引访问条目进行上排他锁（X 锁），也就是说这个语句对应的锁就相当于update带来的效果。\nselect *** for update 的使用场景：为了让自己查到的数据确保是最新数据，并且查到后的数据只允许自己来修改的时候，需要用到 for update 子句。\nselect lock in share mode ：in share mode 子句的作用就是将查找到的数据加上一个 share 锁，这个就是表示其他的事务只能对这些数据进行简单的select 操作，并不能够进行 DML 操作。select *** lock in share mode 使用场景：为了确保自己查到的数据没有被其他的事务正在修改，也就是说确保查到的数据是最新的数据，并且不允许其他人来修改数据。但是自己不一定能够修改数据，因为有可能其他的事务也对这些数据 使用了 in share mode 的方式上了 S 锁。\n性能影响：select for update 语句，相当于一个 update 语句。在业务繁忙的情况下，如果事务没有及时的commit或者rollback 可能会造成其他事务长时间的等待，从而影响数据库的并发使用效率。select lock in share mode 语句是一个给查找的数据上一个共享锁（S 锁）的功能，它允许其他的事务也对该数据上S锁，但是不能够允许对该数据进行修改。如果不及时的commit 或者rollback 也可能会造成大量的事务等待。\nfor update 和 lock in share mode 的区别：\n前一个上的是排他锁（X 锁），一旦一个事务获取了这个锁，其他的事务是没法在这些数据上执行 for update ；后一个是共享锁，多个事务可以同时的对相同数据执行 lock in share mode。\nInnoDB 行锁实现方式：\nInnoDB 行锁是通过给索引上的索引项加锁来实现的，这一点 MySQL 与 Oracle 不同，后者是通过在数据块中对相应数据行加锁来实现的。InnoDB 这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB 才使用行级锁，否则，InnoDB 将使用表锁！\n不论是使用主键索引、唯一索引或普通索引，InnoDB 都会使用行锁来对数据加锁。\n只有执行计划真正使用了索引，才能使用行锁：即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。因此，在分析锁冲突时，别忘了检查 SQL 的执行计划（可以通过 explain 检查 SQL 的执行计划），以确认是否真正使用了索引。（更多阅读：MySQL索引总结）\n由于 MySQL 的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然多个session是访问不同行的记录， 但是如果是使用相同的索引键， 是会出现锁冲突的（后使用这些索引的session需要等待先使用索引的session释放锁后，才能获取锁）。 应用设计的时候要注意这一点。InnoDB的间隙锁：当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁（Next-Key锁）。\n\n很显然，在使用范围条件检索并锁定记录时，InnoDB这种加锁机制会阻塞符合条件范围内键值的并发插入，这往往会造成严重的锁等待。因此，在实际应用开发中，尤其是并发插入比较多的应用，我们要尽量优化业务逻辑，尽量使用相等条件来访问更新数据，避免使用范围条件。\nInnoDB使用间隙锁的目的：\n防止幻读，以满足相关隔离级别的要求；\n满足恢复和复制的需要：\n\nMySQL 通过 BINLOG 录入执行成功的 INSERT、UPDATE、DELETE 等更新数据的 SQL 语句，并由此实现 MySQL 数据库的恢复和主从复制。MySQL 的恢复机制（复制其实就是在 Slave Mysql 不断做基于 BINLOG 的恢复）有以下特点：\n一是 MySQL 的恢复是 SQL 语句级的，也就是重新执行 BINLOG 中的 SQL 语句。\n二是 MySQL 的 Binlog 是按照事务提交的先后顺序记录的， 恢复也是按这个顺序进行的。\n由此可见，MySQL 的恢复机制要求：在一个事务未提交前，其他并发事务不能插入满足其锁定条件的任何记录，也就是不允许出现幻读。\nInnoDB 在不同隔离级别下的一致性读及锁的差异：锁和多版本数据（MVCC）是 InnoDB 实现一致性读和 ISO/ANSI SQL92 隔离级别的手段。\n因此，在不同的隔离级别下，InnoDB 处理 SQL 时采用的一致性读策略和需要的锁是不同的：\n\n对于许多 SQL，隔离级别越高，InnoDB 给记录集加的锁就越严格（尤其是使用范围条件的时候），产生锁冲突的可能性也就越高，从而对并发性事务处理性能的 影响也就越大。\n因此， 我们在应用中， 应该尽量使用较低的隔离级别， 以减少锁争用的机率。实际上，通过优化事务逻辑，大部分应用使用 Read Commited 隔离级别就足够了。对于一些确实需要更高隔离级别的事务， 可以通过在程序中执行 SET SESSION TRANSACTION ISOLATION\nLEVEL REPEATABLE READ 或 SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE 动态改变隔离级别的方式满足需求。\n获取 InnoDB 行锁争用情况：可以通过检查 InnoDB_row_lock 状态变量来分析系统上的行锁的争夺情况：\n1234567891011mysql&gt; show status like &#39;innodb_row_lock%&#39;; +-------------------------------+-------+ | Variable_name | Value | +-------------------------------+-------+ | InnoDB_row_lock_current_waits | 0 | | InnoDB_row_lock_time | 0 | | InnoDB_row_lock_time_avg | 0 | | InnoDB_row_lock_time_max | 0 | | InnoDB_row_lock_waits | 0 | +-------------------------------+-------+ 5 rows in set (0.01 sec)\n\n\nLOCK TABLES 和 UNLOCK TABLESMysql也支持lock tables和unlock tables，这都是在服务器层（MySQL Server层）实现的，和存储引擎无关，它们有自己的用途，并不能替代事务处理。 （除了禁用了autocommint后可以使用，其他情况不建议使用）：\nLOCK TABLES 可以锁定用于当前线程的表。如果表被其他线程锁定，则当前线程会等待，直到可以获取所有锁定为止。UNLOCK TABLES 可以释放当前线程获得的任何锁定。当前线程执行另一个 LOCK TABLES 时，或当与服务器的连接被关闭时，所有由当前线程锁定的表被隐含地解锁\nLOCK TABLES语法：\n在用 LOCK TABLES 对 InnoDB 表加锁时要注意，要将 AUTOCOMMIT 设为 0，否则MySQL 不会给表加锁；\n事务结束前，不要用 UNLOCK TABLES 释放表锁，因为 UNLOCK TABLES会隐含地提交事务；\nCOMMIT 或 ROLLBACK 并不能释放用 LOCK TABLES 加的表级锁，必须用UNLOCK TABLES 释放表锁。正确的方式见如下语句：例如，如果需要写表 t1 并从表 t 读，可以按如下做：12345SET AUTOCOMMIT&#x3D;0; LOCK TABLES t1 WRITE, t2 READ, ...; [do something with tables t1 and t2 here]; COMMIT; UNLOCK TABLES;\n\n\n\n\n使用LOCK TABLES的场景：给表显示加表级锁（InnoDB表和MyISAM都可以），一般是为了在一定程度模拟事务操作，实现对某一时间点多个表的一致性读取。（与MyISAM默认的表锁行为类似）\n在用 LOCK TABLES 给表显式加表锁时，必须同时取得所有涉及到表的锁，并且 MySQL 不支持锁升级。也就是说，在执行 LOCK TABLES 后，只能访问显式加锁的这些表，不能访问未加锁的表；同时，如果加的是读锁，那么只能执行查询操作，而不能执行更新操作。\n其实，在MyISAM自动加锁（表锁）的情况下也大致如此，MyISAM 总是一次获得 SQL 语句所需要的全部锁，这也正是 MyISAM 表不会出现死锁（Deadlock Free）的原因。\n例如，有一个订单表 orders，其中记录有各订单的总金额 total，同时还有一个 订单明细表 order_detail，其中记录有各订单每一产品的金额小计 subtotal，假设我们需要检 查这两个表的金额合计是否相符，可能就需要执行如下两条 SQL：\n12Select sum(total) from orders; Select sum(subtotal) from order_detail; \n这时，如果不先给两个表加锁，就可能产生错误的结果，因为第一条语句执行过程中，order_detail 表可能已经发生了改变。因此，正确的方法应该是：\n1234Lock tables orders read local, order_detail read local; Select sum(total) from orders; Select sum(subtotal) from order_detail; Unlock tables;\n（在 LOCK TABLES 时加了“local”选项，其作用就是允许当你持有表的读锁时，其他用户可以在满足 MyISAM 表并发插入条件的情况下，在表尾并发插入记录（MyISAM 存储引擎支持“并发插入”））\n死锁（Deadlock Free）\n死锁产生：\n\n死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方占用的资源，从而导致恶性循环。\n当事务试图以不同的顺序锁定资源时，就可能产生死锁。多个事务同时锁定同一个资源时也可能会产生死锁。\n锁的行为和顺序和存储引擎相关。以同样的顺序执行语句，有些存储引擎会产生死锁有些不会——死锁有双重原因：真正的数据冲突；存储引擎的实现方式。\n\n\n检测死锁：数据库系统实现了各种死锁检测和死锁超时的机制。InnoDB存储引擎能检测到死锁的循环依赖并立即返回一个错误。\n\n死锁恢复：死锁发生以后，只有部分或完全回滚其中一个事务，才能打破死锁，InnoDB目前处理死锁的方法是，将持有最少行级排他锁的事务进行回滚。所以事务型应用程序在设计时必须考虑如何处理死锁，多数情况下只需要重新执行因死锁回滚的事务即可。\n\n外部锁的死锁检测：发生死锁后，InnoDB 一般都能自动检测到，并使一个事务释放锁并回退，另一个事务获得锁，继续完成事务。但在涉及外部锁，或涉及表锁的情况下，InnoDB 并不能完全自动检测到死锁， 这需要通过设置锁等待超时参数 innodb_lock_wait_timeout 来解决\n\n死锁影响性能：死锁会影响性能而不是会产生严重错误，因为InnoDB会自动检测死锁状况并回滚其中一个受影响的事务。在高并发系统上，当许多线程等待同一个锁时，死锁检测可能导致速度变慢。 有时当发生死锁时，禁用死锁检测（使用innodb_deadlock_detect配置选项）可能会更有效，这时可以依赖innodb_lock_wait_timeout设置进行事务回滚。\nMyISAM避免死锁：\n在自动加锁的情况下，MyISAM 总是一次获得 SQL 语句所需要的全部锁，所以 MyISAM 表不会出现死锁。\nInnoDB避免死锁：\n为了在单个InnoDB表上执行多个并发写入操作时避免死锁，可以在事务开始时通过为预期要修改的每个元祖（行）使用SELECT … FOR UPDATE语句来获取必要的锁，即使这些行的更改语句是在之后才执行的。\n\n在事务中，如果要更新记录，应该直接申请足够级别的锁，即排他锁，而不应先申请共享锁、更新时再申请排他锁，因为这时候当用户再申请排他锁时，其他事务可能又已经获得了相同记录的共享锁，从而造成锁冲突，甚至死锁\n\n如果事务需要修改或锁定多个表，则应在每个事务中以相同的顺序使用加锁语句。 在应用中，如果不同的程序会并发存取多个表，应尽量约定以相同的顺序来访问表，这样可以大大降低产生死锁的机会\n\n通过SELECT … LOCK IN SHARE MODE获取行的读锁后，如果当前事务再需要对该记录进行更新操作，则很有可能造成死锁。\n\n改变事务隔离级别\n\n\n如果出现死锁，可以用 SHOW INNODB STATUS 命令来确定最后一个死锁产生的原因。返回结果中包括死锁相关事务的详细信息，如引发死锁的 SQL 语句，事务已经获得的锁，正在等待什么锁，以及被回滚的事务等。据此可以分析死锁产生的原因和改进措施。\n一些优化锁性能的建议\n尽量使用较低的隔离级别；\n精心设计索引， 并尽量使用索引访问数据， 使加锁更精确， 从而减少锁冲突的机会\n选择合理的事务大小，小事务发生锁冲突的几率也更小\n给记录集显示加锁时，最好一次性请求足够级别的锁。比如要修改数据的话，最好直接申请排他锁，而不是先申请共享锁，修改时再请求排他锁，这样容易产生死锁\n不同的程序访问一组表时，应尽量约定以相同的顺序访问各表，对一个表而言，尽可能以固定的顺序存取表中的行。这样可以大大减少死锁的机会\n尽量用相等条件访问数据，这样可以避免间隙锁对并发插入的影响\n不要申请超过实际需要的锁级别\n除非必须，查询时不要显示加锁。 MySQL的MVCC可以实现事务中的查询不用加锁，优化事务性能；MVCC只在COMMITTED READ（读提交）和REPEATABLE READ（可重复读）两种隔离级别下工作\n对于一些特定的事务，可以使用表锁来提高处理速度或减少死锁的可能乐观锁、悲观锁\n\n\n**乐观锁(Optimistic Lock)**：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。 乐观锁不能解决脏读的问题。乐观锁, 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库如果提供类似于write_condition机制的其实都是提供的乐观锁。\n\n**悲观锁(Pessimistic Lock)**：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。悲观锁，顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。\n\n\n","categories":["mysql"],"tags":["python"]},{"title":"Mysql事务","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/02/08/Mysql事务/","content":"什么是事务\n简单来说就是：做一件事件必须有有头有尾，一旦开始，只有两种结果，要么失败，要么成功，而不能出现成功了部分失败了部分。\n专业点说就是：一个事务必须具备 ACID属性，所谓得 ACID即： 1. Atomicity–原子性: 事务必须是原子工作单元；对于其数据修改，要么全都执行，要么全都不执行。 2. Consistency–一致性 事务进行过后和执行前，所有数据都是预期一致的（一致性的理解有很多，大概是这样子的） 3. Isolation–隔离性 并发得事务之间不会互相影响， 4. Durability–持久性 一个事务成功了，那么他的的改变应该是永久性的。为什么要事务这个其实用脚趾头想想也知道它的重要性，举个简单的例子：一个用户提交了一个订单，那么这条数据包含了两个信息，用户信息 和购买的 商品信息，我需要把他们分别存到 用户表 和 商品表，如果不采用事务，可能会出现，商品信息插入成功，而用户信息没有，这时候就会出现无主商品了，用户付了钱，却得不到商品，这就尴尬了，而如果采用事务，就可以保证，用户信息 和 商品信息 都必须插入成功，该次事务才算成功，那就不会出现这种问题了\n\n支持事务的引擎有时候事务并不一定是必须的，为了提升查询效率，有些mysql引擎是不支持事务的，比如 MyISAM，当然目前新版（好像是5.6以后）默认的引擎是 innoDB 是支持事务的，一般为了提升mysql速度，也会做读写分离，因为事务一般是针对写来说的。\n事务的使用我们主要从以下几个术语开始：\n\n事务（transaction）执行一组SQL语句；start transaction即可开始一个事务\n\n回退（rollback）撤销指定SQL语句的过程；rollback即可回退一个事务：rollback 必须是在一个事务里面才能使用，没有事务，就不能谈回退\n\n提交（commit）将未存储的SQL语句结果写入数据库表；commit使用 commit 来结束一个事务的处理：一般使用事务，我们需要自己手动提交，mysql 默认是自动提交的，所以我们需要设置set autocommit = 0来更改提交模式。其中值得注意的是 rollback也会触发提交事务\n\n保留点（savepoint）指事务处理中设置的临时占位符（placeholder），然后你就可以回退到该点。savepoint s1生成一个保留点，然后可以通过 rollback to s1来回退到 s1 这个保留\n\n\n实例讲了这么多，不来一个实例操作一下，实在是不舒心。。。。下面你可以打开你的mysql 库，按照下面步骤来操作一下，加深下理解：\n1234567891011121314151617181920212223242526272829303132truncate info：先清除一下表,记得用测试表噢，数据没了可别赖我select * from info：查询一下，这里表应该是空的了start transaction：开启一个事务，正式我们的测试insert into info values(1,&quot;s1 before&quot;,20181019)：插入一条数据 “s1 before”savepoint s1：创建一个保留点insert into info values(1,&quot;s1 after&quot;,20181019)：再插入一条数据 “s1 after”select * from info：查询看一下数据，此时应该是可以看到我们上面插入的两条数据。但是你要知道，这两条数据是没有正式入库的，他们只存在你的这个session里面，因为我们的事务还没提交呢。。。不信？你重开一个客户端，查看一下这个表，还是空的噢。rollback to s1：回滚到 s1 保留点。select * from info：这个时候你应该是只能看到 “s1 after” 这条数据了，并且事务没有提交，验证的话，还是重启一个客户端查看就好rollback：回滚事务，也就是说，我们之前做的操作我都不要了，回滚到开始事务时候的状态，并结束事务。insert into info values(1,&quot;no transaction&quot;,20181019)：再插入一条数据select * from info：再另外一个客户端直接查询，立马看到上面插入的数据 “no transaction”，也是验证了事务确实结束了set autocommit&#x3D;0：上面我们插入数据，另外一个客户端立马就查询到了，说明是自动提交了我们的插入，现在我们设置不自动提交。insert into info values(1,&quot;autocommit&#x3D;0&quot;,20181019)：插入一条数据 “autocommit&#x3D;0”select * from info;在另外一个客户端查询，你会发现，上面这条 &quot;autocommit&#x3D;0&quot; 数据并没有插入commit提交，这个时候你再去查询，才能查到你提交的数据 &quot;autocommit&#x3D;0&quot;\n\n这里需要注意的是，不要将非自动提交和事务搞混淆了，非自动提交，一般是为了批次提交从而提升效率，但是并不具备事务性，而事务是在批次提交的基础上保证了事务性，所以还是有一定区别的噢\n","categories":["mysql"],"tags":["python"]},{"title":"MySQL慢查询—开启慢查询","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/01/30/MySQL慢查询—开启慢查询/","content":"一、简介开启慢查询日志，可以让MySQL记录下查询超过指定时间的语句，通过定位分析性能的瓶颈，才能更好的优化数据库系统的性能。\n二、参数说明slow_query_log 慢查询开启状态slow_query_log_file 慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录）long_query_time 查询超过多少秒才记录\n三、设置步骤1.查看慢查询相关参数\n1234567891011121314mysql&gt; show variables like &#x27;slow_query%&#x27;;+---------------------------+----------------------------------+| Variable_name             | Value                            |+---------------------------+----------------------------------+| slow_query_log            | OFF                              || slow_query_log_file       | /mysql/data/localhost-slow.log   |+---------------------------+----------------------------------+mysql&gt; show variables like &#x27;long_query_time&#x27;;+-----------------+-----------+| Variable_name   | Value     |+-----------------+-----------+| long_query_time | 10.000000 |+-----------------+-----------+\n\n2.设置方法\n方法一：全局变量设置将 slow_query_log 全局变量设置为“ON”状态\n1mysql&gt; set global slow_query_log&#x3D;&#39;ON&#39;; \n\n设置慢查询日志存放的位置\n1mysql&gt; set global slow_query_log_file&#x3D;&#39;&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;slow.log&#39;;\n\n查询超过1秒就记录\n1mysql&gt; set global long_query_time&#x3D;1;\n\n方法二：配置文件设置修改配置文件my.cnf，在[mysqld]下的下方加入\n1234[mysqld]slow_query_log &#x3D; ONslow_query_log_file &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;slow.loglong_query_time &#x3D; 1\n\n3.重启MySQL服务\n1service mysqld restart\n\n4.查看设置后的参数\n1234567891011121314mysql&gt; show variables like &#39;slow_query%&#39;;+---------------------+--------------------------------+| Variable_name       | Value                          |+---------------------+--------------------------------+| slow_query_log      | ON                             || slow_query_log_file | &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;slow.log |+---------------------+--------------------------------+mysql&gt; show variables like &#39;long_query_time&#39;;+-----------------+----------+| Variable_name   | Value    |+-----------------+----------+| long_query_time | 1.000000 |+-----------------+----------+\n\n四、测试1.执行一条慢查询SQL语句\n1mysql&gt; select sleep(2);\n\n2.查看是否生成慢查询日志\n1ls &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;slow.log\n\n如果日志存在，MySQL开启慢查询设置成功！\n","categories":["mysql"],"tags":["python"]},{"title":"MySQL存储引擎","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/01/19/MySQL存储引擎/","content":"什么是存储引擎数据库存储引擎是数据库底层软件组件，数据库管理系统使用数据引擎进行创建、查询、更新和删除数据操作。不同的存储引擎提供不同的存储机制、索引技巧、锁定水平等功能，使用不同的存储引擎还可以获得特定的功能。\n现在许多数据库管理系统都支持多种不同的存储引擎。MySQL 的核心就是存储引擎。\n提示：InnoDB 事务型数据库的首选引擎，支持事务安全表（ACID），支持行锁定和外键。MySQL 5.5.5 之后，InnoDB 作为默认存储引擎。\nMyISAM 是基于 ISAM 的存储引擎，并对其进行扩展，是在 Web、数据仓储和其他应用环境下最常使用的存储引擎之一。MyISAM 拥有较高的插入、查询速度，但不支持事务。\nMEMORY 存储引擎将表中的数据存储到内存中，为查询和引用其他数据提供快速访问。\nMySQL 5.7 支持的存储引擎MySQL 支持多种类型的数据库引擎，可分别根据各个引擎的功能和特性为不同的数据库处理任务提供各自不同的适应性和灵活性。在 MySQL 中，可以利用 SHOW ENGINES 语句来显示可用的数据库引擎和默认引擎。\nMySQL 提供了多个不同的存储引擎，包括处理事务安全表的引擎和处理非事务安全表的引擎。在 MySQL 中，不需要在整个服务器中使用同一种存储引擎，针对具体的要求，可以对每一个表使用不同的存储引擎。\nMySQL 5.7 支持的存储引擎有 InnoDB、MyISAM、Memory、Merge、Archive、Federated、CSV、BLACKHOLE 等。可以使用SHOW ENGINES语句查看系统所支持的引擎类型，结果如图所示。\n\nSupport 列的值表示某种引擎是否能使用，YES表示可以使用，NO表示不能使用，DEFAULT表示该引擎为当前默认的存储引擎。\n如何选择 MySQL 存储引擎不同的存储引擎都有各自的特点，以适应不同的需求，如表所示。为了做出选择，首先要考虑每一个存储引擎提供了哪些不同的功能。\n可以根据以下的原则来选择 MySQL 存储引擎：\n\n如果要提供提交、回滚和恢复的事务安全（ACID 兼容）能力，并要求实现并发控制，InnoDB 是一个很好的选择。\n如果数据表主要用来插入和查询记录，则 MyISAM 引擎提供较高的处理效率。\n如果只是临时存放数据，数据量不大，并且不需要较高的数据安全性，可以选择将数据保存在内存的 MEMORY 引擎中，MySQL 中使用该引擎作为临时表，存放查询的中间结果。\n如果只有 INSERT 和 SELECT 操作，可以选择Archive 引擎，Archive 存储引擎支持高并发的插入操作，但是本身并不是事务安全的。Archive 存储引擎非常适合存储归档数据，如记录日志信息可以使用 Archive 引擎。\n\n提示：使用哪一种引擎要根据需要灵活选择，一个数据库中多个表可以使用不同的引擎以满足各种性能和实际需求。使用合适的存储引擎将会提高整个数据库的性能。\nMySQL 默认存储引擎InnoDB 是系统的默认引擎，支持可靠的事务处理。\n使用下面的语句可以修改数据库临时的默认存储引擎\n1SET default_storage_engine&#x3D;&lt; 存储引擎名 &gt;\n例如，将 MySQL 数据库的临时默认存储引擎修改为 MyISAM，输入的 SQL 语句和运行结果如图所示。\n此时，可以发现 MySQL 的默认存储引擎已经变成了 MyISAM。但是当再次重启客户端时，默认存储引擎仍然是 InnoDB。\n","categories":["mysql"],"tags":["python"]},{"title":"MySQL 优化","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/01/12/MySQL 优化/","content":"1. 选取最适用的字段属性表中字段的宽度设得尽可能小：char 的上限为 255 字节（固定占用空间），varchar 的上限 65535 字节（实际占用空间），text 的上限为 65535。char 比 varchar 处理效率高。\n尽量把字段设置为 NOT NULL，执行查询的时候，数据库不用去比较 NULL 值。\n2. 使用连接（JOIN）来代替子查询 (Sub-Queries)连接（JOIN）之所以更有效率一些，是因为 MySQL 不需要在内存中创建临时表来完成这个逻辑上的需要两个步骤的查询工作（联合查询的条件加索引更快）。\n3. 使用联合 (UNION) 来代替手动创建的临时表把需要使用临时表的两条或更多的 SELECT 查询合并的一个查询中。\nSELECT Name, Phone FROM client UNION SELECT Name, BirthDate FROM author UNION SELECT Name, Supplier FROM product;\n4. 事务尽管我们可以使用子查询（Sub-Queries）、连接（JOIN）和联合（UNION）来创建各种各样的查询，但不是所有的数据库操作都可以只用一条或少数几条 SQL 语句就可以完成的。更多的时候是需要用到一系列的语句来完成某种工作。\n作用是：要么语句块中每条语句都操作成功，要么都失败。换句话说，就是可以保持数据库中数据的一致性和完整性。事物以 BEGIN 关键字开始，COMMIT 关键字结束。在这之间的一条 SQL 操作失败，那么，ROLLBACK 命令就可以把数据库恢复到 BEGIN 开始之前的状态。\n5. 锁定表尽管事务是维护数据库完整性的一个非常好的方法，但却因为它的独占性，有时会影响数据库的性能，尤其是在很大的应用系统中。由于在事务执行的过程中，数据库将会被锁定，因此其它的用户请求只能暂时等待直到该事务结束。\n1234567LOCK TABLE inventory WRITE SELECT Quantity FROM inventory WHEREItem=&#x27;book&#x27;; ... UPDATE inventory SET Quantity=11 WHEREItem=&#x27;book&#x27;; UNLOCK TABLES\n这里，我们用一个 SELECT 语句取出初始数据，通过一些计算，用 UPDATE 语句将新值更新到表中。包含有 WRITE 关键字的 LOCK TABLE 语句可以保证在 UNLOCK TABLES 命令被执行之前，不会有其它的访问来对 inventory 进行插入、更新或者删除的操作。\n6、使用外键锁定表的方法可以维护数据的完整性，但是它却不能保证数据的关联性。这个时候我们就可以使用外键。例如，外键可以保证每一条销售记录都指向某一个存在的客户。在这里，外键可以把 customerinfo 表中的 CustomerID 映射到 salesinfo 表中 CustomerID，任何一条没有合法 CustomerID 的记录都不会被更新或插入到 salesinfo 中。\n12345678910111213CREATE TABLE customerinfo ( CustomerID INT NOT NULL , PRIMARY KEY ( CustomerID ) ) TYPE = INNODB; CREATE TABLE salesinfo ( SalesID INT NOT NULL, CustomerID INT NOT NULL, PRIMARY KEY(CustomerID, SalesID), FOREIGN KEY (CustomerID) REFERENCES customerinfo (CustomerID) ON DELETECASCADE ) TYPE = INNODB;\n\n注意例子中的参数 “ON DELETE CASCADE”。该参数保证当 customerinfo 表中的一条客户记录被删除的时候，salesinfo 表中所有与该客户相关的记录也会被自动删除。如果要在 MySQL 中使用外键，一定要记住在创建表的时候将表的类型定义为事务安全表 InnoDB 类型。该类型不是 MySQL 表的默认类型。定义的方法是在 CREATE TABLE 语句中加上 TYPE=INNODB。\n7. 使用索引查询语句当中包含有 MAX (), MIN () 和 ORDERBY 这些命令的时候，性能提高更为明显。\n索引应建立在那些将用于 JOIN, WHERE 判断和 ORDER BY 排序的字段上。尽量不要对数据库中某个含有大量重复的值的字段建立索引。对于一个 ENUM 类型的字段来说，出现大量重复值是很有可能的情况，例如 customerinfo 中的 “province”.. 字段，在这样的字段上建立索引将不会有什么帮助；相反，还有可能降低数据库的性能。\n普通索引（由关键字 KEY 或 INDEX 定义的索引）的唯一任务是加快对数据的访问速度。因此，应该只为那些最经常出现在查询条件（WHEREcolumn=）或排序条件（ORDERBYcolumn）中的数据列创建索引。\n唯一索引的好处：一是简化了 MySQL 对这个索引的管理工作，这个索引也因此而变得更有效率；二是 MySQL 会在有新记录插入数据表时，自动检查新记录的这个字段的值是否已经在某个记录的这个字段里出现过了；如果是，MySQL 将拒绝插入那条新记录。也就是说，唯一索引可以保证数据记录的唯一性。在许多场合，创建唯一索引的目的往往不是为了提高访问速度，而只是为了避免数据出现重复。\n8. 优化的查询语句1234SELECT FROM order WHERE YEAR(OrderDate)&lt;2001; SELECT FROM order WHERE OrderDate&lt;&quot;2001-01-01&quot;;SELECT FROM inventory WHERE Amount/7&lt;24; SELECT FROM inventory WHERE Amount&lt;24*7;\n避免在查询中让 MySQL 进行自动类型转换，因为转换过程也会使索引变得不起作用。\n9. 索引失效情况like 以 % 开头，索引无效；当 like 前缀没有 %，后缀有 % 时，索引有效。\nor 语句前后没有同时使用索引。当 or 左右查询字段只有一个是索引，该索引失效，只有当 or 左右查询字段均为索引时，才会生效。\n组合索引，不是使用第一列索引，索引失效。\n数据类型出现隐式转化。如 varchar 不加单引号的话可能会自动转换为 int 型，使索引无效，产生全表扫描。\n在索引字段上使用 not，&lt;&gt;，!=。不等于操作符是永远不会用到索引的，因此对它的处理只会产生全表扫描。 优化方法： key&lt;&gt;0 改为 key&gt;0 or key&lt;0。\n当全表扫描速度比索引速度快时，mysql 会使用全表扫描，此时索引失效。\n应尽量避免在 where 子句中使用 or,and,in,not in 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，合理使用 union all（允许重复的值，请使用 UNION ALL）。\n1select id from t where num=10 or num=20\n可以这样查询：\n123select id from t where num=10    union all    select id from t where num=20\n10. 引擎的选取MyISAM 索引文件在数据库中存放的对应表的磁盘文件有.frm,.MYD,*.MYI 结尾的三个文件：\nfrm 文件是存放的表结构，表的定义信息；\nMYD 文件是存放着表中的数据；\nMYI 文件存放着表的索引信息；\nInnoDB 存储引擎在磁盘中存放的对应的表的磁盘文件有.frm,.ibd 这两个文件；\nfrm 文件是存放表结构，表的定义信息；\nibd 文件是存放 表中的数据、索引信息；\n性能方面的优化：1.分表的分类（单表记录条数达到百万到千万级别时就要使用分表）\n纵向分表\n\n文章标题，作者，分类，创建时间等，是变化频率慢，查询次数多，而且最好有很好的实时性的数据，我们把它叫做冷数据。\n浏览量，回复数等，类似的统计信息，或者别的变化频率比较高的数据，我们把它叫做活跃数据。\n首先存储引擎的使用不同，冷数据使用 MyIsam 可以有更好的查询数据。活跃数据，可以使用 Innodb , 可以有更好的更新速度。\n就是把原来一张表里的字段，冷数据的字段和活跃数据的字段分别建立 2 张表来管理。\n\n横向分表\n\n把大的表结构，横向切割为同样结构的不同表，如，用户信息表，user_1,user_2 等，表结构是完全一样。\n2. 慢查询12show variables like &#x27;slow%&#x27;;show global status like &#x27;slow%&#x27;;\n使用 mysqlreport；\n正确使用索引：explain 分析查询语句，组合索引，索引副作用（占空间、update）\n开启慢查询日志、使用慢查询分析工具 mysqlsla；\n索引缓存、索引代价（插入更新索引）；\n表锁，行锁，行锁副作用（update 多时候变慢），在 select 和 update 混合的情况下，行锁巧妙解决了读写互斥的问题；\n开启使用查询缓存；\n修改临时表内存空间；\n开启线程池；\nMySQL Query 语句优化的基本思路和原则1、优化需要优化的 Query；\n2、定位优化对象的性能瓶颈；\n3、明确优化目标；\n4、从 Explaing 入手；\n5、多使用 Profile；\n6、永远用小结果集推动大的结果集；\n7、尽可能在索引中完成排序；\n8、只取自己需要的 Columns；\n9、仅仅使用最有效的过滤条件；\n10、尽可能避免复杂的 Join 和子查询。\n","categories":["mysql"],"tags":["python"]},{"title":"MD基本语法","url":"https://github.com/xuMr6/xumr6.github.io.git/2019/01/07/MD基本语法/","content":"md是什么.md即markdown文件的基本常用编写语法,是一种快速标记、快速排版语言\n1.基本符号：* - + &gt;基本上所有的markdown标记都是基于这四个符号或组合，需要注意的是，如果以基本符号开头的标记，注意基本符号后有一个用于分割标记符和内容的空格。\n2.标题前面带#号，后面带文字，分别表示h1-h6,只到h6，而且h1下面会有一条横线  –&gt; 相当于标签闭合123456# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题\n\n123456# 一级标题 ### 二级标题 ##### 三级标题 ####### 四级标题 ######### 五级标题 ########### 六级标题 #####\n效果图：\n\n3.列表\n无序列表1234567891011121314//形式一+ a+ b+ c//形式二- d- e- f//形式三* g* h* i\n\n\n\n以上三种形式，效果其实都是一样的：\n\n\n有序列表123456789//正常形式1. abc2. bcd3. cde//错序效果2. fgh3. ghi5. hij\n效果图：\n\n\n注意：12如图,数字后面的点只能是英文的点，有序列表的序号是根据第一行列表的数字顺序来的，错序列表的序号本来是序号是乱的， 但是还是显示 2 3 5\n\n\n嵌套列表1234567891011121314//无序列表嵌套+ 123    + abc    + bcd    + cde+ 465+ 789//有序列表嵌套1. abcd    1. abcde    2. abcde    3. abcde2. bcde3. cdef\n效果图：\n\n列表可以嵌套，使用时在嵌套列表前按 tab 或 空格 来缩进,去控制列表的层数\n4.引用说明区块对某个部分做的内容做一些说明或者引用某某的话等，可以用这个语法。\n正常形式1&gt; 引用内容、说明内容。在语句前面加一个 &gt; ，注意是英文的那个右尖括号，注意空格，引用因为是一个区块，理论上是应该什么内容都可以放，比如说：标题，列表，引用等等。\n效果图：\n\n\n\n嵌套区块这里我只介绍一下我常用的方法，也是个人认为比较规范的一种方法，就是给区块的下一级区块多加一个右尖括号123456&gt; 一级引用&gt;&gt; 二级引用&gt;&gt;&gt; 三级引用&gt;&gt;&gt;&gt; 四级引用&gt;&gt;&gt;&gt;&gt; 五级引用&gt;&gt;&gt;&gt;&gt;&gt; 六级引用\n\n\n\n效果图：\n\n5.代码块在发布一些技术文章会涉及展示代码的问题，这时候代码块就显得尤为重要。\n\n少量代码，单行使用，直接用`包裹起来就行了1` shaoliangdaima,danhangshiyong `\n效果图：\n\n\n\n大量代码，需要多行使用，用```包裹起来123456```    daliangdaima,xuyaoduohangshiyong    daliangdaima,xuyaoduohangshiyong    daliangdaima,xuyaoduohangshiyong    daliangdaima,xuyaoduohangshiyong    daliangdaima,xuyaoduohangshiyong\n1234567891011###### 效果图：![多行代码](&#x2F;hugoblog&#x2F;多行代码.png)---## 6.链接- 行内式###### 链接的文字放在[]中，链接地址放在随后的()中，链接也可以带title属性，链接地址后面空一格，然后用引号引起来&#96;&#96;&#96;python[简书](https:&#x2F;&#x2F;www.jianshu.com &quot;创作你的创作&quot;),是一个创作社区,任何人均可以在其上进行创作。用户在简书上面可以方便的创作自己的作品,互相交流。 \n\n\n\n\n参数式链接的文字放在[]中，链接地址放在随后的:后，链接地址后面空一格，然后用引号引起来\n\n123456[简书]: https://www.jianshu.com &quot;创作你的创作&quot;[简书]是一个创作社区,任何人均可以在其上进行创作。用户在简书上面可以方便的创作自己的作品,互相交流。//参数定义的其他写法[简书]: https://www.jianshu.com &#x27;创作你的创作&#x27;[简书]: https://www.jianshu.com (创作你的创作)[简书]: &lt;https://www.jianshu.com&gt; &quot;创作你的创作&quot;\n\n以上两种方式其效果图都是一样的，如下：\n\n7.图片\n行内式和链接的形式差不多，图片的名字放在[]中，图片地址放在随后的()中，title属性（图片地址后面空一格，然后用引号引起来）,注意的是[]前要加上!\n\n1![my-logo.png](https://upload-images.jianshu.io/upload_images/13623636-6d878e3d3ef63825.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 &quot;my-logo&quot;)\n\n参数式图片的文字放在[]中，图片地址放在随后的:后，title属性（图片地址后面空一格，然后用引号引起来）,注意引用图片的时候在[]前要加上!\n\n12345678910[my-logo.png]: https://upload-images.jianshu.io/upload_images/13623636-6d878e3d3ef63825.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 &quot;my-logo&quot;![my-logo.png]//参数定义的其他写法[my-logo.png]: https://upload-images.jianshu.io/upload_images/13623636-6d878e3d3ef63825.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 &#x27;my-logo&#x27;[my-logo.png]: https://upload-images.jianshu.io/upload_images/13623636-6d878e3d3ef63825.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 (my-logo)[my-logo.png]: &lt;https://upload-images.jianshu.io/upload_images/13623636-6d878e3d3ef63825.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&gt; &quot;my-logo&quot;\n\n以上两种方式其效果图都是一样的，如下：\n\n8.分割线分割线可以由* - _（星号，减号，底线）这3个符号的至少3个符号表示，注意至少要3个，且不需要连续，有空格也可以123456789---- - -------**** * *******____ _ _______\n效果图：\n\n\n9.其他\n强调字体一个星号或者是一个下划线包起来，会转换为倾斜，如果是2个，会转换为加粗12345*md*    **md**_md_    __md__\n\n\n\n效果图：\n\n\n转义基本上和js转义一样,\\加需要转义的字符\n\n123456\\\\\\*\\+\\-\\`\\_\n\n删除线用~~把需要显示删除线的字符包裹起来12~~删除~~\n效果图：\n\n\n10.表格12345678910111213141516171819//例子一|123|234|345||:-|:-:|-:||abc|bcd|cde||abc|bcd|cde||abc|bcd|cde|//例子二|123|234|345||:---|:---:|---:||abc|bcd|cde||abc|bcd|cde||abc|bcd|cde|//例子三123|234|345:-|:-:|-:abc|bcd|cdeabc|bcd|cdeabc|bcd|cde\n上面三个例子的效果一样，由此可得：\n表格的格式不一定要对的非常起，但是为了良好的变成风格，尽量对齐是最好的\n分割线后面的冒号表示对齐方式，写在左边表示左对齐，右边为右对齐，两边都写表示居中\n\n效果图：\n\n","categories":["python"],"tags":["python"]},{"title":"md5加密原理","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/12/31/md5加密原理/","content":"用户在进行注册和登录行为时 都会涉及到密码这一敏感参数的传递 出于保护隐私的需求 我们要对于接收到的敏感参数做加密处理 对于django自带的加密算法  步骤如下：\n首先导入django自带的加密算法模块1from django.contrib.auth.hashers import make_password, check_password\n123456make_password(参数1， 参数2， 参数3)参数1： 需要加密的字符串参数2： 是否每次都生成不同的加密串，默认为None, 如果给定任意一个字符串，    则表示每次生成相同的加密串。参数3： 表示加密算法，常见的加密算法如下： df2_sha256’,  ‘pbkdf2_sha1’,   ‘bcrypt_sha256’,  ‘unsalted_md5’ 等…\n\n\n\n12my_password = make_password(&#x27;1234567890&#x27;, None, &#x27;pbkdf2_sha256&#x27;)print(my_password)     \n\n加密后的效果：1pbkdf2_sha256$12000$xzMLhCNvQbb8$i1XDnJIpb/cRRGRX2x7Ym74RNfPRCUp5pbU6Sn+V3J0=\n\n12check_password（参数1， 参数2）返回值： True或False 参数： 参数1：原始密码，参数2： 数据库查询出的密码\n123isSame = check_password(&#x27;1234567890&#x27;, sha_pwd )print(isSame)# 如果为： True: 表示密码相同，如果为：False，则表示密码不相同。\n\n\nmd5加密的内部逻辑转码时需要注意接收到的数据类型,如果是int需要强转1234567891011121314151617181920212223#导入加密库import hashlib# md5加密方法def make_password(pass):\t#生成md5对象\tmd5 = hashlib.md5()\t#转码操作\tnew_pass = str(pass).encode(encoding=&quot;utf-8&quot;)\t#加密操作\tmd5.update(new_pass)\t#返回密文\treturn md5.hexdigest()\n\n\n","categories":["django"],"tags":["python"]},{"title":"Linux 命令","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/12/26/Linux命令/","content":"1.系统信息1234567arch \t\t\t\t\t显示机器的处理器架构uname -m \t\t\t\t显示机器的处理器架构uname -r \t\t\t\t显示正在使用的内核版本cat &#x2F;proc&#x2F;cpuinfo \t\t显示CPU info的信息cat &#x2F;proc&#x2F;interrupts \t显示中断cat &#x2F;proc&#x2F;meminfo \t\t校验内存使用cat &#x2F;proc&#x2F;swaps \t\t显示哪些swap被使用\n\n2.关机12345678shutdown -h now \t\t关闭系统init 0 \t\t\t\t\t关闭系统telinit 0 \t\t\t\t关闭系统shutdown -h hours:minutes &amp; \t按预定时间关闭系统shutdown -c \t\t\t取消按预定时间关闭系统shutdown -r now \t\t重启reboot \t\t\t\t\t重启logout \t\t\t\t\t注销\n3.文件和目录1234567891011121314151617181920212223242526272829303132333435cd &#x2F;home \t\t\t\t进入 &#39;&#x2F; home&#39; 目录&#39; cd .. \t\t\t\t\t返回上一级目录 cd ..&#x2F;.. \t\t\t\t\t返回上两级目录 cd \t\t\t\t\t\t进入个人的主目录 cd ~user1 \t\t\t\t进入个人的主目录 cd - \t\t\t\t\t返回上次所在的目录pwd \t\t\t\t\t显示工作路径 ls \t\t\t\t\t\t查看目录中的文件 ls -F \t\t\t\t\t查看目录中的文件 ls -l \t\t\t\t\t显示文件和目录的详细资料 ls -a \t\t\t\t\t显示隐藏文件 ls [0-9] \t\t\t\t\t显示包含数字的文件名和目录名 tree \t\t\t\t\t显示文件和目录由根目录开始的树形结构lstree \t\t\t\t\t显示文件和目录由根目录开始的树形结构mkdir dir1 \t\t\t\t创建一个叫做 &#39;dir1&#39; 的目录&#39; mkdir dir1 dir2 \t\t\t同时创建两个目录 mkdir -p &#x2F;tmp&#x2F;dir1&#x2F;dir2 创建一个目录树 rm -f file1 \t\t\t\t删除一个叫做 &#39;file1&#39; 的文件&#39; rmdir dir1 \t\t\t\t删除一个叫做 &#39;dir1&#39; 的目录&#39; rm -rf dir1 \t\t\t\t删除一个叫做 &#39;dir1&#39; 的目录并同时删除其内容 rm -rf dir1 dir2 \t\t\t同时删除两个目录及它们的内容 mv dir1 new_dir \t\t重命名&#x2F;移动 一个目录 cp file1 file2 \t\t\t复制一个文件 cp dir&#x2F;* . \t\t\t\t复制一个目录下的所有文件到当前工作目录 cp -a &#x2F;tmp&#x2F;dir1 . \t\t复制一个目录到当前工作目录 cp -a dir1 dir2 \t\t\t复制一个目录cp -r dir1 dir2 \t\t\t复制一个目录及子目录ln -s file1 lnk1 \t\t\t创建一个指向文件或目录的软链接 ln file1 lnk1 \t\t\t创建一个指向文件或目录的物理链接 touch -t 0712250000 file1 修改一个文件或目录的时间戳 - (YYMMDDhhmm) \n\n4.文件搜索12345678910find &#x2F; -name file1 从 &#39;&#x2F;&#39; \t开始进入根文件系统搜索文件和目录 find &#x2F; -user user1 \t\t\t搜索属于用户 &#39;user1&#39; 的文件和目录 find &#x2F;home&#x2F;user1 -name *.bin \t在目录 &#39;&#x2F; home&#x2F;user1&#39; 中搜索带有&#39;.bin&#39; 结尾的文件 find &#x2F;usr&#x2F;bin -type f -atime +100 搜索在过去100天内未被使用过的执行文件 find &#x2F;usr&#x2F;bin -type f -mtime -10 \t搜索在10天内被创建或者修改过的文件 find &#x2F; -name *.rpm -exec chmod 755 &#39;&#123;&#125;&#39; \\; 搜索以 &#39;.rpm&#39; 结尾的文件并定义其权限 find &#x2F; -xdev -name *.rpm \t\t搜索以 &#39;.rpm&#39; 结尾的文件，忽略光驱、捷盘等可移动设备 locate *.ps \t\t\t\t寻找以 &#39;.ps&#39; 结尾的文件 - 先运行 &#39;updatedb&#39; 命令 whereis halt \t\t\t显示一个二进制文件、源码或man的位置 which halt \t\t\t\t显示一个二进制文件或可执行文件的完整路径\n5.挂载系统1234567891011mount &#x2F;dev&#x2F;hda2 &#x2F;mnt&#x2F;hda2 \t挂载一个叫做hda2的盘 - 确定目录 &#39;&#x2F; mnt&#x2F;hda2&#39; \t\t\t\t\t\t已经存在 umount &#x2F;dev&#x2F;hda2 卸载一个叫做hda2的盘 - 先从挂载点 &#39;&#x2F; mnt&#x2F;hda2&#39; 退出 fuser -km &#x2F;mnt&#x2F;hda2 \t\t\t当设备繁忙时强制卸载 umount -n &#x2F;mnt&#x2F;hda2 运行卸载操作而不写入 &#x2F;etc&#x2F;mtab 文件- 当文件为只读或当磁盘写满时非常有用 mount &#x2F;dev&#x2F;fd0 &#x2F;mnt&#x2F;floppy \t挂载一个软盘 mount &#x2F;dev&#x2F;cdrom &#x2F;mnt&#x2F;cdrom \t\t挂载一个cdrom或dvdrom mount &#x2F;dev&#x2F;hdc &#x2F;mnt&#x2F;cdrecorder \t挂载一个cdrw或dvdrom mount &#x2F;dev&#x2F;hdb &#x2F;mnt&#x2F;cdrecorder \t挂载一个cdrw或dvdrom mount -o loop file.iso &#x2F;mnt&#x2F;cdrom \t挂载一个文件或ISO镜像文件 mount -t vfat &#x2F;dev&#x2F;hda5 &#x2F;mnt&#x2F;hda5 \t挂载一个Windows FAT32文件系统 mount &#x2F;dev&#x2F;sda1 &#x2F;mnt&#x2F;usbdisk \t\t挂载一个usb 捷盘或闪存设备 \n\n6.RPM管理工具1234567891011121314151617181920212223242526rpm -ivh package.rpm \t安装一个rpm包 rpm -ivh --nodeeps package.rpm 安装一个rpm包而忽略依赖关系警告 rpm -U package.rpm \t更新一个rpm包但不改变其配置文件 rpm -F package.rpm \t更新一个确定已经安装的rpm包 rpm -e package_name.rpm \t删除一个rpm包 rpm -qa 显示系统中所有已经安装的rpm包 rpm -qa | grep httpd \t显示所有名称中包含 &quot;httpd&quot; 字样的rpm包 rpm -qi package_name \t获取一个已安装包的特殊信息 rpm -qg &quot;System Environment&#x2F;Daemons&quot; 显示一个组件的rpm包 rpm -ql package_name \t显示一个已经安装的rpm包提供的文件列表 rpm -qc package_name \t显示一个已经安装的rpm包提供的配置文件列表 rpm -q package_name --whatrequires \t显示与一个rpm包存在依赖关系的列表 rpm -q package_name --whatprovides 显示一个rpm包所占的体积 rpm -q package_name --scripts 显示在安装&#x2F;删除期间所执行的脚本l rpm -q package_name --changelog \t显示一个rpm包的修改历史 rpm -qf &#x2F;etc&#x2F;httpd&#x2F;conf&#x2F;httpd.conf 确认所给的文件由哪个rpm包所提供 rpm -qp package.rpm -l \t显示由一个尚未安装的rpm包提供的文件列表 rpm --import &#x2F;media&#x2F;cdrom&#x2F;RPM-GPG-KEY 导入公钥数字证书 rpm --checksig package.rpm \t\t\t确认一个rpm包的完整性 rpm -qa gpg-pubkey \t\t确认已安装的所有rpm包的完整性 rpm -V package_name \t检查文件尺寸、 许可、类型、所有者、群组、MD5检查以及最后修改时间 rpm -Va \t\t\t\t检查系统中所有已安装的rpm包- 小心使用 rpm -Vp package.rpm \t确认一个rpm包还未安装 rpm2cpio package.rpm | cpio --extract --make-directories bin 从一个rpm包运行可执行文件 rpm -ivh &#x2F;usr&#x2F;src&#x2F;redhat&#x2F;RPMS&#x2F;arch&#x2F;package.rpm 从一个rpm源码安装一个构建好的包 rpmbuild --rebuild package_name.src.rpm 从一个rpm源码构建一个 rpm 包\n7.YUM管理工具12345678910yum install package_name \t\t\t下载并安装一个rpm包 yum localinstall package_name.rpm \t将安装一个rpm包，使用你自己的软件仓库为你解决所有依赖关系 yum update package_name.rpm \t\t更新当前系统中所有安装的rpm包 yum update package_name \t\t\t更新一个rpm包 yum remove package_name \t\t\t删除一个rpm包 yum list \t\t\t\t\t\t\t列出当前系统中安装的所有包 yum search package_name \t\t\t在rpm仓库中搜寻软件包 yum clean packages \t\t\t\t\t清理rpm缓存删除下载的包 yum clean headers \t\t\t\t\t删除所有头文件 yum clean all \t\t\t\t\t\t删除所有缓存的包和头文件\n38查看文件内容1234567cat file1 \t\t从第一个字节开始正向查看文件的内容 tac file1 \t\t从最后一行开始反向查看一个文件的内容 more file1 \t\t查看一个长文件的内容 less file1 \t\t类似于 &#39;more&#39; 命令，但是它允许在文件中和正向操作一样的反向操作 head -2 file1 \t查看一个文件的前两行 tail -2 file1 \t\t查看一个文件的最后两行 tail -f &#x2F;var&#x2F;log&#x2F;messages \t实时查看被添加到一个文件中的内容\n\n9.字符设置、文件格式转换12345dos2unix filedos.txt fileunix.txt \t将一个文本文件的格式从MSDOS转换成UNIX unix2dos fileunix.txt filedos.txt \t将一个文本文件的格式从UNIX转换成MSDOS recode ..HTML &lt; page.txt &gt; page.html \t将一个文本文件转换成html recode -l | more \t\t\t\t显示所有允许的转换格式\n\n10.备份123456789101112131415161718dump -0aj -f &#x2F;tmp&#x2F;home0.bak &#x2F;home 制作一个 &#39;&#x2F;home&#39; 目录的完整备份 dump -1aj -f &#x2F;tmp&#x2F;home0.bak &#x2F;home 制作一个 &#39;&#x2F;home&#39; 目录的交互式备份 restore -if &#x2F;tmp&#x2F;home0.bak 还原一个交互式备份 rsync -rogpav --delete &#x2F;home &#x2F;tmp 同步两边的目录 rsync -rogpav -e ssh --delete &#x2F;home ip_address:&#x2F;tmp 通过SSH通道rsync rsync -az -e ssh --delete ip_addr:&#x2F;home&#x2F;public &#x2F;home&#x2F;local 通过ssh和压缩将一个远程目录同步到本地目录 rsync -az -e ssh --delete &#x2F;home&#x2F;local ip_addr:&#x2F;home&#x2F;public 通过ssh和压缩将本地目录同步到远程目录 dd bs&#x3D;1M if&#x3D;&#x2F;dev&#x2F;hda | gzip | ssh user@ip_addr &#39;dd of&#x3D;hda.gz&#39; 通过ssh在远程主机上执行一次备份本地磁盘的操作 dd if&#x3D;&#x2F;dev&#x2F;sda of&#x3D;&#x2F;tmp&#x2F;file1 备份磁盘内容到一个文件 tar -Puf backup.tar &#x2F;home&#x2F;user 执行一次对 &#39;&#x2F;home&#x2F;user&#39; 目录的交互式备份操作 ( cd &#x2F;tmp&#x2F;local&#x2F; &amp;&amp; tar c . ) | ssh -C user@ip_addr &#39;cd &#x2F;home&#x2F;share&#x2F; &amp;&amp; tar x -p&#39; 通过ssh在远程目录中复制一个目录内容 ( tar c &#x2F;home ) | ssh -C user@ip_addr &#39;cd &#x2F;home&#x2F;backup-home &amp;&amp; tar x -p&#39; 通过ssh在远程目录中复制一个本地目录 tar cf - . | (cd &#x2F;tmp&#x2F;backup ; tar xf - ) 本地将一个目录复制到另一个地方，保留原有权限及链接 find &#x2F;home&#x2F;user1 -name &#39;*.txt&#39; | xargs cp -av --target-directory&#x3D;&#x2F;home&#x2F;backup&#x2F; --parents 从一个目录查找并复制所有以 &#39;.txt&#39; 结尾的文件到另一个目录 find &#x2F;var&#x2F;log -name &#39;*.log&#39; | tar cv --files-from&#x3D;- | bzip2 &gt; log.tar.bz2 查找所有以 &#39;.log&#39; 结尾的文件并做成一个bzip包 dd if&#x3D;&#x2F;dev&#x2F;hda of&#x3D;&#x2F;dev&#x2F;fd0 bs&#x3D;512 count&#x3D;1 做一个将 MBR (Master Boot Record)内容复制到软盘的动作 dd if&#x3D;&#x2F;dev&#x2F;fd0 of&#x3D;&#x2F;dev&#x2F;hda bs&#x3D;512 count&#x3D;1 从已经保存到软盘的备份中恢复MBR内容","categories":["linux"],"tags":["Linux"]},{"title":"JWT使用","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/12/21/JWT使用/","content":"Json Web Token(JWT) 是为了在网络应用环境间传递声明而执行的一种基于JSON的开放标准,JWT的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其它业务逻辑所必须的声明信息，该token也可直接被用于认证，也可被加密。\n什么场景使用JWTPermalink在前后端分离情况下前端的每一次请求都会携带指定url地址和参数， 从而很可能被不法分子根据指定的url来仿照前端直接请求后端，造成性能与数据的流失 ， 只需更换一些参数就能获取其他用户的数据 岂不是很不安全我们需要在请求的时候加一些身份验证，保证后端服务器不能被第三方访问\n生成jwt:Permalink&amp;使用jwt模块 jwt.encode(参数,秘钥,加密方式)12345678910111213import jwtimport datetime# playload 载荷就是存放有效信息的地方。这个名字像是特指飞机上承载的货品，这些有效信息包含三个部分playload = &#123;    # 创建一个过期时间    &#x27;exp&#x27;: int((datetime.datetime.now() + datetime.timedelta(hours=4)).timestamp()),    &#x27;data&#x27;: &#123;&#x27;uid&#x27;: user.id&#125;&#125;encode_jwt = jwt.encode(playload, &#x27;秘钥&#x27;, algorithm=&#x27;HS256&#x27;)#加密后是二进制形式 要转码为strencode_str = str(encode_jwt, &#x27;utf-8&#x27;)return Response(&#123;&quot;code&quot;: 200, &quot;msg&quot;: &quot;登录成功&quot;,&#x27;jwt&#x27;:encode_str&#125;)\n\n这样在登录的时候就将认证的jwt传递给前端并且有过期时间 当jwt过期的时候就重新登录   如何验证jwt:123456789101112131415161718192021from django.utils.decorators import method_decorator  # 导入django自带的方法装饰器模块# 定义权限检测装饰器def my_decorator(func):    def warpper(request,*args,**kwargs):        # 拦截获取参数        uid=request.GET.get(&#x27;uid&#x27;,None)        myjwt=request.GET.get(&#x27;jwt&#x27;,None)        try:            # 对jwt进行解码  algorithms=[&#x27;HS256&#x27;] 加密方式 HS256            # 如果jwt过期会报错 就需要使用try            decode_jwt = jwt.decode(myjwt, &#x27;秘钥&#x27;, algorithms=[&#x27;HS256&#x27;])        except Exception as e:            return Response(&#123;&#x27;code&#x27;: 401, &#x27;msg&#x27;: &#x27;您的秘钥已失效&#x27;&#125;)\t\t        # 判断jwt内的参数是否被篡改        if int(uid)!= int(decode_jwt[&#x27;data&#x27;][&#x27;uid&#x27;]):            return Response(&#123;&#x27;code&#x27;:401,&#x27;msg&#x27;:&#x27;您的秘钥无权限&#x27;&#125;)                return func(request,*args,**kwargs)    return warpper\n创建一个装饰器 在需要验证的函数上方进行拦截验证12345# 当使用的是类方法需要这样注册装饰器@method_decorator(my_decorator)def get(slef,request):    pass\n","categories":["jwt"],"tags":["python"]},{"title":"json的转换","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/12/18/json的转换/","content":"在vue中,我们需要将数据转成json格式和将json格式转成字符串12改为json格式：JSON.stringify()将json格式转为字典格式：JSON.parse()\n\n\n代码示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697&lt;template&gt;  &lt;div&gt;      &lt;table&gt;            &lt;tr&gt;                &lt;td&gt;                    商品名称：                &lt;/td&gt;                &lt;td&gt;                    &lt;input type=&quot;text&quot; v-model=&quot;name&quot; placeholder=&quot;请输入商品名称&quot;&gt;                &lt;/td&gt;            &lt;/tr&gt;            &lt;tr&gt;                &lt;td&gt;                    商品价格：                &lt;/td&gt;                &lt;td&gt;                    &lt;input type=&quot;number&quot; v-model=&quot;price&quot; placeholder=&quot;请输入商品价格&quot;&gt;                &lt;/td&gt;            &lt;/tr&gt;            &lt;tr&gt;                &lt;td&gt;                    商品颜色：                &lt;/td&gt;                &lt;td&gt;                    &lt;input type=&quot;text&quot; v-model=&quot;color&quot; placeholder=&quot;请输入商品颜色&quot;&gt;                &lt;/td&gt;            &lt;/tr&gt;            &lt;tr&gt;                &lt;td&gt;                    商品尺寸：                &lt;/td&gt;                &lt;td&gt;                    &lt;input type=&quot;text&quot; v-model=&quot;size&quot; placeholder=&quot;请输入商品尺寸&quot;&gt;                &lt;/td&gt;            &lt;/tr&gt;            &lt;tr&gt;                &lt;td&gt;                    商品季节：                &lt;/td&gt;                &lt;td&gt;                    &lt;input type=&quot;text&quot; v-model=&quot;season&quot; placeholder=&quot;请输入商品季节&quot;&gt;                &lt;/td&gt;            &lt;/tr&gt;             &lt;tr&gt;                &lt;td&gt;                &lt;/td&gt;                &lt;td&gt;                    &lt;Button color=&quot;blue&quot; @click=&quot;submit&quot;&gt;添加商品&lt;/Button&gt;                &lt;/td&gt;            &lt;/tr&gt;              &lt;/table&gt;  &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default&#123;  data()&#123;    return &#123;        name:&quot;&quot;,//商品名      price:0,//商品价格      color:&quot;&quot;,//商品颜色      size:&quot;&quot;,//商品尺寸      season:&quot;&quot;,//商品季节    &#125;  &#125;,  methods:&#123;      //进行添加商品      submit()&#123;          //将季节，颜色规格等字段转换为json          var param = &#123;&#125;;          param[&#x27;color&#x27;] = this.color;          param[&#x27;size&#x27;] = this.size;          param[&#x27;season&#x27;] = this.season;          console.log(param);         //&#123;color: &quot;green&quot;, size: &quot;xxl&quot;, season: &quot;夏季&quot;&#125;          //将字段转换为json类型      JSON.stringify          param = JSON.stringify(param)          console.log(param)          //&#123;&quot;color&quot;:&quot;green&quot;,&quot;size&quot;:&quot;xxl&quot;,&quot;season&quot;:&quot;夏季&quot;&#125;          //将json类型转为字符串      JSON.parse          // param = JSON.parse(param)          // console.log(param)          //&#123;color: &quot;gray&quot;, size: &quot;xxl&quot;, season: &quot;秋季&quot;&#125;          this.axios(&#123;              url: &quot;http://localhost:8000/insertgoods/&quot;,              method: &#x27;GET&#x27;,              params:&#123;                  name:this.name,                  price:this.price,                  params:param,              &#125;          &#125;).then(resp =&gt; &#123;              console.log(resp)              this.$Message(resp.data.message)          &#125;)      &#125;,  &#125;&#125;&lt;/script&gt;\n","categories":["django"],"tags":["python"]},{"title":"HTTP与HTTPS","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/12/12/HTTP与HTTPS/","content":"\n基本概念HTTP（HyperText Transfer Protocol：超文本传输协议）是一种用于分布式、协作式和超媒体信息系统的应用层协议。 简单来说就是一种发布和接收 HTML 页面的方法，被用于在 Web 浏览器和网站服务器之间传递信息。\nHTTP 默认工作在 TCP 协议 80 端口，用户访问网站 http:// 打头的都是标准 HTTP 服务。\nHTTP 协议以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此，HTTP协议不适合传输一些敏感信息，比如：信用卡号、密码等支付信息。\nHTTPS（Hypertext Transfer Protocol Secure：超文本传输安全协议）是一种透过计算机网络进行安全通信的传输协议。HTTPS 经由 HTTP 进行通信，但利用 SSL/TLS 来加密数据包。HTTPS 开发的主要目的，是提供对网站服务器的身份认证，保护交换数据的隐私与完整性。\nHTTPS 默认工作在 TCP 协议443端口，它的工作流程一般如以下方式：\n\nTCP 三次同步握手\n客户端验证服务器数字证书\nDH 算法协商对称加密算法的密钥、hash 算法的密钥\nSSL 安全加密隧道协商完成\n网页以加密的方式传输，用协商的对称加密算法和密钥加密，保证数据机密性；用协商的hash算法进行数据完整性保护，保证数据不被篡改。HTTP 与 HTTPS 区别\n\n\nHTTP 明文传输，数据都是未加密的，安全性较差，HTTPS（SSL+HTTP） 数据传输过程是加密的，安全性较好。使用 HTTPS 协议需要到 CA（Certificate Authority，数字证书认证机构） 申请证书，一般免费证书较少，因而需要一定费用。证书颁发机构如：Symantec、Comodo、GoDaddy 和 GlobalSign 等。\nHTTP 页面响应速度比 HTTPS 快，主要是因为 HTTP 使用 TCP 三次握手建立连接，客户端和服务器需要交换 3 个包，而 HTTPS除了 TCP 的三个包，还要加上 ssl 握手需要的 9 个包，所以一共是 12 个包。\nHTTP 和 HTTPS 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443。\nHTTPS 其实就是建构在 SSL/TLS 之上的 HTTP 协议，所以，要比较 HTTPS 比 HTTP 要更耗费服务器资源。\nTCP 三次握手\n\n在TCP/IP协议中，TCP协议通过三次握手建立一个可靠的连接\n\n第一次握手：客户端尝试连接服务器，向服务器发送 syn 包（同步序列编号Synchronize Sequence Numbers），syn=j，客户端进入 SYN_SEND 状态等待服务器确认\n第二次握手：服务器接收客户端syn包并确认（ack=j+1），同时向客户端发送一个 SYN包（syn=k），即 SYN+ACK 包，此时服务器进入 SYN_RECV 状态\n第三次握手：第三次握手：客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK(ack=k+1），此包发送完毕，客户端和服务器进入ESTABLISHED状态，完成三次握手客户端在使用HTTPS方式与Web服务器通信时的步骤\n\n1. 客户使用https的URL访问Web服务器，要求与Web服务器建立SSL连接。2. Web服务器收到客户端请求后，会将网站的证书信息（证书中包含公钥）传送一份给客户端。3. 客户端的浏览器与Web服务器开始协商SSL/TLS连接的安全等级，也就是信息加密的等级。4. 客户端的浏览器根据双方同意的安全等级，建立会话密钥，然后利用网站的公钥将会话密钥加密，并传送给网站。5. Web服务器利用自己的私钥解密出会话密钥。6. Web服务器利用会话密钥加密与客户端之间的通信。\n\n\n 尽管HTTPS并非绝对安全，掌握根证书的机构、掌握加密算法的组织同样可以进行中间人形式的攻击，但HTTPS仍是现行架构下最安全的解决方案，但他大幅增加了中间人攻击的成本\nCA证书的申请及其使用过程上面客户端使用HTTPS与服务器通信中使用到了CA认证，这里可能大家会问为什么不直接使用非对称加密的形式直接进行，首先这里先介绍下非对称加密。\n非对称加密：客户端和服务端均拥有一个公有密匙和一个私有密匙。公有密匙可以对外暴露，而私有密匙只有自己可见。\n使用公有密匙加密的消息，只有对应的私有密匙才能解开。反过来，使用私有密匙加密的消息，只有公有密匙才能解开。这样客户端在发送消息前，先用服务器的公匙对消息进行加密，服务器收到后再用自己的私匙进行解密。重做：\n\n非对称加密的优点：非对称加密采用公有密匙和私有密匙的方式，解决了http中消息保密性问题，而且使得私有密匙泄露的风险降低。\n因为公匙加密的消息只有对应的私匙才能解开，所以较大程度上保证了消息的来源性以及消息的准确性和完整性。\n非对称加密的缺点：非对称加密时需要使用到接收方的公匙对消息进行加密，但是公匙不是保密的，任何人都可以拿到，中间人也可以。那么中间人可以做两件事，第一件是中间人可以在客户端与服务器交换公匙的时候，将客户端的公匙替换成自己的。这样服务器拿到的公匙将不是客户端的，而是中间人的。服务器也无法判断公匙来源的正确性。第二件是中间人可以不替换公匙，但是他可以截获客户端发来的消息，然后篡改，然后用服务器的公匙加密再发往服务器，服务器将收到错误的消息。\n非对称加密的性能相对对称加密来说会慢上几倍甚至几百倍，比较消耗系统资源。正是因为如此，https将两种加密结合了起来。\n为了应对上面非对称加密带来的问题，我们就引入了数字证书与数字签名\n故CA认证介入我们的HTTPS连接的过程如下：\n1、服务器拥有自己的私钥与公钥\n2、服务器将公钥交给CA认证机构，请求给予一份数字证书\n3、CA认证机构生成数字证书，并颁发给服务器\n4、服务器将带有公钥信息的数字证书发给客户端\n5、进入客户端生成对称密钥再进行对接的过程……\n\nHTTPS的缺点 　　虽然说HTTPS有很大的优势，但其相对来说，还是存在不足之处的：\n　　（1）HTTPS协议握手阶段比较费时，会使页面的加载时间延长近50%，增加10%到20%的耗电；\n　　（2）HTTPS连接缓存不如HTTP高效，会增加数据开销和功耗，甚至已有的安全措施也会因此而受到影响；\n　　（3）SSL证书需要钱，功能越强大的证书费用越高，个人网站、小网站没有必要一般不会用。\n　    （4）SSL证书通常需要绑定IP，不能在同一IP上绑定多个域名，IPv4资源不可能支撑这个消耗。\n　　（5）HTTPS协议的加密范围也比较有限，在黑客攻击、拒绝服务攻击、服务器劫持等方面几乎起不到什么作用。最关键的，SSL证书的信用链体系并不安全，特别是在某些国家可以控制CA根证书的情况下，中间人攻击一样可行。\n实践中建议保留http。所以我们在切换的时候可以做http和https的兼容，具体实现方式是，去掉页面链接中的http头部，这样可以自动匹配http头和https头。例如：将http://www.baidu.com改为//www.baidu.com。然后当用户从http的入口进入访问页面时，页面就是http，如果用户是从https的入口进入访问页面，页面即使https的\nSSL与TLS的区别SSL：（Secure Socket Layer，安全套接字层），位于可靠的面向连接的网络层协议和应用层协议之间的一种协议层。SSL通过互相认证、使用数字签名确保完整性、使用加密确保私密性，以实现客户端和服务器之间的安全通讯。该协议由两层组成：SSL记录协议和SSL握手协议。\nTLS：(Transport Layer Security，传输层安全协议)，用于两个应用程序之间提供保密性和数据完整性。该协议由两层组成：TLS记录协议和TLS握手协议。\n如何优化HTTPS的速度1、HSTS重定向技术\nHSTS（HTTP Strict Transport Security）技术，启用HSTS后，将保证浏览器始终连接到网站的 HTTPS 加密版本。\n\n用户在浏览器里输入 HTTP 协议进行访问时，浏览器会自动将 HTTP 转换为 HTTPS 进行访问，确保用户访问安全；\n\n省去301跳转的出现，缩短访问时间；\n\n能阻止基于 SSL Strip 的中间人攻击，万一证书有错误，则显示错误，用户不能回避警告，从而能够更加有效安全的保障用户的访问。\n\n\n2、TLS握手优化\n  在应用数据之前，客户端必须与服务端协商密钥、加密算法等信息，服务端还要把自己的证书发给客户端表明其身份，这些环节构成 TLS 握手过程。\n采用 False Start （抢先开始）技术，浏览器在与服务器完成 TLS 握手前，就开始发送请求数据，服务器在收到这些数据后，完成 TLS 握手的同时，开始发送响应数据。\n开启 False Start 功能后，数据传输时间将进一步缩短。\n3、Session Identifier（会话标识符）复用\n如果用户的一个业务请求包含了多条的加密流，客户端与服务器将会反复握手，必定会导致更多的时间损耗。或者某些特殊情况导致了对话突然中断，双方就需要重新握手，增加了用户访问时间。（1）服务器为每一次的会话都生成并记录一个 ID 号，然后发送给客户端；\n（2）如果客户端发起重新连接，则只要向服务器发送该 ID 号；\n（3）服务器收到客户端发来的 ID 号，然后查找自己的会话记录，匹配 ID 之后，双方就可以重新使用之前的对称加密秘钥进行数据加密传输，而不必重新生成，减少交互时间。\n4、开启OSCP Stapling，提高TLS握手效率\n采用OCSP Stapling ，提升 HTTPS 性能。服务端主动获取 OCSP 查询结果并随着证书一起发送给客户端，从而客户端可直接通过 Web Server 验证证书，提高 TLS 握手效率。\n服务器模拟浏览器向 CA 发起请求，并将带有 CA 机构签名的 OCSP 响应保存到本地，然后在与客户端握手阶段，将 OCSP 响应下发给浏览器，省去浏览器的在线验证过程。由于浏览器不需要直接向 CA 站点查询证书状态，这个功能对访问速度的提升非常明显。\n5、完全前向加密PFS，保护用户数据，预防私钥泄漏\n非对称加密算法 RSA，包含了公钥、私钥，其中私钥是保密不对外公开的，由于此算法既可以用于加密也可以用于签名，所以用途甚广，但是还是会遇到一些问题：\n（1） 假如我是一名黑客，虽然现在我不知道私钥，但是我可以先把客户端与服务器之前的传输数据（已加密）全部保存下来\n（2）如果某一天，服务器维护人员不小心把私钥泄露了，或者服务器被我攻破获取到了私钥\n（3）那我就可以利用这个私钥，破解掉之前已被我保存的数据，从中获取有用的信息\n所以为了防止上述现象发生，我们必须保护好自己的私钥。\n如果私钥确实被泄漏了，那我们改如何补救呢？那就需要PFS（perfect forward secrecy）完全前向保密功能，此功能用于客户端与服务器交换对称密钥，起到前向保密的作用，也即就算私钥被泄漏，黑客也无法破解先前已加密的数据。维基解释是：长期使用的主密钥泄漏不会导致过去的会话密钥泄漏\n 实现此功能需要服务器支持以下算法和签名组合：\n（1）ECDHE 密钥交换、RSA 签名；（2）ECDHE 密钥交换、ECDSA 签名；\n\n\n面试常见问题，HTTPS优化总结易记版：1、HSTS重定向技术：将http自动转换为https，减少301重定向2、TLS握手优化：在TLS握手完成前客户端就提前向服务器发送数据3、会话标识符：服务器记录下与某客户端的会话ID，下次连接客户端发ID过来就可以直接用之前的私钥交流了4、OSCP Stapling：服务器将带有 CA 机构签名的 OCSP 响应在握手时发给客户端，省的客户端再去CA查询5、完全前向加密PFS：使用更牛逼复杂的秘钥算法\n","categories":["django"],"tags":["python"]},{"title":"Django中间件","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/12/04/Django中间件/","content":"什么是中间件中间件就是在目标和结果之间进行的额外处理过程，在Django中就是request和response之间进行的处理，相对来说实现起来比较简单，但是要注意它是对全局有效的，可以在全局范围内改变输入和输出结果，因此需要谨慎使用，否则不仅会造成难以定位的错误，而且可能会影响整体性能。\n中间件有什么用？如果想要修改HttpRequest或者HttpResponse，就可以通过中间件来实现。\n登陆认证：在中间件中加入登陆认证，所有请求就自动拥有登陆认证，如果需要放开部分路由，只需要特殊处理就可以了。\n\n流量统计：可以针对一些渲染页面统计访问流量。\n\n恶意请求拦截：统计IP请求次数，可以进行频次限制或者封禁IP。\n\n\n在Django中自定义中间件:在settings.py中找到MIDDLEWARE项，把添加的中间件配置到这里就行了。例如我在myapp文件夹下(该文件夹与Django文件夹同级)有一个views.py文件，在views.py中有一个叫做MyMiddleware的中间件，那么配置的时候只要在MIDDLEWARE列表中添加一条:1&#x27;myapp.views.MyMiddleware&#x27;\n\n\n每个中间件可以包含五个方法:12345process_request(self,request)process_view(self, request, callback, callback_args, callback_kwargs)process_template_response(self,request,response)process_exception(self, request, exception)process_response(self, request, response)\n\n\n执行流程:\n请求到达中间件后先依次执行每个中间件的process_request函数\n然后再依次执行每个中间件的process_view函数，找到我们的视图函数\n执行视图函数处理请求数据\n如果在上面的过程中出现异常，则依次反方向执行每个中间件的process_exception函数\n如果请求包含模板渲染，则依次反方向执行每个中间件的process_template_response函数\n最后依次反方向执行每个中间件的process_response函数\n\n\n以上这些执行函数将返回None或者HttpResponse对象，如果返回None，则交给下一个中间件的对应函数处理；如果返回HttpResponse对象，则将其返回给用户。应用:12345678910111213class MyMiddleware(MiddlewareMixin):    def process_request(self, request):        print(&#x27;过滤中间件&#x27;)        pass    def process_view(self, request, view_func, view_args, view_kwargs):        pass    def process_exception(self, request, exception):        pass    def process_response(self, request, response):        return response\n\n\n每次请求时，都会打印一行”过滤中间件”","categories":["django"],"tags":["python"]},{"title":"django实现文件上传操作","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/11/30/django实现文件上传操作/","content":"思路：先从前端接受到图片 后台进行重命名/压缩/入库 之后将图片名返回前端 用于展示图片\n\n修改配置settings12345678STATIC_URL = &#x27;/static/&#x27;STATICFILES_DIRS=[     os.path.join(BASE_DIR,&#x27;static&#x27;)]#定义上传文件夹的路径UPLOAD_ROOT = os.path.join(BASE_DIR,&#x27;static/upload&#x27;)\n\n\n前端发送post请求123456789101112131415161718192021222324upload(e) &#123;  //@change=&quot;upload&quot; 侦听操作 e是变化的数据      // 获取文件      let file = e.target.files[0];      // 生名表单参数      let formdata = new FormData();      var uid = localStorage.getItem(&quot;uid&quot;);      formdata.append(&quot;file&quot;, file, file.name);      formdata.append(&quot;uid&quot;, uid);      // 请求头      let config = &#123; headers: &#123; &quot;Content-Type&quot;: &quot;multipart/form-data&quot; &#125; &#125;;      // 请求后台      this.axios        .post(&quot;http://127.0.0.1:8000/myapp/upload/&quot;, formdata, config)        .then(result =&gt; &#123;          console.log(result.data);          if (result.data.code == 200) &#123;            this.src =&quot;http://127.0.0.1:8000/static/upload/&quot; + result.data.filename;            this.$Message(result.data.msg);          &#125; else &#123;            this.$Message(result.data.msg);          &#125;        &#125;);    &#125;\n\n\n后台获取数据1234567为了防止文件名重复 使用了md5和时间模块 进行加密在更改图片之后 我们要删除之前的 防止垃圾过多造成资源浪费加水印函数：\n12345678910# 压缩#压缩 两种压缩模式，png不适合压缩，jpg适合import cv2#读图img = cv2.imread(&#x27;./dingding.png&#x27;)#开始压缩 png压缩等级清晰0-9模糊cv2.imwrite(&#x27;./dingding1.png&#x27;,img,[cv2.IMWRITE_PNG_COMPRESSION,5])#jpg压缩等级清晰0-100模糊 cv2.imwrite(&#x27;./dingding1.jpg&#x27;,img,[cv2.IMWRITE_JPEG_QUALITY,50])\n\n\n1234567891011121314151617181920# 加水印def cn_logo(img_name):    img = Image.open(&#x27;.//static/upload/&#x27;+img_name)    # 获取宽高    width, height = img.size    # 根据图片大小定义logo大小    text = &#x27;美多商城&#x27;    font = ImageFont.truetype(&#x27;msyh.ttc&#x27;, (width - height) // 10)    # 获取字体宽高    font_width, font_height = font.getsize(text)    # 使用画笔    draw = ImageDraw.Draw(img)    # 写入文本    draw.text((width - font_width, height - font_height - 10), text, (100, 100, 100), font=font)    # 加密下文件名防止重复    filename = make_password(img_name) + str((int(round(time.time() * 1000)))) + &quot;.jpg&quot;    # 根据路径保存文件    img.save(os.path.join(UPLOAD_ROOT, &#x27;&#x27;, filename))    os.remove(&#x27;.//static/upload/&#x27; + img_name)  # 删除原图片    return filename\n\n\n文件接口1234567891011121314151617181920212223def post(self,request):    myfile=request.FILES.get(&#x27;file&#x27;)    uid=request.POST.get(&#x27;uid&#x27;)    # 判断文件类型    if not re.match(&#x27;.*(jpg|png|jpeg)$&#x27;, myfile.name):        return Response(&#123;&#x27;code&#x27;: 403,&#x27;msg&#x27;:&#x27;请上传png或者jpg格式&#x27;&#125;)    with open(os.path.join(UPLOAD_ROOT,&#x27;&#x27;,myfile.name),&#x27;wb&#x27;)as f:        for chunk in myfile.chunks():            f.write(chunk)            # 调用水印函数    filename=cn_logo(myfile.name)            # 更改数据库    try:        user=User.objects.filter(pk=uid).first()        if user.img:  # 删除旧图片           os.remove(&#x27;.//static/upload/&#x27;+user.img)           user.img=filename           user.save()        except Exception as e:            print(e)            pass \treturn Response(&#123;&quot;code&quot;:200,&quot;msg&quot;:&quot;更改成功&quot;,&#x27;filename&#x27;:filename&#125;)\n\n\n12345上面进行了 更改数据库 删除文件 写入文件那么如何进行展示呢？写一个get方法 根据用户返回图片名\n\n12345678def get(self,request):    # 获取参数    uid=request.GET.get(&#x27;uid&#x27;,None)    user=User.objects.filter(pk=uid).first()    if user.img:        return Response(&#123;&quot;code&quot;:200,&#x27;filename&#x27;:user.img&#125;)    else:        return Response(&#123;&quot;code&quot;:200,&#x27;filename&#x27;:&#x27;user.png&#x27;&#125;)\n\n\n前端进行get请求12345678get_avatar() &#123;    var uid = localStorage.getItem(&quot;uid&quot;);    this.axios.get(&quot;http://127.0.0.1:8000/myapp/upload/&quot;, &#123;        params: &#123; &#x27;uid&#x27;: uid &#125;    &#125;).then(result =&gt; &#123;        this.src =&quot;http://127.0.0.1:8000/static/upload/&quot; + result.data.filename;    &#125;);&#125;\n\n1这里将src 拼接为图片路径之后用img 标签就可以显示了","categories":["django"],"tags":["python"]},{"title":"django配置","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/11/27/django配置/","content":"1.django配置(settings.py)12345678910111213DATABASES = &#123;   &#x27;default&#x27;: &#123;          &#x27;ENGINE&#x27;: &#x27;django.db.backends.mysql&#x27;,   #数据库引擎          &#x27;NAME&#x27;: &#x27;md&#x27;,                          #数据库名          &#x27;USER&#x27;: &#x27;root&#x27;,                         #用户名          &#x27;PASSWORD&#x27;: &#x27;&#x27;,                         #密码--根据自身数据库密码填写          &#x27;HOST&#x27;: &#x27;localhost&#x27;,                    #数据库主机，默认为localhost          &#x27;PORT&#x27;: 3306,                           #数据库端口，MySQL默认为3306          &#x27;OPTIONS&#x27;: &#123;             &#x27;autocommit&#x27;: True,         &#125;    &#125;&#125;\n\n\n2.运行项目1python manage.py runserver\n\n3.django的数据库迁移生成1python manage.py makemigrations myapp\n\n注意：首次迁移后面写myapp，以后不用写迁移1python manage.py migrate\n\n\n3.配置跨域12345678910111213MIDDLEWARE = [    &#x27;django.middleware.security.SecurityMiddleware&#x27;,    &#x27;django.contrib.sessions.middleware.SessionMiddleware&#x27;,    &#x27;corsheaders.middleware.CorsMiddleware&#x27;, #这里是新增的中间件    &#x27;django.middleware.common.CommonMiddleware&#x27;,    #&#x27;django.middleware.csrf.CsrfViewMiddleware&#x27;,    &#x27;django.contrib.auth.middleware.AuthenticationMiddleware&#x27;,    &#x27;django.contrib.messages.middleware.MessageMiddleware&#x27;,    &#x27;django.middleware.clickjacking.XFrameOptionsMiddleware&#x27;,    #加载自定义中间件   文件夹名.文件名.类名    #&#x27;myapp.views.MyMiddleware&#x27;]\n设置跨域12CORS_ALLOW_CREDENTIALS = TrueCORS_ORIGIN_ALLOW_ALL = True\n\n定义上传文件夹的路径1UPLOAD_ROOT = os.path.join(BASE_DIR,&#x27;static/upload&#x27;)\n\n\n","categories":["django"],"tags":["python"]},{"title":"django创建表","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/11/27/django创建表/","content":"在settings.py配置好的前提下，打开models.py创建基类(增加代码的复用)12345678910111213141516171819202122232425262728293031#导包from django.utils import timezone#基类class Base(models.Model):    #创建时间     create_time = models.DateTimeField(default=timezone.now,null=True)    class Meta:        #能否被继承        abstract = True#用户类class User(Base):    #  用户名    username = models.CharField(max_length=200)    #  密码    password = models.CharField(max_length=200)    #  头像    img = models.CharField(max_length=200)    #分类 0普通用户 1会员    type = models.IntegerField()    #  主页    num = models.IntegerField()    #  联系方式    phone = models.CharField(max_length=200)    #  声明表名    class Meta:        db_table = &#x27;user&#x27;\n\n\n数据库迁移12python manage.py makemigrations myapppython manage.py migrate\n\n如果迁移不成功\n删除myapp下的migrations\n通过可视化工具Navicat进行物理删除，及删除数据库中的所有表（有时候删一次删不彻底，记得多重复几次物理删除操作）\n重新建立数据库关系映射，迁移数据库\n\nMysql 用Navicat手动建表格手动建表格，输入反向数据库入库命令\n1python manage.py inspectdb &gt; mymodel.py\n\n\n","categories":["django"],"tags":["python"]},{"title":"又拍云存储","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/11/27/又拍云存储/","content":"安装1pip install upyun\n\ndjango接口123456789101112131415161718#又拍云import upyun#定义文件上传类class YouPai(View):    def post(self,request):        #接受参数        img = request.FILES.get(&#x27;file&#x27;)        # 生成独特的文件名        filename = make_password(img.name) + str((int(round(time.time() * 1000)))) + &quot;.jpg&quot;        # 实例化对象        up = upyun.UpYun(&#x27;md-admin&#x27;, username=&#x27;a2925087209&#x27;, password=&#x27;by0sFyYQ62GfZSIqCPVRXJLHlierRJsT&#x27;)        headers = &#123; &#x27;x-gmkerl-rotate&#x27;: &#x27;50&#x27; &#125;        # 分块上传        for chunk in img.chunks():            res = up.put(&#x27;/%s&#x27;%filename,chunk,checksum=True,headers=headers)        #返回结果        return HttpResponse(json.dumps(&#123;&#x27;filename&#x27;:filename&#125;),content_type=&#x27;application/json&#x27;)\n\n\nvue接口1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&lt;template&gt;  &lt;div&gt;    &lt;div&gt;        &lt;!-- &lt;/img :src=&quot;src&quot; /&gt; --&gt;        又拍云存储：&lt;Avatar :src=&quot;yp_url&quot; :width=&#x27;150&#x27; fil=&#x27;fill&#x27;&gt;&lt;/Avatar&gt;    &lt;/div&gt;    &lt;div&gt;        &lt;input type=&quot;file&quot; @change=&quot;upload_upyun&quot; /&gt;        &lt;div class=&quot;upload&quot;&gt;            拖拽上传        &lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123;    data () &#123;        return &#123;            yp_url :&#x27;&#x27;,        &#125;    &#125;    mounted:function()&#123;    //又拍云    //注册推托拽容器    let upload = document.querySelector(&#x27;.upyun&#x27;);    //声明监听事件    //点击    upload.addEventListener(&#x27;dragenter&#x27;, this.onDrag, false);    //悬停    upload.addEventListener(&#x27;dragover&#x27;, this.onDrag, false);    //释放    upload.addEventListener(&#x27;drop&#x27;, this.onDrop, false);    &#125;,    methods:&#123;        //又拍云上传        //监听鼠标        onDrag (e) &#123;        e.stopPropagation();        e.preventDefault();        &#125;,        onDrop (e) &#123;        e.stopPropagation();        e.preventDefault();        this.upyun(e.dataTransfer.files);        &#125;,        //上传又拍云        upyun:function(files)&#123;            //获取文件对象        //   let file = e.target.files[0];            let file = files[0];            //声明参数            let param = new FormData();            param.append(&#x27;file&#x27;,file);            const config = &#123;            headers: &#123; &#x27;Content-Type&#x27;: &#x27;multipart/form-data&#x27; &#125;            &#125;             var a = this            a.axios.post(&#x27;http://localhost:8000/youpai/&#x27;, param, config)// 上传图片            .then(function(res) &#123;                console.log(res)\t                a.yp_url = &#x27;http://md-admin.test.upcdn.net/&#x27;+res.data.filename            &#125;);        &#125;,    &#125;,&#125;&lt;/script&gt;&lt;style&gt;.upload &#123;  margin: 100px auto;  width: 300px;  height: 150px;  border: 2px dashed #f00;  padding-top: 50px;  padding-left: 80px;&#125;&lt;/style&gt;\n\n又拍云文件操作1234567891011121314151617181920212223import upyun#实例化up = upyun.UpYun(&quot;空间名&quot;,&#x27;操作员&#x27;,&#x27;密码&#x27;)#读内存上传up.put(&#x27;自定义文件名&#x27;,&#x27;文件内容&#x27;)#文件流操作（节省内存）with open(&#x27;文件路径/文件名&#x27;,&#x27;rb&#x27;)as f:    res = up.put(&#x27;自定义文件名&#x27;,f,checksum=True)#目录操作up.mkdir(&#x27;/文件夹名/&#x27;)#移动文件up.move(&#x27;/文件路径/文件名&#x27;,&#x27;/新文件路径/文件名&#x27;)#复制文件up.copy(&#x27;/文件路径/文件名&#x27;,&#x27;/新文件路径/文件名&#x27;)#断点续传with open(&#x27;文件路径/文件名&#x27;,&#x27;rb&#x27;)as f:    res = up.put(&#x27;自定义文件名&#x27;,f,checksum=True，need_resume=True)#下载res = up.get(&#x27;/文件路径/文件名&#x27;)#删除up.delete(&#x27;/文件路径/文件名&#x27;)\n","categories":["django"],"tags":["python"]},{"title":"一次完整的HTTP请求过程","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/11/20/一次完整的HTTP请求过程/","content":"问题：当我们在web浏览器的地址栏中输入：www.baidu.com，具体发生了什么？概述\n对www.baidu.com这个网址进行DNS域名解析，得到对应的IP地址\n根据这个IP，找到对应的服务器，发起TCP的三次握手\n建立TCP连接后发起HTTP请求\n服务器响应HTTP请求，浏览器得到html代码\n浏览器解析html代码，并请求html代码中的资源（如js、css、图片等）（先得到html代码，才能去找这些资源）\n浏览器对页面进行渲染呈现给用户\n服务器关闭关闭TCP连接\n\n注意：1.DNS怎么找到域名的？1DNS域名解析采用的是递归查询的方式，过程是，先去找DNS缓存-&gt;缓存找不到就去找根域名服务器-&gt;根域名又会去找下一级，这样递归查找之后，找到了，给我们的web浏览器\n2.为什么HTTP协议要基于TCP来实现？1TCP是一个端到端的可靠的面相连接的协议，HTTP基于传输层TCP协议不用担心数据传输的各种问题（当发生错误时，会重传）\n\n3.最后一步浏览器是如何对页面进行渲染的？1234a)  解析html文件构成 DOM树b）解析CSS文件构成渲染树c）边解析，边渲染d）JS 单线程运行，JS有可能修改DOM结构，意味着JS执行完成前，后续所有资源的下载是没有必要的，所以JS是单线程，会阻塞后续资源下载\n\n各个步骤具体细节DNS解析（域名解析服务器）a）首先会搜索浏览器自身的DNS缓存（缓存时间比较短，大概只有1分钟，且只能容纳1000条缓存）\nb）如果浏览器自身的缓存里面没有找到，那么浏览器会搜索系统自身的DNS缓存\nc）如果还没有找到，那么尝试从 hosts文件里面去找\nd）在前面三个过程都没获取到的情况下，就递归地去域名服务器去查找，具体过程如下\nDNS优化两个方面：DNS缓存、DNS负载均衡TCP连接建立（三次握手）　　拿到域名对应的IP地址之后，User-Agent（一般指浏览器）会以一个随机端口（1024&lt;端口&lt;65535）向服务器的WEB程序（常用的有httpd，nginx）等的80端口。这个连接请求（原始的http请求经过TCP/IP4层模型的层层封包）到达服务器端后（这中间有各种路由设备，局域网内除外），进入到网卡，然后是进入到内核的TCP/IP协议栈（用于识别连接请求，解封包，一层一层的剥开），还有可能要经过Netfilter防火墙（属于内核的模块）的过滤，最终达到WEB程序，最终建立了TCP/IP的连接\n发起HTTP请求(建立连接后)HTTP请求报文由三部分组成：请求行，请求头、空格、请求正文\n请求行：用于描述客户端的请求方式（GET/POST等），请求的资源名称(URL)以及使用的HTTP协议的版本号\n请求头：用于描述客户端请求哪台主机及其端口，以及客户端的一些环境信息等\n空行：空行就是\\r\\n (POST请求时候有)\n请求正文：当使用POST等方法时，通常需要客户端向服务器传递数据。这些数据就储存在请求正文中（GET方式是保存在url地址后面，不会放到这里）\nGET请求下面是浏览器对 http://localhost:8081/test?name=XXG&amp;age=23的GET 请求时发送给服务器的数据：\n可以看出请求包含请求行和请求头两部分。其中请求行中包含 method（例如 GET、POST）、URI（通一资源标志符）和协议版本三部分，三个部分之间以空格分开。请求行和每个请求头各占一行，以换行符 CRLF（即 \\r\\n）分割。\nPOST请求下面是浏览器对 http://localhost:8081/test 的 POST 请求时发送给服务器的数据，消息体中带上参数 name=XXG&amp;age=23可以看出，上面的请求包含三个部分：请求行、请求头、空格、消息体，比之前的 GET 请求多了一个请求消息，其中 请求头和消息体之间用一个空行分割。POST 请求的参数不在 URL 中，而是在消息体中，请求头中多了一项 Content-Length 用于表示消息体的字节数，这样服务器才能知道请求是否发送结束。这也就是 GET 请求和 POST 请求的主要区别。\n那么起始行中的请求方法有哪些种呢？\n1234567GET: 完整请求一个资源 （常用）  HEAD: 仅请求响应首部  POST：提交表单  （常用）  PUT: (webdav) 上传文件（但是浏览器不支持该方法）  DELETE：(webdav) 删除  OPTIONS：返回请求的资源所支持的方法的方法  TRACE: 追求一个资源请求中间所经过的代理（该方法不能由浏览器发出）\n那什么是URL、URI、URN？\n123URI  Uniform Resource Identifier 统一资源标识符URL  Uniform Resource Locator 统一资源定位符URN  Uniform Resource Name 统一资源名称\nURL和URN 都属于 URI，为了方便就把URL和URI暂时都通指一个东西\n服务器响应http请求，浏览器得到html代码HTTP响应也由三部分组成：状态行，响应头，空格，消息体\n状态行包括：协议版本、状态码、状态码描述\n状态码：状态码用于表示服务器对请求的处理结果\n123451xx：指示信息——表示请求已经接受，继续处理2xx：成功——表示请求已经被成功接收、理解、接受。3xx：重定向——要完成请求必须进行更进一步的操作4xx：客户端错误——请求有语法错误或请求无法实现5xx：服务器端错误——服务器未能实现合法的请求。\n列举几种常见的：\n1234567200（没有问题） 302（要你去找别人） 304（要你去拿缓存） 307（要你去拿缓存） 403（有这个资源，但是没有访问权限） 404（服务器没有这个资源） 500（服务器这边有问题）\n\n响应头：响应头用于描述服务器的基本信息，以及客户端如何处理数据\n空格：CRLF（即 \\r\\n）分割\n消息体：服务器返回给客户端的数据\n响应格式如下图上面的 HTTP 响应中，响应头中的 Content-Length 同样用于表示消息体的字节数。Content-Type 表示消息体的类型，通常浏览网页其类型是HTML，当然还会有其他类型，比如图片、视频等。\n浏览器解析html代码，并请求html代码中的资源浏览器拿到html文件后，就开始解析其中的html代码，遇到js/css/image等静态资源时，就向服务器端去请求下载（会使用多线程下载，每个浏览器的线程数不一样），这是时候就用上 keep-alive特性了，建立一次HTTP连接，可以请求多个资源，下载资源的顺序就是按照代码里面的顺序，但是由于每个资源大小不一样，而浏览器又是多线程请求请求资源，所以这里显示的顺序并不一定是代码里面的顺序。\n浏览器对页面进行渲染呈现给用户最后，浏览器利用自己内部的工作机制，把请求的静态资源和html代码进行渲染，渲染之后呈现给用户，浏览器是一个边解析边渲染的过程。首先浏览器解析HTML文件构建DOM树，然后解析CSS文件构建渲染树，等到渲染树构建完成后，浏览器开始布局渲染树并将其绘制到屏幕上。这个过程比较复杂，涉及到两个概念: reflow(回流)和repain(重绘)。DOM节点中的各个元素都是以盒模型的形式存在，这些都需要浏览器去计算其位置和大小等，这个过程称为relow;当盒模型的位置,大小以及其他属性，如颜色,字体,等确定下来之后，浏览器便开始绘制内容，这个过程称为repain。页面在首次加载时必然会经历reflow和repain。reflow和repain过程是非常消耗性能的，尤其是在移动设备上，它会破坏用户体验，有时会造成页面卡顿。所以我们应该尽可能少的减少reflow和repain。JS的解析是由浏览器中的JS解析引擎完成的。JS是单线程运行，JS有可能修改DOM结构，意味着JS执行完成前，后续所有资源的下载是没有必要的，所以JS是单线程，会阻塞后续资源下载。\n服务器关闭关闭TCP连接一般情况下，一旦Web服务器向浏览器发送了请求数据，它就要关闭TCP连接，然后如果浏览器或者服务器在其头信息加入了这行代码：\n1Connection:keep-alive \nTCP连接在发送后将仍然保持打开状态，于是，浏览器可以继续通过相同的连接发送请求。保持连接节省了为每个请求建立新连接所需的时间，还节约了网络带宽。\n自此一次完整的HTTP事务宣告完成.转载于：https://www.cnblogs.com/WindSun/p/11489356.html","categories":["django"],"tags":["python"]},{"title":"验证码设置&检测","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/10/26/验证码设置&检测/","content":"django设置验证码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101#导入图片库#绘画库from PIL import ImageDraw#字体库from PIL import ImageFont#图片库from PIL import Image#随机库import random#文件流import io#导包import redis#定义ip和端口号host = &#x27;localhost&#x27;port = 6379#建立连接r = redis.Redis(host=host,port=port)#自定义图片验证码class MyCode(View):    #定义rgb随机颜色获取    def get_random_color(self):        R = random.randrange(255)        G = random.randrange(255)        B = random.randrange(255)        return (R,G,B)    def get(self,request):        #定义画布        img_size = (120,50)#宽120px，高50px        #定义图片对象        image = Image.new(&#x27;RGB&#x27;,img_size,&#x27;white&#x27;)#RGB的颜色模式 #画布 #背景颜色为白色        #定义画笔        draw = ImageDraw.Draw(image,&#x27;RGB&#x27;)  #图片对象  #参数mode：RGB颜色模式        #画笔内容        source = &#x27;0123456789abcdefghijklmnopqrstuvwxyz&#x27;#内容为数字，字母，特殊符号，增加安全性        #接收容器        code_str = &#x27;&#x27;        #定义字体，在本地路径下,size字体大小        myfont = ImageFont.truetype(font=&#x27;C:\\\\Windows\\\\Fonts\\\\Arial.ttf&#x27;,size=25)        for i in range(4):            #获取字母颜色            text_color = self.get_random_color()            #获取随机下标            tmp_num = random.randrange(len(source))            #随机验证码内容            random_str = source[tmp_num]            #装入容器            code_str += random_str            #绘制验证码            # 参数(横坐标，纵坐标20),#验证码内容#验证码颜色#字体            draw.text((10+30*i,10),random_str,text_color,font=myfont)        #获取缓冲区        buf = io.BytesIO()        #将临时图片保存到缓冲区        image.save(buf,&#x27;png&#x27;)        #保存随机验证码        r.set(&#x27;code&#x27;,code_str)        #保存session到数据库中的django_session        print(&#x27;222&#x27;,r.get(&#x27;code&#x27;))        # request.session[&#x27;code&#x27;] = code_str        #返回获取缓冲区的图片        return HttpResponse(buf.getvalue(),&#x27;image/png&#x27;)\n\n\nvue展示验证码1234567891011121314151617181920212223242526272829303132333435363738&lt;template&gt;  &lt;div&gt;    &lt;!-- 图片验证码 --&gt;    &lt;img :src=&quot;src&quot; alt=&quot;点击刷新&quot; @click=&quot;img_code&quot; class=&quot;imgcode&quot;&gt;  &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123;  data () &#123;    return &#123;\t  src:&#x27;http://127.0.0.1:8000/code/&#x27;,    &#125;  &#125;,  methods:&#123;    //刷新验证码\timg_code()&#123;\t\t//给图片网址加一个随机参数，可以点击刷新验证码\t\tvar lsrc = &#x27;http://127.0.0.1:8000/code/&#x27;+&#x27;?&#x27;+Math.random()\t\tthis.src = lsrc\t\t//var num = Math.ceil(Math.random()*10)\t\t// this.src = this.src + &#x27;?num=&#x27;+num\t&#125;  &#125;&#125;&lt;/script&gt;&lt;style&gt;.imgcode &#123;    /* 给图片点击鼠标更换小手样式 */\tcursor: pointer;&#125;&lt;/style&gt;\n\n\nApi检测验证码(百度AI)1234567891011121314151617181920212223242526272829303132333435363738394041421.创建百度账号或者登陆百度账号2.选择总览--&gt;人工智能--&gt;文字识别--&gt;创建一个应用3.输入应用名,选择不需要文字识别包名--&gt;输入描述--&gt;点击创建(会有API Key和Secret Key)4.在左边目录中点击技术文档--&gt;点击API文档，立即使用5.选择通用文字识别--&gt;通用文字识别(高精度版)--&gt;点击Access token获取6.获取Access token：    1.向授权服务地址https:&#x2F;&#x2F;aip.baidubce.com&#x2F;oauth&#x2F;2.0&#x2F;token发送请求（推荐使用POST),并在URL中带上以下参数        grant_type： 必须参数，固定为client_credentials；        client_id： 必须参数，应用的API Key；        client_secret： 必须参数，应用的Secret Key；    2.url &#x3D; https:&#x2F;&#x2F;aip.baidubce.com&#x2F;oauth&#x2F;2.0&#x2F;token?      grant_type&#x3D;client_credentials&amp;client_id&#x3D;百度云应用的API Key&amp;client_secret&#x3D;百度云应用的Secret Key    3.使用json()获取：      &#123;        &quot;refresh_token&quot;: &quot;25.b55fe1d287227ca97aab219bb249b8ab.315360000.1798284651.282335-8574074&quot;,        &quot;expires_in&quot;: 2592000,  # Access Token的有效期(秒为单位，一般为1个月)        &quot;scope&quot;: &quot;public wise_adapt&quot;,        &quot;session_key&quot;: &quot;9mzdDZXu3dENdFZQurfg0Vz8slgSgvvOAUebNFzyzcpQ5EnbxbF+hfG9DQkpUVQdh4p6HbQcAiz5RmuBAja1JJGgIdJI&quot;,        &quot;access_token&quot;: &quot;24.6c5e1ff107f0e8bcef8c46d3424a0e78.2592000.1485516651.282335-8574074&quot;,  #要获取的Access Token        &quot;session_secret&quot;: &quot;dfac94a3489fe9fca7c3221cbf7525ff&quot;      &#125;    4.关于报错信息：      &#123;        error： 错误码；关于错误码的详细信息请参考下方鉴权认证错误码。        error_description： 错误描述信息，帮助理解和解决发生的错误。      &#125;      &#123;        error(错误码)     error_description(错误描述信息)\t  解释        invalid_client\t unknown client id\t              API Key不正确        invalid_client\t Client authentication failed\t    Secret Key不正确      &#125;7.访问url：&#39;https:&#x2F;&#x2F;aip.baidubce.com&#x2F;rest&#x2F;2.0&#x2F;ocr&#x2F;v1&#x2F;accurate_basic?access_token&#x3D;&#39; + access_token8.HTTP方法：POST9.Header参数：&#123;&#39;Content-Type&#39;:&#39;application&#x2F;x-www-form-urlencoded&#39;&#125;10.body必要参数：image   说明：ase64编码后进行urlencode11.返回结果：&#123;              log_id：唯一的log id，用于问题定位,              words_result:\t识别结果数组,              words_result_num:\t识别结果数，表示words_result的元素个数,              words：识别结果字符串,            &#125;\n\n\n代码123456789101112131415161718192021222324252627282930313233343536373839404142434445import requestsimport base64import urllib#访问urlres = requests.post(&#x27;https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&amp;client_id=jHUoq7NLRtijZz4RaVUHxnZC&amp;client_secret=jqBWDRAkrWlB3mKzjkGYRoTxW8fgpW5m&#x27;)#获取access_tokenaccess_token = res.json()[&#x27;access_token&#x27;]print(access_token)#开始智能识图#接口地址，访问ulrurl = &#x27;https://aip.baidubce.com/rest/2.0/ocr/v1/accurate_basic?access_token=&#x27;+access_token#定义请求头headers = &#123;    &quot;Content-Type&quot;:&#x27;application/x-www-form-urlencoded&#x27;&#125;#操作图片#读取图片myimg = open(&#x27;../code.png&#x27;,&#x27;rb&#x27;)img_temp = myimg.read()myimg.close()#对图片进行base64编码temp_data = &#123;    &#x27;image&#x27;:base64.b64encode(img_temp)&#125;#对图片地址进行urlencode操作temp_data = urllib.parse.urlencode(temp_data)#发起post请求res = requests.post(url=url,headers=headers,data=temp_data)res_list = res.json()print(res_list)for i in res_list[&#x27;words_result&#x27;]:    code = i[&#x27;words&#x27;].replace(&#x27; &#x27;,&quot;&quot;)    print(code)\n\n\n","categories":["django"],"tags":["python"]},{"title":"序列化底层","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/10/23/序列化底层/","content":"DRF的核心 就是 前后端分离的核心前后端分离开发的核心： 将模型转换为json 称之为 序列化 将json转换为模型 称之为 反序列化制作序列化类12345678from rest_framework import serializersfrom myapp.models import *class userlSer(serializers.ModelSerializer):    class Meta:        model=User        fields = &#x27;__all__&#x27;\n\n\n序列化底层是根据 sql语句查出的结果集在进行json格式化我们就来实现一下原理\n123456789101112131415161718# 导入原生sql模块from django.db import connection# 搜索接口class Search(APIView):    def get(self,request):        # 检索字段        text = eval(request.GET.get(&#x27;text&#x27;,None))        # 建立游标对象        cursor=connection.cursor()        # 执行sql语句        cursor.execute(&quot;select * from goods where name like &#x27;%%%s%%&#x27;&quot; %text)        # 查询        res=dictfetch(cursor)        # 判断长度        count=len(res)        return Response(&#123;&#x27;msg&#x27;:text,&quot;data&quot;:res,&#x27;total&#x27;:count&#125;)\n\n在执行查询的部分 我们调用了dictfetch 函数   因为游标查询出的数据是元祖形式 但是与前端类型不符合   函数内的逻辑就是 将查询出的数据 进行列表嵌套键值对的二次重组12345678#  格式化结果集def dictfetch(cursor):    # 声明描述符 description获取字段名    desc= cursor.description    # 重组结果    return [dict(zip([col[0] for col in desc],row))        for row in cursor.fetchall()    ]\nzip()函数将可迭代对象作为参数，并打包成元组    之后在使用dict将tuple 转换为键值对12[(&#x27;name&#x27;, &#x27;2&#x27;)]","categories":["django"],"tags":["python"]},{"title":"序列化的使用","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/10/13/序列化的使用/","content":"rest_framework的下载1pip install djangorestframework\n\n在settings中注册rest_framework1234567891011121314INSTALLED_APPS = [    &#x27;django.contrib.admin&#x27;,    &#x27;django.contrib.auth&#x27;,    &#x27;django.contrib.contenttypes&#x27;,    &#x27;django.contrib.sessions&#x27;,    &#x27;django.contrib.messages&#x27;,    &#x27;django.contrib.staticfiles&#x27;,    &#x27;corsheaders&#x27;,    #后端跨域    &#x27;rest_framework&#x27;,   #rest_framework框架    &#x27;myapp&#x27;,    &#x27;dwebsocket&#x27;]\n\n\n在django的app中创建一个serializers.py文件123456789101112131415161718#导包from rest_framework import serializers#导入需要序列化的表from myapp.models import *#建立序列化类class CarouselSer(serializers.ModelSerializer):    #针对表进行序列化    class Meta:        model = Carousel #表名        fields = &#x27;__all__&#x27;    #所有字段\n\n\n在views视图中123456789101112from myapp.myser import *  #导入序列器class GetCarousel(APIView):    def get(self,request):        #读库        carousels = Carousel.objects.all()        #序列化操作        carousels_ser = CarouselSer(carousels,many=True)        return Response(carousels_ser.data)\n\n\n使用序列化的增删改查操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546class GetCarousel(APIView):    #查询操作    def get(self,request):        #读库        carousels = Carousel.objects.all()        #序列化操作        carousels_ser = CarouselSer(carousels,many=True)        return Response(&#123;&#x27;data&#x27;:carousels_ser.data&#125;)    #添加操作    def post(self,request):        name = request.data.get(&#x27;name&#x27;,None)        src = request.data.get(&#x27;link&#x27;,None)        img = request.data.get(&#x27;imgs&#x27;,None)        ser = Carousel.objects.filter(name=name).first()        if ser:            return Response(&#123;&#x27;msg&#x27;:&#x27;幻灯片已存在&#x27;&#125;)        carousel = Carousel(name=name,src=src,img=img)        carousel.save() #保存入库        return Response(&#123;&#x27;code&#x27;: 200, &#x27;msg&#x27;: &#x27;添加成功&#x27;&#125;)    #删除操作    def delete(self,request):        id = request.GET.get(&#x27;id&#x27;,None)        Carousel.objects.filter(id=id).delete()        return Response(&#123;&#x27;code&#x27;: 200, &#x27;msg&#x27;: &#x27;删除成功&#x27;&#125;)    #修改操作    def put(self,request):        id = request.data.get(&#x27;id&#x27;,None)        name = request.data.get(&#x27;name&#x27;,None)        src = request.data.get(&#x27;link&#x27;,None)        img = request.data.get(&#x27;imgs&#x27;,None)        car = Carousel.objects.filter(id=id).first()        car.name = name        car.src = src        car.img = img        car.save()        return Response(&#123;&#x27;code&#x27;:200, &#x27;msg&#x27;: &#x27;修改成功&#x27;&#125;)\nrest_framework自带增删改查四种方法1234查询：get：Carousel.objects.all() ---&gt;method:&#39;GET&#39;增加：post：carousel.save()       ---&gt;method:&#39;POST(data)&#39;,request.data.get()删除：delete：Carousel.objects.filter(id&#x3D;id).delete() ---&gt;method:&#39;DELETE(params)&#39;,request.GET.get()修改：put：carousel.save()        ---&gt;method:&#39;PUT(data)&#39;,request.data.get()\n\n\nvue代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179&lt;template&gt;    &lt;div&gt;        &lt;center&gt;            &lt;table border=&quot;1&quot; style=&quot;text-align: center;&quot;&gt;                &lt;tr&gt;                    &lt;td&gt;                        名称                    &lt;/td&gt;                    &lt;td&gt;                        链接                    &lt;/td&gt;                    &lt;td&gt;                        图片                    &lt;/td&gt;                    &lt;td&gt;                        添加                    &lt;/td&gt;                    &lt;td&gt;                        修改                    &lt;/td&gt;                    &lt;td&gt;                        删除                    &lt;/td&gt;                &lt;/tr&gt;                &lt;tr v-for=&quot;(item,index) in lunbo_list&quot; :key=&quot;index&quot;&gt;                    &lt;td&gt;                        &amp;emsp;&amp;emsp;            &#123;&#123;item.name&#125;&#125;           &amp;emsp;&amp;emsp;                    &lt;/td&gt;                    &lt;td&gt;                        &amp;emsp;&amp;emsp;        &lt;a :href=&quot;item.src&quot; target=&quot;_black&quot;&gt;&#123;&#123;item.src&#125;&#125;&lt;/a&gt;        &amp;emsp;&amp;emsp;                    &lt;/td&gt;                    &lt;td&gt;                        &amp;emsp;&amp;emsp;        &lt;img :src=&quot;item.img&quot; alt=&quot;&quot;&gt;        &amp;emsp;&amp;emsp;                    &lt;/td&gt;                    &lt;td&gt;                        &amp;emsp;&amp;emsp;        &lt;Button color=&quot;green&quot; @click=&quot;put_lunbo(item.id)&quot;&gt;修改&lt;/Button&gt;     &amp;emsp;&amp;emsp;                    &lt;/td&gt;                    &lt;td&gt;                        &amp;emsp;&amp;emsp;        &lt;Button color=&quot;red&quot; @click=&quot;del_lunbo(item.id)&quot;&gt;删除&lt;/Button&gt;       &amp;emsp;&amp;emsp;                    &lt;/td&gt;            &lt;/tr&gt;        &lt;/table&gt;        &lt;br&gt;        &lt;table&gt;            &lt;tr&gt;                &lt;td&gt;                    name：&lt;input type=&quot;text&quot; v-model=&quot;name&quot;&gt;                &lt;/td&gt;            &lt;/tr&gt;            &lt;tr&gt;                &lt;td&gt;                    img：&lt;input type=&quot;text&quot; v-model=&quot;imgs&quot;&gt;                &lt;/td&gt;            &lt;/tr&gt;            &lt;tr&gt;                &lt;td&gt;                    src：&lt;input type=&quot;text&quot; v-model=&quot;link&quot;&gt;                &lt;/td&gt;            &lt;/tr&gt;            &lt;br&gt;            &lt;tr&gt;                    &amp;emsp;&amp;emsp;        &lt;Button color=&quot;blue&quot; @click=&quot;add_lunbo&quot;&gt;添加&lt;/Button&gt;      &amp;emsp;&amp;emsp;            &lt;/tr&gt;        &lt;/table&gt;    &lt;/table&gt;    &lt;br&gt;    &lt;table&gt;        &lt;tr&gt;            &lt;td&gt;                name：&lt;input type=&quot;text&quot; v-model=&quot;name&quot;&gt;            &lt;/td&gt;        &lt;/tr&gt;        &lt;tr&gt;            &lt;td&gt;                img：&lt;input type=&quot;text&quot; v-model=&quot;imgs&quot;&gt;            &lt;/td&gt;        &lt;/tr&gt;        &lt;tr&gt;            &lt;td&gt;                src：&lt;input type=&quot;text&quot; v-model=&quot;link&quot;&gt;            &lt;/td&gt;        &lt;/tr&gt;        &lt;br&gt;        &lt;tr&gt;                &amp;emsp;&amp;emsp;        &lt;Button color=&quot;blue&quot; @click=&quot;put_lunbo1&quot;&gt;修改&lt;/Button&gt;      &amp;emsp;&amp;emsp;        &lt;/tr&gt;    &lt;/table&gt;    &lt;/center&gt;    &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123;    data()&#123;        return&#123;            lunbo_list:[],            name:&quot;&quot;,            link:&quot;&quot;,            imgs:&quot;&quot;,        &#125;    &#125;,    mounted() &#123;        this.lunbo();    &#125;,    methods: &#123;        lunbo()&#123;            this.axios(&#123;                url:&#x27;http://localhost:8000/getcarousel/&#x27;,                method:&#x27;GET&#x27;,            &#125;).then(resp=&gt;&#123;                console.log(resp)                this.lunbo_list = resp.data.data            &#125;)        &#125;,        //增加        add_lunbo()&#123;             this.axios(&#123;                url:&#x27;http://127.0.0.1:8000/getcarousel/&#x27;,                method:&#x27;POST&#x27;,                data:&#123;                    name:this.name,                    link:this.link,                    imgs:this.imgs,                &#125;            &#125;).then(resp=&gt;&#123;                console.log(resp)                              this.$router.go(0)            &#125;)        &#125;,        //删除        del_lunbo(id)&#123;            this.axios(&#123;                url:&#x27;http://127.0.0.1:8000/getcarousel/&#x27;,                method:&#x27;DELETE&#x27;,                params:&#123;                    id:id                &#125;            &#125;).then(resp=&gt;&#123;                console.log(resp)                alert(resp.data.msg)                this.$router.go(0)            &#125;)        &#125;,        //修改跳转        put_lunbo(id)&#123;            //网页跳转传递id            this.$router.push(&#123;&#x27;path&#x27;:&#x27;/my_lunbo&#x27;,query:&#123;&#x27;aid&#x27;:id&#125;&#125;)        &#125;,        //修改        put_lunbo1()&#123;            this.axios(&#123;                url:&#x27;http://127.0.0.1:8000/getcarousel/&#x27;,                method:&#x27;PUT&#x27;,//指定修改方法                data:&#123;                    id:this.$route.query.aid,                    name:this.name,                    link:this.link,                    imgs:this.imgs,                &#125;            &#125;).then(resp=&gt;&#123;                console.log(resp)                alert(resp.data.msg)                this.$router.go(0)            &#125;)        &#125;    &#125;,&#125;&lt;/script&gt;&lt;style&gt;img&#123;    width: 100px;    height: 100px;&#125;&lt;/style&gt;\n","categories":["django"],"tags":["python"]},{"title":"使用python发送QQ邮件","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/10/10/使用python发送QQ邮件/","content":"这里用到了Python的两个包来发送邮件： smtplib 和 email 。Python 的 email 模块里包含了许多实用的邮件格式设置函数，可以用来创建邮件“包裹”。使用的 MIMEText 对象，为底层的 MIME（Multipurpose Internet MailExtensions，多用途互联网邮件扩展类型）协议传输创建了一封空邮件，最后通过高层的SMTP 协议发送出去。 MIMEText 对象 msg 包括收发邮箱地址、邮件正文和主题，Python 通过它就可以创建一封格式正确的邮件。smtplib 模块用来设置服务器连接的相关信息。　　要想通过QQ邮箱来发送邮件，需要开启QQ邮箱的设置-账户里SMTP服务，接下来会通过发送短信验证来获得授权码，有了授权码后就可以在代码里添加了。        接下来看看QQ的邮件服务器配置：\n使用SSL的通用配置：pop.qq.com，使用SSL，端口号995发送邮件服务器：smtp.qq.com，使用SSL，端口号465或587根据此配置来设置smtplib.SMTP_SSL()函数的参数。具体代码如下：\n12345678910111213141516171819202122232425262728293031323334353637383940import smtplibfrom email.mime.text import MIMETextfrom email.header import Header# come_from 是自己的邮箱，password是邮箱的授权码come_from = &#x27;3426628229@qq.com&#x27;  #  用户是谁  用户名password = &#x27;qtyohkdcrbozdbcf&#x27;   #  秘钥串  相当于密码#  to_email 是接收的邮箱to_email = &#x27;1052117505@qq.com&#x27;  #  要发送给谁# 邮箱服务器smtp_server = &#x27;smtp.qq.com&#x27;infos = &#x27;董老师好，我是海日汉，有机会来我们大内蒙骑马丫~~&#x27;#  邮箱正文，第一个参数为内容，第二个参数为格式，默认为纯文本，第三个参数是编码#  这里我用把发送内容赋值给变量infosmsg = MIMEText(infos,&#x27;plain&#x27;,&#x27;utf-8&#x27;)#  邮件头部信息msg[&#x27;From&#x27;] = Header(come_from)#  发送给谁msg[&#x27;To&#x27;] = Header(to_email)#  邮件主题msg[&#x27;Subject&#x27;] = Header(&#x27;1908大实训&#x27;)#  创建实例server = smtplib.SMTP_SSL(smtp_server)#  QQ邮箱SMTP的端口号时465或者587server.connect(smtp_server,465)#  登录邮件server.login(come_from,password)#  发送邮件server.sendmail(come_from,to_email,msg.as_string())print(&#x27;邮件已发送&#x27;)#  退出邮件server.quit()\n","categories":["django"],"tags":["python"]},{"title":"实现微博第三方登录","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/10/02/实现微博第三方登录/","content":"在微博网站 注册使用apphttps://open.weibo.com/登录账号选择网站接入\n\n创建应用\n\n应用创建成功后，会立即跳转审核页面，需要填写一些资质，用来进行审核，其实这些审核字段都不用搭理，应用压根就不需要通过审核\n\n之后填写指定的回调路径\n\n此时我们有了 App Key App Secret 回调页 需要构思编码的流程\n\n首先根据 app key 来访问请求API接口123456//新浪微博第三方登录sina()&#123;    let client_id=2464168997    let url=&quot;https://api.weibo.com/oauth2/authorize?client_id=&quot;+client_id+&quot;&amp;redirect_uri=回调路由&quot;    window.location.href=url&#125;\n\n\n在指定回调路由下 接受参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def wb_back(request):    # 初始返回一个code    code=request.GET.get(&#x27;code&#x27;,None)    # 微博接口地址    access_token_url = &quot;https://api.weibo.com/oauth2/access_token&quot;    # 定义参数    re = requests.post(access_token_url, data=&#123;        &quot;client_id&quot;: &#x27;app key&#x27;,        &quot;client_secret&quot;: &quot;secert key&quot;,        &quot;grant_type&quot;: &quot;authorization_code&quot;,        &quot;code&quot;: code,        &quot;redirect_uri&quot;: &quot;回调路由&quot;,    &#125;)    # 获取用户信息    res=requests.get(&#x27;https://api.weibo.com/2/users/show.json&#x27;,params=&#123;&quot;access_token&quot;:re.json()[&quot;access_token&quot;],&quot;uid&quot;:re.json()[&quot;uid&quot;]&#125;)    # res.json（）就是用户登录的信息    print(res.json())上面代码实现了接受用户的参数下面根据用户参数实现一些判断逻辑    username=str(res.json()[&#x27;name&#x27;])    # 判断是否用新浪微博登录过    user=User.objects.filter(username=username).first()    sina_id=&#x27;&#x27;    user_id=&#x27;&#x27;    if user:        # 代表曾经登录过        sina_id=user.username        user_id=user.id    else:        # 首次登陆        User(username=username,password=res.json()[&#x27;id&#x27;]).save()        user=User.objects.filter(username=username).first()        sina_id=user.username        user_id=user.id    print(sina_id,user_id)    time.sleep(2)    # 重定向到主页 并传递参数    return redirect(&quot;http://localhost:8080?sina_id=&quot;+str(sina_id)+&quot;&amp;uid=&quot;+str(user_id))前端解析后端传递的参数var sina_id = this.$route.query.sina_id;var user_id = this.$route.query.uid;if (sina_id)&#123;    // 自动帮用户登录    localStorage.setItem(&#x27;username&#x27;,sina_id)    localStorage.setItem(&#x27;uid&#x27;,user_id)&#125;\n\n\n","categories":["django"],"tags":["python"]},{"title":"切换主题模式","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/09/27/切换主题模式/","content":"设置主题按钮，点击即从白天模式切换为暗黑模式：背景色由白转黑，字体颜色由黑转白。\n首先在前端查找控制背景色的style样式是由哪个属性控制的。\n\n找到是.header中的background控制的。从main.js发现样式是从/src/assets/bootstrap/style.css引入的。\n定义背景颜色变量，然后将定义的变量替换为写死的背景变量；然后前端设置按钮，封装改变背景变量的方法。1234567891011121314151617/* 定义变量 */ :root&#123;  --bg-color:#fff;  --a-color:black;&#125;a.navbar-brand, a.logo &#123;  font-family: &quot;Lato&quot;, &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif;  color: var(--a-color);  font-size: 2rem;  font-weight: bold;  margin-top: 0; &#125;.header &#123;  /* 使用变量 */  background:var(--bg-color);  min-height: 7em;\n\n\n定义变量123456789export default &#123;  data () &#123;    return &#123;\t  //夜间模式切换开关信息\t  style_night:&#x27;夜间模式&#x27;,\t  style:0,\t  &#125;    &#125;  &#125;\n\n\n\n\n在前端vue页面methods里定义开关切换主题方法。12345678910111213141516171819202122 methods:&#123;      //按钮切换主题颜色方法      change_back:function()&#123;        console.log(this.style)          if(this.style==true)&#123;                //获取样式表 所有节点                var styles = getComputedStyle(document.documentElement)                //动态更改 背景颜色                document.documentElement.style.setProperty(&quot;--bg-color&quot;,&quot;#292a2d&quot;);                //字体颜色                document.documentElement.style.setProperty(&quot;--a-color&quot;,&quot;white&quot;)          &#125;else&#123;              //获取样式表 所有节点                var styles = getComputedStyle(document.documentElement)                //动态更改 背景颜色                document.documentElement.style.setProperty(&quot;--bg-color&quot;,&quot;white&quot;);                //字体颜色                document.documentElement.style.setProperty(&quot;--a-color&quot;,&quot;black&quot;)          &#125;                &#125;,&#125;\n\n最后设置一个开关，绑定切换主题方法和变量。1&lt;h-switch v-model=&quot;style&quot; @change=&quot;change_back&quot; &gt;&#123;&#123;style_night&#125;&#125;&lt;/h-switch&gt;\n\n","categories":["django"],"tags":["python"]},{"title":"七牛云存储","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/09/22/七牛云存储/","content":"当今有很多云服务 比如什么百度云 阿里云 可以自己的文件存储到云端服务器进行保存,好处是 防止占用空间 防止数据丢失 一劳永逸,而我们结合云端存储 将数据存储到云端服务器中 而就防止我们自己的服务器的空间占用大,当用户使用数据时 服务器就像中间桥一样进行链接 用户-服务器-云端数据,而我们思路就是在用户上传文件的时候就上传到云端，而本地数据库只是更改一下文件名,这样在访问的时候根据域名+文件名就可以展示出文件七牛云流程图\n先到七牛云注册账号七牛云,之后选择 产品-对象存储,选择使用对象存储并创建空间\n之后点击秘钥\npython根据秘钥来请求token\n123python中有一个qiniu的模块可以使用该模块 直接请求到token\n\n1234567891011# 七牛tokenfrom qiniu import  Auth# qiniu 对象会自动请求tokendef get(self,request):    # 声明认证对象    q = Auth(access_key=&#x27;4CmF2Dy-Xi7PQwHm7Ovpk7YlOiw2pqzUqyf7ybhA&#x27;,             secret_key=&#x27;y3tlCWWdvmgsm2a9j6Plu_WWM4b0jFRTD-f3bHZ7&#x27;)    # 获取token    token=q.upload_token(&#x27;md1907rgzn&#x27;)    return Response(&#123;&#x27;token&#x27;:token&#125;)\n\n\n1然后前端vue 接受到对应的token 并存入全局变量\n123456get_token()&#123;    this.axios.get(&quot;http://127.0.0.1:8000/myapp/qiniu/&quot;).then(result =&gt; &#123;        console.log(result.data.token)        this.token = result.data.token    &#125;);&#125;\n\n对表单设置提交按钮的函数12345678910111213141516171819202122232425// 存入七牛云upload_qiniu:function()&#123;    //获取文件    var img=document.getElementById(&quot;img&quot;)    //声明表单参数    let param = new FormData();    param.append(&quot;img&quot;,img.files[0])    param.append(&#x27;token&#x27;,this.token);    //自定义axios 不允许传输cookie    const axios_qiniu = this.axios.create(&#123;withCredentials:false&#125;);    //发送请求    axios_qiniu(&#123;        method:&#x27;POST&#x27;,        url:&#x27;http://up-z1.qiniup.com/&#x27;,  // 七牛域名        data:param,        timeout:30000  //超时设置    &#125;).then(result =&gt;&#123;        if(result.status==200)&#123;            // 拼接七牛空间的域名 进行展示            this.src = &quot;http://q9ksotoly.bkt.clouddn.com/&quot;+result.data.key;            // this.videosrc = config[&#x27;baseurl&#x27;]+result.data.key;        &#125;    &#125;);&#125;\n","categories":["django"],"tags":["python"]},{"title":"模糊查询及关键字高亮","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/09/17/模糊查询及关键字高亮/","content":"如何实现多条件模糊查询123456在我们一般使用淘宝网站的时候经常使用一些连词 来精准查询的结果例：使用 夏季 鞋子 男 休闲 等词语来指定查找那么如何实现该功能呢\n\n12&lt;!--实现搜索框--&gt;&lt;input @change=&#x27;search&#x27; v-model=&quot;text&quot; &gt;\n\n\ntext 变量就是绑定的搜索的参数  我们只需要判断该变量就可以了1234567\t// 查询字符串中是否有空格if (this.text.indexOf(&#x27; &#x27;))&#123;    //由空格为间隔切片为list    var text = this.text.split(&quot; &quot;)    // 生成list形式的字符串    text = JSON.stringify(text)&#125;\n\n\n对查询参数进行操作 生成字符串：[“value1”,”value2”]   在后台接受的参数时将该字符串转换为列表12# 检索字段text = eval(request.GET.get(&#x27;text&#x27;,None))\n\n\n模糊查询操作操作思路：循环查询参数 每次查询之后添加进列表123456789101112131415# 是否进行模糊查询if text:    goods=[]    # 循环条件列表    for key in text:        # 每次循环 查询数据        good_obj=Goods.objects.filter(Q(name__contains=key)|Q(desc__contains=key)).all()        # 向列表尾部拼接        goods.extend(good_obj)        # 去重 防止重复 有时重复查询同一个 会出现多数据 就需要去重        goods=list(set(goods))        count = len(goods)else:            # 查询所有商品个数     count = Goods.objects.count()\n\n\n关键字高亮Permalink12345在我们访问百度的时候 经常看见自己查询的关键字是红色的这是怎么做到的呢可以用过滤器来操作\n12345678910//过滤器 filters:&#123;\t make_text(str)&#123;\t\tvar mytext=str.toString()\t\tvar text=&#x27;关键字&#x27;        // new RegExp(text,&#x27;g&#x27;) 正则模式全文检索 \t\treturn mytext.replace(new RegExp(text,&#x27;g&#x27;),&#x27;&lt;span class=&quot;highlight&quot;&gt;&#x27;+text+&#x27;&lt;/span&gt;&#x27;)\t &#125; &#125;,\n\n\n可以实现为 关键字 词语   这样我们就可以设置指定样式来输出了   使用 v-html 来输出123&lt;!--因为v-html与其他不同 需要以调用方式来使用过滤器--&gt;&lt;!--如果全局声明了装饰器就不需要 否则就要以$options.filters.装饰器 来使用--&gt;&lt;span v-html=&#x27;$options.filters.make_text(item.name)&#x27;&gt;&lt;/span&gt;\n","categories":["django"],"tags":["python"]},{"title":"密码错误五次以上账号锁定功能","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/09/10/密码错误五次以上账号锁定功能/","content":"利用Redis数据进行密码输入错误锁号的功能限制操作1.构造一个计数器12345678910111213# 导包import redis# 定义ip 和端口号host = &quot;localhost&quot;port = 6379# 建立链接r = redis.Redis(host=host,port=port)# 记录插入放入次数# a = r.incr(‘keys’0)# print(a)if a &gt; 5：\tprint(&quot;你的账号被锁定了&quot;)\n\n\n2.通过列表类型的数组12345678910111213141516171819202122232425#导包import redis#定义ip和端口号host = &quot;127.0.0.1&quot;port = 6379#建立连接r = redis.Redis(host=host,port=port)#用户名username = &#x27;laowang&#x27;#账号密码输入错误5次后锁住账号逻辑#列表操作r.lpush(username,1)#设置过期时间,单位是秒r.expire(username,30)#打印过期时间print(r.ttl(username))#打印列表长度print(r.llen(username))if r.llen(username) &gt; 5:    print(&#x27;你的账号被锁定&#x27;)","categories":["django"],"tags":["python"]},{"title":"拦截器","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/09/04/拦截器/","content":"前端vue(index.js)代码示例：123456789101112131415161718&#123;  path:&#x27;/myprofile&#x27;,  name:&#x27;myprofile&#x27;,  component:myprofile,  &lt;!-- to去哪，from来自哪，next下一步 --&gt;  beforeEnter: (to, from, next) =&gt; &#123;    if(localStorage.getItem(&#x27;username&#x27;))&#123;      console.log(&#x27;已经登录&#x27;);      &lt;!-- 注意必须有下一步 --&gt;      next();    &#125;else&#123;      console.log(&#x27;没有登录&#x27;);      next(&#x27;/login&#x27;);    &#125;  &#125;&#125;,\n","categories":["django"],"tags":["python"]},{"title":"进程，线程和协程的区别","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/09/01/进程，线程和协程的区别/","content":"现在多进程多线程已经是老生常谈了，协程也在最近几年流行起来。python中有协程库gevent，py web框架tornado中也用了gevent封装好的协程。本文主要介绍进程、线程和协程三者之间的区别。\n一.  概念1. 进程\n进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动,进程是系统进行资源分配和调度的一个独立单位。每个进程都有自己的独立内存空间，不同进程通过进程间通信来通信。由于进程比较重量，占据独立的内存，所以上下文进程间的切换开销（栈、寄存器、虚拟内存、文件句柄等）比较大，但相对比较稳定安全。\n2.  线程\n线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。线程间通信主要通过共享内存，上下文切换很快，资源开销较少，但相比进程不够稳定容易丢失数据。\n3. 协程\n协程是一种用户态的轻量级线程，协程的调度完全由用户控制。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。\n二.  区别1， 进程多与线程比较\n线程是指进程内的一个执行单元,也是进程内的可调度实体。线程与进程的区别:\n\n地址空间:线程是进程内的一个执行单元，进程内至少有一个线程，它们共享进程的地址空间，而进程有自己独立的地址空间\n\n资源拥有:进程是资源分配和拥有的单位,同一个进程内的线程共享进程的资源\n\n线程是处理器调度的基本单位,但进程不是\n\n二者均可并发执行\n\n每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口，但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制\n\n\n2，协程多与线程进行比较\n\n一个线程可以多个协程，一个进程也可以单独拥有多个协程，这样python中则能使用多核CPU。\n\n线程进程都是同步机制，而协程则是异步\n\n协程能保留上一次调用时的状态，每次过程重入时，就相当于进入上一次调用的状态\n三.  进程和线程、协程在python中的使用\n多进程一般使用multiprocessing库，来利用多核CPU，主要是用在CPU密集型的程序上，当然生产者消费者这种也可以使用。多进程的优势就是一个子进程崩溃并不会影响其他子进程和主进程的运行，但缺点就是不能一次性启动太多进程，会严重影响系统的资源调度，特别是CPU使用率和负载。使用多进程可以查看文章《python 多进程使用总结》。注：python2的进程池在类中的使用会有问题，需要把类函数定义成全局函数。具体可参考 http://bbs.chinaunix.net/thread-4111379-1-1.html\n\n多线程一般是使用threading库，完成一些IO密集型并发操作。多线程的优势是切换快，资源消耗低，但一个线程挂掉则会影响到所有线程，所以不够稳定。现实中使用线程池的场景会比较多，具体可参考《python线程池实现》。\n\n协程一般是使用gevent库，当然这个库用起来比较麻烦，所以使用的并不是很多。相反，协程在tornado的运用就多得多了，使用协程让tornado做到单线程异步，据说还能解决C10K的问题。所以协程使用的地方最多的是在web应用上。\n四.  一个形象的例子解释进程和线程的区别这副图是一个双向多车道的道路图，假如我们把整条道路看成是一个“进程”的话，那么图中由白色虚线分隔开来的各个车道就是进程中的各个“线程”了。\n\n\n\n这些线程(车道)共享了进程(道路)的公共资源(土地资源)。\n\n这些线程(车道)必须依赖于进程(道路)，也就是说，线程不能脱离于进程而存在(就像离开了道路，车道也就没有意义了)。\n\n这些线程(车道)之间可以并发执行(各个车道你走你的，我走我的)，也可以互相同步(某些车道在交通灯亮时禁止继续前行或转弯，必须等待其它车道的车辆通行完毕)。\n\n这些线程(车道)之间依靠代码逻辑(交通灯)来控制运行，一旦代码逻辑控制有误(死锁，多个线程同时竞争唯一资源)，那么线程将陷入混乱，无序之中。\n\n这些线程(车道)之间谁先运行是未知的，只有在线程刚好被分配到CPU时间片(交通灯变化)的那一刻才能知道。\n\n\n\n总结一下就是IO密集型一般使用多线程或者多进程，CPU密集型一般使用多进程，强调非阻塞异步并发的一般都是使用协程，当然有时候也是需要多进程线程池结合的，或者是其他组合方式。\n","categories":["python"],"tags":["python"]},{"title":"画中画","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/08/28/画中画/","content":"123456789101112131415161718192021222324252627282930313233&lt;template&gt;  &lt;div&gt;    &lt;div&gt;        视频：&lt;video id=&#x27;video&#x27; :src=&quot;img_url&quot; width=&quot;350&quot; height=&quot;240&quot;&gt;&lt;/video&gt;        &lt;Button @click=&quot;into&quot;&gt;&#123;&#123; this.hzh&#125;&#125;&lt;/Button&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123;    data () &#123;        return &#123;            //画中画            hzh : &#x27;进入画中画&#x27;,        &#125;    &#125;,    methods:&#123;        //画中画        into:function()&#123;            if(video != document.pictureInPictureElement)&#123;                //尝试进入画中画                video.requestPictureInPicture();                this.hzh = &#x27;退出画中画&#x27;            &#125;else&#123;                //退出画中画                document.exitPictureInPicture();                this.hzh = &#x27;进入画中画&#x27;            &#125;        &#125;,    &#125;，&#125;&lt;/script&gt;\n\n\n","categories":["django"],"tags":["python"]},{"title":"分页器的使用","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/08/23/分页器的使用/","content":"根据商品列表展示分页django代码123456789101112131415161718192021222324252627url:    path(&#x27;goodslist/&#x27;,GoodsList.as_view()),#商品列表分页页class GoodsList(APIView):    def get(self,request):        #当前页        page = int(request.GET.get(&#x27;page&#x27;,1))        #一页显示个数        size = int(request.GET.get(&#x27;size&#x27;,4))        #计算从哪开始切        data_start = (page-1)*size        #计算切到哪        data_end = page*size        #查询 切片操作        goods = Goods.objects.all()[data_start:data_end]        #查询所有个数        count = Goods.objects.count()        #序列化操作        ser = GoodsSer(goods,many=True)        return Response(&#123;&#x27;data&#x27;:ser.data,&quot;total&quot;:count&#125;)\n\n\n前端：根据heyui自带的分页器实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;template&gt;  &lt;div&gt;    &lt;div v-for=&quot;(item,index) in goodslist&quot; :key=&quot;index&quot;&gt;      &lt;a :href=&quot;&#x27;http://localhost:8080/item?id=&#x27;+item.id&quot;&gt;&#123;&#123;item.name&#125;&#125;&lt;/a&gt;      &lt;p&gt;&lt;span class=&quot;emphasis&quot;&gt;$&#123;&#123;item.price&#125;&#125;&lt;/span&gt;&lt;/p&gt;      &lt;Pagination v-model=&quot;pagination&quot; align=&quot;center&quot; small @change=&quot;get_goods&quot;&gt;&lt;/Pagination&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default&#123;  data()&#123;    return&#123;      //分页器变量      pagination:&#123;          //当前页          page: 1,          //每页多少个          size: 2,          //总数          total: 4,      &#125;,    &#125;  &#125;,  mounted:&#123;    //调用商品列表分页    this.get_goods();  &#125;,  methods:&#123;    \t//获取商品列表      get_goods()&#123;        this.axios(&#123;          url:&#x27;http://localhost:8000/goodslist/&#x27;,          method:&#x27;GET&#x27;,          params:&#123;            //分页传参            page:this.pagination.page,            size:this.pagination.size,          &#125;        &#125;).then(resp=&gt;&#123;          console.log(resp)          //获取商品列表和总页码          this.goodslist = resp.data.data;          this.pagination.total =resp.data.total;        &#125;)      &#125;,  &#125;&#125;&lt;/script&gt;\n\n\n前端：自主分页和分页偏移123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130&lt;template&gt;  &lt;div&gt;    &lt;div&gt;\t\t\t\t&lt;span&gt;&lt;a @click=&quot;get_goods_self(1)&quot;&gt;首页&lt;/a&gt;&lt;/span&gt;\t\t\t\t&amp;emsp;&amp;emsp;\t\t\t\t&lt;Button v-show=&quot;lastpage&quot; @click=&quot;get_goods_self(lastpage)&quot;&gt;上一页&lt;/Button&gt;\t\t\t\t&lt;!-- &lt;span v-for=&quot;index in allpage&quot;&gt;&lt;a @click=&quot;get_goods_self(index)&quot;&gt;&#123;&#123;index&#125;&#125;&lt;/a&gt;&amp;emsp;&amp;emsp;&lt;/span&gt; --&gt;\t\t\t\t&lt;!-- 左侧分页偏移 --&gt;\t\t\t\t&lt;span v-for=&quot;item in last_page&quot;&gt;&lt;a @click=&quot;get_goods_self(item)&quot;&gt;&#123;&#123;item&#125;&#125;&lt;/a&gt;&amp;emsp;&amp;emsp;&lt;/span&gt;\t\t\t\t&lt;!-- 当前页 --&gt;\t\t\t\t&lt;a @click=&quot;get_goods_self(page)&quot;&gt;&#123;&#123;page&#125;&#125;&amp;emsp;&amp;emsp;&lt;/a&gt;\t\t\t\t&lt;!-- 右侧分页品偏移 --&gt;\t\t\t\t&lt;span v-for=&quot;item in next_page&quot;&gt;&lt;a @click=&quot;get_goods_self(item)&quot;&gt;&#123;&#123;item&#125;&#125;&lt;/a&gt;&amp;emsp;&amp;emsp;&lt;/span&gt;\t\t\t\t&lt;Button v-show=&quot;nextpage&quot; @click=&quot;get_goods_self(nextpage)&quot;&gt;下一页&lt;/Button&gt;\t\t\t\t&lt;!-- 跳转分页  --&gt;\t\t\t\t&lt;input type=&quot;text&quot; style=&quot;width:40px;&quot; @input=&quot;jump_page($event)&quot;&gt;\t\t\t\t&amp;emsp;&amp;emsp;\t\t\t\t&lt;span&gt;&lt;a @click=&quot;get_goods_self(allpage)&quot;&gt;尾页&lt;/a&gt;&lt;/span&gt;      &lt;/div&gt;  &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default&#123;  data()&#123;    return&#123;      //上一页      lastpage:0,      //下一页      nextpage:0,      //当前页      page:1,      //每页展示个数      size:1,      //总页数      allpage:0,      //自主分页总个数      total_self:0,      //商品列表      goodslist_self:[],    &#125;  &#125;,  mounted()&#123;    //因为打开页面是从第一页开始的，所以参数是1    this.get_goods_self(1);  &#125;,  methods:&#123;    //input输入页面跳转\t  jump_page(e)&#123;      this.page = e.target.value      var val = e.target.value      console.log(val)      if(val!=&quot;&quot;)&#123;        if(val&gt;this.allpage)&#123;          this.$Message(&#x27;您输入的页码有误&#x27;)          return false;        &#125;        else if(val&lt;this.page)&#123;          this.$Message(&#x27;您输入的页码有误&#x27;)          return false;        &#125;      &#125;else&#123;        return false;      &#125;      this.get_goods_self(val);    &#125;,    //自定义分页\t  get_goods_self(page)&#123;      //确定当前页      this.page = page      this.axios(&#123;        url:&#x27;http://localhost:8000/goodslist/&#x27;,        method:&#x27;GET&#x27;,        params:&#123;          page:page,          size:this.size,        &#125;      &#125;).then(resp=&gt;&#123;        console.log(resp)        //获取列表内容        this.goodslist_self = resp.data.data;        //商品总数        this.total_self =resp.data.total;        //判断上一页        if(page==1)&#123;          //如果当前页为1，就是第一页，没有上一页          this.lastpage = 0;        &#125;else&#123;          //如果当前页不是第一个，上一页就是当前页-1          this.lastpage = page-1        &#125;        //计算总页数        //Meth.ceil向上取整，只要有余数，就为整数        this.allpage = Math.ceil(this.total_self / this.size);        //判断下一页        if(page == this.allpage)&#123;          //如果最后一页和总页数相同，该页数就是最后一页          this.nextpage = 0;        &#125;else&#123;          //如果最后一页和总页数不同，下一页就是当前页+1          this.nextpage = page+1        &#125;        //设置偏移量        var move_page = 2;        var my_last = [];                //计算左侧偏移量        for(let i=page-move_page;i&lt;page;i++)&#123;          if(i&gt;0)&#123;            my_last.push(i)          &#125;        &#125;        //计算右侧偏移量        var my_next = [];        //计算左侧偏移量        for(let i=page+1;i&lt;=page+move_page;i++)&#123;          if(i&lt;=this.allpage)&#123;            my_next.push(i)          &#125;        &#125;        this.last_page = my_last;        this.next_page = my_next;              &#125;)\t  &#125;,  &#125;&#125;&lt;/script&gt;","categories":["django"],"tags":["python"]},{"title":"反序列化","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/08/15/反序列化/","content":"反序列化是将所有参数打包为一个dict   之后将dict提交给序列化类 通过save就可以直接入库保存了123456789101112131415161718class UserInsert(APIView):    def post(self,request):        # 初始化参数        name = request.GET.get(&#x27;name&#x27;)        password = request.GET.get(&#x27;password&#x27;)        # 反序列化添加        data=&#123;            &#x27;name&#x27;:name,            &#x27;password&#x27;:password        &#125;        user = User_Ser(data=data)        # 验证字段是否错误        if user.is_valid():            # 进行入库操作            user.save()                    return Response(&#123;&#x27;code&#x27;:200,&#x27;msg&#x27;:&#x27;ok&#x27;&#125;)\n在反序列化时候一定要注意字段的类型(要与字段个数一致)","categories":["django"],"tags":["python"]},{"title":"增量式与分布式","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/08/09/增量式与分布式/","content":"基于crawlSpider的全站数据爬取123# 项目的创建scrapy startproject projectnamescrapy genspider -t crawl spidername www.baidu.com\n12345# crawlspider全站数据爬取:- CrawlSpider是一个爬虫类, 是scrapy.spider的子类, 功能比spider更强大.- CrawlSpider的机制:    - 连接提取器: 可以根据指定的规则进行连接的提取    - 规则解析器: 更具指定的规则对响应数据进行解析\n案例: 基于CrawlSpider对笑话网进行全站深度数据爬取, 抓取笑话标题与内容, 并存储于MongoDB中12345# item编码:import scrapyclass JokeItem(scrapy.Item):    title &#x3D; scrapy.Field()    content &#x3D; scrapy.Field()\n\n1234567891011121314151617181920212223242526272829303132# spider编码:import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom..items import JokeItemclass ZSpider(CrawlSpider):    name &#x3D; &#39;z&#39;    # allowed_domains &#x3D; [&#39;www.baidu.com&#39;]    start_urls &#x3D; [&#39;http:&#x2F;&#x2F;xiaohua.zol.com.cn&#x2F;lengxiaohua&#x2F;&#39;]    link &#x3D; LinkExtractor(allow&#x3D;r&#39;&#x2F;lengxiaohua&#x2F;\\d+.html&#39;)    link_detail &#x3D; LinkExtractor(allow&#x3D;r&#39;.*?\\d+\\.html&#39;)    rules &#x3D; (        Rule(link, callback&#x3D;&#39;parse_item&#39;, follow&#x3D;True),        Rule(link_detail, callback&#x3D;&#39;parse_detail&#39;),    )    def parse_item(self, response):        pass    def parse_detail(self, response):        title &#x3D; response.xpath(&#39;&#x2F;&#x2F;h1[@class&#x3D;&quot;article-title&quot;]&#x2F;text()&#39;).extract_first()        content &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;article-text&quot;]&#x2F;&#x2F;text()&#39;).extract()        content &#x3D; &#39;&#39;.join(content)        if title and content:            item &#x3D; JokeItem()            item[&quot;title&quot;] &#x3D; title            item[&quot;content&quot;] &#x3D; content            print(dict(item))            yield item\n\n123456789101112131415161718192021222324# pipeline编码:class JokePipeline(object):    def __init__(self, mongo_uri, mongo_db):        self.mongo_uri &#x3D; mongo_uri        self.mongo_db &#x3D; mongo_db    @classmethod    def from_crawler(cls, crawler):        return cls(            mongo_uri&#x3D;crawler.settings.get(&#39;MONGO_URI&#39;),            mongo_db&#x3D;crawler.settings.get(&#39;MONGO_DB&#39;)        )    def open_spider(self, spider):        self.client &#x3D; pymongo.MongoClient(self.mongo_uri)        self.db &#x3D; self.client[self.mongo_db]    def process_item(self, item, spider):        self.db[&quot;joke&quot;].insert(dict(item))        return item    def close(self, spider):        self.client.close()\n\n电影天堂: 全站深度抓取电影名与下载链接:1234567# item定义存储字段:import scrapyclass BossItem(scrapy.Item):    title &#x3D; scrapy.Field()    downloadlink &#x3D; scrapy.Field()\n\n1234567891011121314151617181920212223242526272829# spider编码:import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom ..items import MvItemclass BSpider(CrawlSpider):    name &#x3D; &#39;mv&#39;    # allowed_domains &#x3D; [&#39;www.baidu.com&#39;]    start_urls &#x3D; [&#39;https:&#x2F;&#x2F;www.ygdy8.net&#x2F;html&#x2F;gndy&#x2F;oumei&#x2F;index.html&#39;]    link &#x3D; LinkExtractor(allow&#x3D;r&#39;list.*?html&#39;)    link_detail &#x3D; LinkExtractor(allow&#x3D;r&#39;.*?&#x2F;\\d+\\.html&#39;)    rules &#x3D; (        Rule(link, callback&#x3D;&#39;parse_item&#39;, follow&#x3D;True,),        Rule(link_detail, callback&#x3D;&#39;parse_detail&#39;, follow&#x3D;True,),    )    def parse_item(self, response):        pass    def parse_detail(self, response):        title &#x3D; response.xpath(&#39;&#x2F;&#x2F;h1&#x2F;&#x2F;text()&#39;).extract_first()        downloadlink &#x3D; response.xpath(&#39;&#x2F;&#x2F;tbody&#x2F;tr&#x2F;td&#x2F;a&#x2F;text()&#39;).extract_first()        if title and downloadlink and &#39;ftp&#39; in downloadlink:            item &#x3D; BossItem()            item[&#39;title&#39;] &#x3D; title            item[&#39;downloadlink&#39;] &#x3D; downloadlink            yield item\n\n123456789101112131415161718192021222324# piplines编码:class MvPipeline(object):    def __init__(self, mongo_uri, mongo_db):        self.mongo_uri &#x3D; mongo_uri        self.mongo_db &#x3D; mongo_db    @classmethod    def from_crawler(cls, crawler):        return cls(            mongo_uri&#x3D;crawler.settings.get(&#39;MONGO_URI&#39;),            mongo_db&#x3D;crawler.settings.get(&#39;MONGO_DB&#39;)        )    def open_spider(self, spider):        self.client &#x3D; pymongo.MongoClient(self.mongo_uri)        self.db &#x3D; self.client[self.mongo_db]    def process_item(self, item, spider):        self.db[&quot;mv&quot;].insert(dict(item))        return item    def close(self, spider):        self.client.close()\n\n1.分布式12345678910# 分布式概念:使用多台机器组成一个分布式的机群，在机群中运行同一组程序，进行联合数据的爬取。# 原生scrapy无法实现分布式原因:\t- 原生的scrapy中的调度器不可以被共享\t- 原生的scrapy的管道不可以被共享# 使用scrapy实现分布式思路:- 为原生的scrapy框架提供共享的管道和调度器- pip install scrapy_redis\n\n\n12345678910111213141516171819202122232425262728293031- 1. 创建工程: scrapy startproject projectname- 2. 爬虫文件: scrapy genspider -t crawl spidername www.baidu.com- 3. 修改爬虫文件：\t- 3.1 导包：from scrapy_redis.spiders import RedisCrawlSpider\t- 3.2 将当前爬虫类的父类进行修改RedisCrawlSpider\t- 3.3 allowed_domains，start_url注释掉，添加一个新属性redis_key&#x3D;&#39;qn&#39;(调度器队列的名称)\t- 3.4 指定redis_key &#x3D; &#39;xxx&#39; , 即共享调度器队列名字\t- 3.4 数据解析，将解析的数据封装到item中然后向管道提交- 4. 配置文件的编写：\t- 4.1 指定管道：\t\tITEM_PIPELINES &#x3D; &#123;\t\t\t&#39;scrapy_redis.pipelines.RedisPipeline&#39;: 400\t\t&#125;\t- 4.2 指定调度器：\t\t# 增加了一个去重容器类的配置, 作用使用Redis的set集合来存储请求的指纹数据, 从而实现请求去重的持久化\t\tDUPEFILTER_CLASS &#x3D; &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;\t\t# 使用scrapy-redis组件自己的调度器\t\tSCHEDULER &#x3D; &quot;scrapy_redis.scheduler.Scheduler&quot;\t\t# 配置调度器是否要持久化, 也就是当爬虫结束了, 要不要清空Redis中请求队列和去重指纹的set。如果是True, 就表示要持久化存储, 就不清空数据, 否则清空数据\t\tSCHEDULER_PERSIST &#x3D; True\t- 4.3 指定具体的redis：\t\tREDIS_HOST &#x3D; &#39;redis服务的ip地址&#39;\t\tREDIS_PORT &#x3D; 6379- 5. 修改Redis配置并指定配置启动：\t- #bind 127.0.0.1\t- protected-mode no\t- 开启redis服务(携带redis的配置文件：redis-server .&#x2F;redis.windows.conf),和客户端(redis-cli)：- 6. 启动程序：scrapy runspider xxx.py(需要进入spider文件夹)- 7. 向调度器队列中扔入一个起始的url（redis的客户端）：lpush xxx www.xxx.com\t(xxx表示的就是redis_key的值)\n\n案例: 阳光热线问政平台投诉信息爬取–&gt;网址: http://wz.sun0769.com/index.php/question/questionType?type=412345# items编码:import scrapyclass FbsproItem(scrapy.Item):    # define the fields for your item here like:    title &#x3D; scrapy.Field()\n\n123456789101112131415161718192021222324# spider编码:import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom scrapy_redis.spiders import RedisCrawlSpiderfrom fbspro.items import FbsproItem  class TestSpider(RedisCrawlSpider):    name &#x3D; &#39;test&#39;      # allowed_domains &#x3D; [&#39;ww.baidu.com&#39;]    # start_urls &#x3D; [&#39;http:&#x2F;&#x2F;ww.baidu.com&#x2F;&#39;]    redis_key &#x3D; &#39;urlscheduler&#39;    link &#x3D; LinkExtractor(allow&#x3D;r&#39;.*?&amp;page&#x3D;\\d+&#39;)    rules &#x3D; (        Rule(link, callback&#x3D;&#39;parse_item&#39;, follow&#x3D;True),    )    def parse_item(self, response):        a_lst &#x3D; response.xpath(&#39;&#x2F;&#x2F;a[@class&#x3D;&quot;news14&quot;]&#39;)        for a in a_lst:            title &#x3D; a.xpath(&#39;.&#x2F;text()&#39;).extract_first()            # print(title)            item &#x3D; FbsproItem()            item[&#39;title&#39;] &#x3D; title            yield item\n\n123456789101112131415161718# settings配置编码:USER_AGENT &#x3D; &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;76.0.3809.100 Safari&#x2F;537.36&#39;ROBOTSTXT_OBEY &#x3D; FalseCONCURRENT_REQUESTS &#x3D; 3ITEM_PIPELINES &#x3D; &#123;   # &#39;fbspro.pipelines.FbsproPipeline&#39;: 300,    &#39;scrapy_redis.pipelines.RedisPipeline&#39;: 400&#125;# 增加了一个去重容器类的配置, 作用使用Redis的set集合来存储请求的指纹数据, 从而实现请求去重的持久化DUPEFILTER_CLASS &#x3D; &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;# 使用scrapy-redis组件自己的调度器SCHEDULER &#x3D; &quot;scrapy_redis.scheduler.Scheduler&quot;# 配置调度器是否要持久化, 也就是当爬虫结束了, 要不要清空Redis中请求队列和去重指纹的set。如果是True, 就表示要持久化存储, 就不清空数据, 否则清空数据SCHEDULER_PERSIST &#x3D; True# redis配置REDIS_HOST &#x3D; &#39;192.168.12.198&#39;REDIS_PORT &#x3D; 6379\n\n\n2.增量式12345# 概念:\t- 检测网站数据更新, 只爬取更新的内容\t- 核心: 去重        - url        - 数据指纹\n\n增量式爬虫: 电影名称与电影类型的爬取–&gt;url: https://www.4567tv.co/list/index1.html12345# items编码:import scrapyclass MvproItem(scrapy.Item):    title &#x3D; scrapy.Field()    position &#x3D; scrapy.Field()\n123456789101112131415161718192021222324252627282930313233343536# spider编码:import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom redis import Redisfrom mvpro.items import MvproItemclass MoveSpider(CrawlSpider):    conn &#x3D; Redis(&#39;127.0.0.1&#39;, 6379)    name &#x3D; &#39;move&#39;    # allowed_domains &#x3D; [&#39;www.baidu.com&#39;]    start_urls &#x3D; [&#39;https:&#x2F;&#x2F;www.4567tv.co&#x2F;list&#x2F;index1.html&#39;]    link &#x3D; LinkExtractor(allow&#x3D;r&#39;&#x2F;list&#x2F;index1-\\d+\\.html&#39;)    rules &#x3D; (        Rule(link, callback&#x3D;&#39;parse_item&#39;, follow&#x3D;True),    )    def parse_item(self, response):        li_list &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[contains(@class, &quot;index-area&quot;)]&#x2F;ul&#x2F;li&#39;)        for li in li_list:            mv_link &#x3D; &#39;https:&#x2F;&#x2F;www.4567tv.co&#39; + li.xpath(&#39;.&#x2F;a&#x2F;@href&#39;).extract_first()            ex &#x3D; self.conn.sadd(&#39;mv_link&#39;, mv_link)            if ex:                print(&#39;有新数据可以爬取..........................&#39;)                yield scrapy.Request(url&#x3D;mv_link, callback&#x3D;self.parse_detail)            else:                print(&#39;没有新数据可以爬取!!!!!!!!!!!!!!!!!!!!!!!!!&#39;)    def parse_detail(self, response):        title &#x3D; response.xpath(&#39;&#x2F;&#x2F;dt[@class&#x3D;&quot;name&quot;]&#x2F;text()&#39;).extract_first()        pro &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;ee&quot;]&#x2F;text()&#39;).extract_first()        item &#x3D; MvproItem()        item[&#39;title&#39;] &#x3D; title        item[&#39;position&#39;] &#x3D; pro        yield item\n\n需求: 基于数据指纹的增量式爬虫, 爬取糗百文字1234567891011121314151617181920212223242526# spider编码:import scrapyfrom qiubai.items import QiubaiItemimport hashlibfrom redis import Redisclass QbSpider(scrapy.Spider):    conn &#x3D; Redis(&#39;127.0.0.1&#39;, 6379)    name &#x3D; &#39;qb&#39;    # allowed_domains &#x3D; [&#39;www.baidu.com&#39;]    start_urls &#x3D; [&#39;https:&#x2F;&#x2F;www.qiushibaike.com&#x2F;text&#x2F;&#39;]    def parse(self, response):        div_list &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@id&#x3D;&quot;content-left&quot;]&#x2F;div&#39;)        for div in div_list:            content &#x3D; div.xpath(&#39;.&#x2F;a[1]&#x2F;div[@class&#x3D;&quot;content&quot;]&#x2F;span[1]&#x2F;text()&#39;).extract_first()            fp &#x3D; hashlib.md5(content.encode(&#39;utf-8&#39;)).hexdigest()            ex &#x3D; self.conn.sadd(&#39;fp&#39;, fp)            if ex:                print(&#39;有更新数据需要爬取........................&#39;)                item &#x3D; QiubaiItem()                item[&#39;content&#39;] &#x3D; content                yield item            else:                print(&#39;没有数据更新~~~&#39;)\n3.scrapy提高数据爬取效率12345678910111213141.增加并发：默认scrapy开启的并发线程为32个，可以适当进行增加。在settings配置文件中修改CONCURRENT_REQUESTS &#x3D; 100值为100,并发设置成了为100。2.降低日志级别：    在运行scrapy时，会有大量日志信息的输出，为了减少CPU的使用率。可以设置log输出信息为INFO或者ERROR即可。在配置文件中编写：LOG_LEVEL &#x3D; ‘INFO’3.禁止cookie：    如果不是真的需要cookie，则在scrapy爬取数据时可以禁止cookie从而减少CPU的使用率，提升爬取效率。在配置文件中编写：COOKIES_ENABLED &#x3D; False4.禁止重试：    对失败的HTTP进行重新请求（重试）会减慢爬取速度，因此可以禁止重试。在配置文件中编写：RETRY_ENABLED &#x3D; False5.减少下载超时：    如果对一个非常慢的链接进行爬取，减少下载超时可以能让卡住的链接快速被放弃，从而提升效率。在配置文件中进行编写：DOWNLOAD_TIMEOUT &#x3D; 10 超时时间为10s\n\n4.虚拟环境安装:1pip install virtualenvwrapper-win\n\n1# 常用命令:mkvirtualenv envname  # 创建虚拟环境并自动切换到该环境下workon envname  # 切换到某虚拟环境下pip list rmvirtualenv envname  # 删除虚拟环境deactivate  # 退出虚拟环境lsvirtualenv  # 列出所有常见的虚拟环境mkvirtualenv --python&#x3D;&#x3D;C:\\...\\python.exe envname  # 指定Python解释器创建虚拟环境","categories":["爬虫"],"tags":["python"]},{"title":"多线程爬虫","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/08/01/多线程爬虫/","content":"1.整体思路12345678910111213&gt; 队列保存变化的参数，即之前构建的类似页码的东西。&gt; 一个类实现：三个线程进行数据采集，依次从队列中获取参数，拼接url，请求到的网\t页源码保存到新队列。&gt; 一个类实现 ：1.  三个线程进行数据处理，依次从队列中获取源码，xpath定位元素\t取出文本，构建字典。2. 定义函数数据入库，pymongon数据插入数据即可主程序 mian 分为 两 部分- 实例化队列，我们有两组线程在工作，初始队列一个，线程间通信需要一个，一共两个- 实例化线程，通过遍历实例化线程类，创建俩组三线程。调用主程序\n\n2.具体部署12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849定义第一个类：  定义类：需要继承threading.Thread父类  类属性：想清楚传那些熟悉   1. 参数队列要传进来   2. 子类重写初始化属性      先继承父类属性----&gt;然后写自己要加的属性  定义方法：   重写run方法   ​\t通过无限循环获取队列中的参数，实现请求多页面，   ​\t这个队列是被三个线程去访问的。定义第二个类： ​\t需要考虑的和第一个类思路是一样的，这里有一个互斥锁的加入 ​\t就是在处理数据的时候加入，因为写入数据库的字段都一样，避免数据量过大 ​\t出错，导致数据错乱。主程序代码  实例队列对象   page_Queue &#x3D; Queue() 构建参数   data_Queue &#x3D; Queue() 负责两个主线程间的通信     实例互斥锁对象   lock &#x3D; Lock()     构建两个主线程，一个线程负责采集数据，一个线程处理数据并入库  当然都是用for循环来构建的，记得线程类需要传入那些属性调用主程序\n\n重写父类同名方法需要注意:12345678class ParseTread(threading.Thread):    def __init__(self, threadName, dataQueue, lock):        # 先把父类的方法继承下来        super(ParseTread, self).__init__()        # 然后定义自己的方法        self.threadName &#x3D; threadName        self.dataQueue &#x3D; dataQueue        self.lock &#x3D; lock\n\n首先明白什么是进程什么是线程，线程和进程的区别？1234567891011121314151617 线程（有时候称为轻量级进程）与进程类似，不过它们是在同一个进程下执行的，并共享相同的上下文。可以将它们认为是在一个主进程或“主线程”中并行运行的一些“迷你进程”。  这里讲的很清楚&lt;https:&#x2F;&#x2F;www.cnblogs.com&#x2F;chbo&#x2F;p&#x2F;7043660.html&gt; 我们的子线程为什么要设置join()，其实设置了守护线程才是join()出现的时机 如果是为了避免数据的恶性竞争的话，互斥锁是接这个的小能手。为什么要设置守护线程 如果给线程设置守护线程，那么后台线程执行完毕就会杀死这些线程。大致的作用就是当你退出进程，后台线程关闭那么这些子线程也要同时关闭。join()的作用，和其存在的意义 join()的功能是设置阻塞，之所以要设置join()就是有选择的设置守护线程，因为我们有的线程需要同步主线程结束而结束，而有的线程需要执行完毕菜行。设置了守护线程的子线程都会随着主线的结束而结束。 我们这里设置的join()是没有意义的，因为并没有设置守护线程，因此没必要设置阻塞。\n\n示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125import requestsfrom urllib import requestfrom chaojiying import Chaojiying_Clientfrom lxml import etreeimport re, threadingfrom threading import Lockfrom queue import Queueimport pymongoimport timeclass CrawlThread(threading.Thread):    def __init__(self, threadName, page_Queue, data_Queue):        super(CrawlThread, self).__init__()        self.threadName &#x3D; threadName        self.pageQueue &#x3D; page_Queue        self.dataQueue &#x3D; data_Queue        self.headers &#x3D; &#123;&quot;User-Agent&quot;: &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;74.0.3729.169 Safari&#x2F;537.36&quot;,&#125;    def run(self):        url1 &#x3D; &#39;http:&#x2F;&#x2F;www.ziroom.com&#x2F;z&#x2F;nl&#x2F;z3.html?p&#x3D;%s&#39;        while True:            try:                # 如果队列为空 block 为 True 那么进入阻塞状态                # 如果队列为控 block 为 false 那么会抛出异常                p &#x3D; self.pageQueue.get(block&#x3D;False)                print(&#39;%s开始工作...&#39; % self.threadName)                response &#x3D; requests.get(url&#x3D;url1 % p, headers&#x3D;self.headers)                content &#x3D; response.text                self.dataQueue.put(content)                # print(self.dataQueue.qsize())                print(&#39;%s工作结束...&#39; % self.threadName)            except:                break                class ParseTread(threading.Thread):    def __init__(self, threadName, dataQueue, lock):        super(ParseTread, self).__init__()        self.threadName &#x3D; threadName        self.dataQueue &#x3D; dataQueue        self.lock &#x3D; lock    def run(self):        while True:            try:                html &#x3D; self.dataQueue.get(block&#x3D;False)                print(&#39;%s开始处理数据........&#39; % self.threadName)                self.get_price_list(html)                print(&#39;%s数据处理完毕........&#39; % self.threadName)            except Exception as e:                print(e)                # print(&#39;出错了&#39;)                break    def get_price_list(self, html):        # 正则匹配图片地址，超级鹰打码，获取列表两个参数 code 下标列表        price_image &#x3D; re.findall(r&#39;&quot;image&quot;:&quot;(.*?)&quot;&#39;, html)[0]        index_list &#x3D; re.findall(r&#39;&quot;offset&quot;:(.*?)&#125;;&#39;, html)[0]        index_list &#x3D; eval(index_list)        # print(index_list)        price_image_link &#x3D; &#39;http:&#39; + price_image        # print(price_image_link)        request.urlretrieve(price_image_link, &#39;.&#x2F;im.jpg&#39;)        # 超级鹰打码，获取到图片中的数字        chaojiying &#x3D; Chaojiying_Client(&#39;ipython&#39;, &#39;123456789&#39;, &#39;96001&#39;)        im &#x3D; open(&#39;im.jpg&#39;, &#39;rb&#39;).read()        code &#x3D; chaojiying.PostPic(im, 4111)[&#39;pic_str&#39;]        price_list &#x3D; []        for j in index_list:            # print(j)            str1 &#x3D; &#39;&#39;            for i in j:                str1 +&#x3D; code[int(i)]            # print(code)            # print(str1)            price_list.append(int(str1))        self.get_detail_list(html, price_list)    def get_detail_list(self, html, price_list):        html &#x3D; etree.HTML(html)        data_list &#x3D; html.xpath(&#39;&#x2F;&#x2F;ul[@id&#x3D;&quot;houseList&quot;]&#x2F;li&#39;)        a &#x3D; 0        for i in data_list:            item &#x3D; &#123;&#125;            title &#x3D; i.xpath(&#39;.&#x2F;&#x2F;h3&#x2F;a[@class&#x3D;&quot;t1&quot;]&#x2F;text()&#39;)[0]            size_detail &#x3D; i.xpath(&#39;.&#x2F;&#x2F;div[@class&#x3D;&quot;detail&quot;]&#x2F;p&#39;)[0].xpath(&#39;.&#x2F;&#x2F;span&#x2F;text()&#39;)            position &#x3D; i.xpath(&#39;.&#x2F;&#x2F;div[@class&#x3D;&quot;detail&quot;]&#x2F;p&#39;)[1].xpath(&#39;.&#x2F;&#x2F;span&#x2F;text()&#39;)[0]            item &#x3D; &#123;&#39;标题&#39;: title, &#39;面积&#39;: size_detail, &#39;位置&#39;: position, &#39;价格&#39;: price_list[a]&#125;            a +&#x3D; 1            with self.lock:                self.save(item)    def save(self, item):        conn &#x3D; pymongo.MongoClient(&quot;localhost&quot;,27017)        db &#x3D; conn.qiubai        table &#x3D; db.qiubai        table.insert(item)       def main(n):    page_Queue &#x3D; Queue()    data_Queue &#x3D; Queue()    lock &#x3D; Lock()    for i in range(1,n+1):        page_Queue.put(i)    crawlList &#x3D; &#123;&#39;长征1号&#39;, &#39;长征2号&#39;, &#39;长征3号&#39;&#125;    TreadCrawl &#x3D; []    for var in crawlList:        c &#x3D; CrawlThread(var, page_Queue, data_Queue)        c.start()        TreadCrawl.append(c)    for var in TreadCrawl:        var.join()    parseList &#x3D; [&#39;嫦娥1号&#39;, &#39;嫦娥2号&#39;, &#39;嫦娥3号&#39;]    TreadParse &#x3D; []    for var in parseList:        p &#x3D; ParseTread(var, data_Queue, lock)        p.start()        TreadParse.append(p)    for var in TreadParse:        var.join()if __name__ &#x3D;&#x3D; &#39;__main__&#39;:    main(10)\n\n","categories":["爬虫"],"tags":["python"]},{"title":"数据解析之正则&BS4&xpath","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/07/25/数据解析之正则&BS4&xpath/","content":"数据解析\n数据解析就是应用一定的技术手段在响应数据中获取目标数据\n常用数据解析方式:\n正则: 匹配高效, 但正则表达式书写有难度\nBS4: 解析数据速度慢, 但使用简单\nxpath: 解析速度快, 使用简单\n\n\n环境配置:\n正则: \n1pip install re\nBS4:\n12pip install lxmlpip install BeautifulSoup4\n\nxpath:\n1pip install lxml\n\n\n\n1.正则元字符匹配1234567891011121314151617 .     匹配任意字符，除了换行符[]    用来表示一组字符,单独列出：[abc] 匹配 &#39;a&#39;，&#39;b&#39;或&#39;c&#39;[^...]\t匹配除了字符组中字符的所有字符\\d    匹配任意数字，等价于 [0-9].\\D    匹配任意非数字\\w    匹配字母数字及下划线\\W    匹配非字母数字及下划线\\s    匹配任意空白字符，等价于 [\\t\\n\\r\\f].\\S    匹配任意非空字符\n\n字符组：要求在一个位置匹配的字符可能出现很多种情况, 各种情况组成一个组123456[0123456789]: 匹配0到9任意字符[0-9]: 同上[a-z]: 匹配a到z的任意小写字母[A-Z]: 匹配A到Z的任意大写字母[0-9a-fA-F]: 以上三种的组合, 匹配0-9任意数组或a到f之间任意字母, 不区分大小写自定义字符组:[a3h5]  ---&gt;  代表匹配a, 3, h, 5等字符\n\n量词:1234567*  \t重复零次或更多次+\t重复一次或更多次?\t重复零次或一次&#123;n&#125;\t重复n次&#123;n,&#125;\t重复n次或更多次&#123;n,m&#125;\t重复n到m次\n\n\n边界修饰符12^\t匹配开始$\t匹配结尾\n\n分组123456在正则表达式中添加(), 就形成了一个分组, 在re模块中优先匹配显示分组内容import res &#x3D; &quot;&lt;a href&#x3D;&#39;www.baidu.com&#39;&gt;正则匹配实验&lt;&#x2F;a&gt;&quot;res &#x3D; re.findall(&quot;href&#x3D;&#39;(.*)&#39;&gt;&quot;, s)print(res)\n\n匹配模式123re.S  单行模式re.M  多行模式\tre.I 忽略大小写\n\n贪婪匹配与非贪婪匹配123贪婪匹配是指: 在使用量词:  * ,  +  等时, 尽可能的匹配内容非贪婪匹配是指: 使用?对正则表达式进行修饰, 使量词的匹配尽可能少, 如+代表匹配1次或多次, 在?的修饰下, 只匹配1次.\n\nre模块\nre.findall(‘正则表达式’, ‘待匹配字符串’): 返回所有满足匹配条件的结果, 以列表形式返回\nre.search(‘正则表达式’, ‘带匹配字符串’): 匹配到第一个就返回一个对象, 该对象使用group()进行取值, 如果为匹配到则返回None\nre.match(‘正则表达式’, ‘待匹配字符串’): 只从字符串开始进行匹配, 如果匹配成功返回一个对象,同样使用group()进行取值, 匹配不成功返回None\nre.compile(‘正则表达式’): 将正则表达式编译为对象, 但需要按该正则表达式匹配是可以在直接使用该对象调用以上方法即可.\n\nre模块示例:1234567891011121314151617181920212223s &#x3D; &quot;abcabc你好&quot;# findall方法演示res_findall &#x3D; re.findall(&#39;a&#39;, s)print(&#39;findall匹配结果:&#39;, res_findall)# search方法演示, 不确定是否能匹配出结果, 不可直接使用group进行取值, 需要判断或进行异常处理res_search &#x3D; re.search(&quot;你&quot;, s)if res:    print(&#39;search匹配结果&#39;, res.group())else:    print(&#39;None&#39;)    # match方法演示:res_match_1 &#x3D; re.match(&#39;abc&#39;, s)res_match_2 &#x3D; re.match(&#39;bc&#39;, s)print(&#39;res_match_1结果:&#39;, res_match_1)print(&#39;res_match_2结果:&#39;, res_match_2)# compile方法演示:re_obj &#x3D; re.compile(&#39;ab&#39;)res &#x3D; re_obj.findall(s)print(res)\n\n利用正则表达式抓取校花网图片1234567891011121314151617181920import reimport requests# 请求url, 抓取页面url &#x3D; &#39;http:&#x2F;&#x2F;www.xiaohuar.com&#x2F;hua&#x2F;&#39;headers &#x3D; &#123;    &#39;User-Agent&#39;:&#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;75.0.3770.142 Safari&#x2F;537.36&#39;&#125;res &#x3D; requests.get(url&#x3D;url, headers&#x3D;headers)html &#x3D; res.text# 利用正则匹配页面中的img标签, 获取其src属性值all_src &#x3D; re.findall(&#39;&lt;img.*src&#x3D;&quot;(.*jpg)&quot;&#39;, html, re.M)# 遍历获取到的图片连接, 处理连接为完整的url, 再次请求抓取图片二进制流数据, 写入文件for num, url in enumerate(all_src):    url &#x3D; &#39;http:&#x2F;&#x2F;www.xiaohuar.com&#39; + url    image &#x3D; requests.get(url&#x3D;url, headers&#x3D;headers)    with open(&#39;%s.jpg&#39; % num, &#39;wb&#39;) as f:        f.write(image.content)\n\n\n2.BS4BS4解析库解析数据原理:定位标签节点  —&gt;  提取标签节点内容或属性值\n\nBS4基本使用步骤:12345from BS4 import BeautifulSoup# 网络文件实例化对象soup &#x3D; BeautifulSoup(text, &#39;lxml&#39;)  soup &#x3D; BeautifulSoup(open(&#39;filename&#39;, &#39;lxml&#39;)tag_element_or_attribute &#x3D; soup.选择器\n\nBS4可用选择器:节点选择器:1234567891011121314- 节点名单选: soup.div- 嵌套选择: soup.div.span- 关联选择:     - 子节点:contents返回字节子节点, 包含换行符, 返回类型是一个列表            children效果相同, 但返回一个生成器    - 子孙节点:descendants, 返回生成器遍历取值        1).深度选择, 从第一个子节点开始, 直至第一个子节点内的所有孙节点全部选择到        2).如果两个标签在两行上, 匹配第一个节点的所有深度后选择换行符.        3).如果标签中的文本是换行的, 则不单独匹配换行符, 换行符包含在文本内    - 父节点:parent    - 祖先节点:parents    - 兄弟节点:            next_sibling: 当前节点的下一个兄弟节点            previous_sibling: 当前节点的上一个兄弟节点\n\n\n方法选择器:12345678910111213141).find_all(name, attrs, recursive, text, **kwargs)        - 标签名选择:soup.find_all(name&#x3D;&#39;ul&#39;)   # 选择所有ul标签        - 嵌套选择:        for ul in soup.find_all(name&#x3D;&#39;ul&#39;):               ul.find_all(name&#x3D;&#39;li&#39;)  # 循环ul列表, 选择每个ul中的li标签    - 属性值选择:soup.find_all(attrs&#x3D;&#123;&#39;class&#39;: &#39;element&#39;&#125;), # 根据属性选择节点,等效:soup.find_all(class&#x3D;&#39;element&#39;)        - 文本正则选择:soup.find_all(text&#x3D;re.compile(&#39;link&#39;))  # 返回所有标签中包含link字符的文字的对象2).find():返回一个对象\n\ncss选择器:12345678910111213141516171819css选择器需要调用select方法, 改方法返回一个列表1).选择:        soup.select(&#39;ul li&#39;)  # 选择ul下的li所有标签        soup.select(&#39;.panel&#39;)  # 选择class的值为panel的标签        soup.select(&#39;#item1&#39;)  # 选择id为item1的标签2).嵌套选择:    for ul in soup.select(&#39;ul&#39;):        ul.select(&#39;li&#39;)3).获取文本与属性:    获取属性:        for li in soup.select(&#39;ul li&#39;):            print(li.attrs[&#39;id&#39;])            print(li[&#39;id&#39;])   # 上下两种形式效果一致    获取文本:        for li in soup.select(&#39;ul li&#39;):            print(li.get_text())            print(li.string)\n\n节点文本或属性的获取1234567tag_element.name  # 获取节点名称tag_element.attrs  # 获取节点所有属性, 结果为字典形式tag_element.attrs[&#39;name&#39;]  # 获取节点的单个属性值, 等效: tag_element[&#39;name&#39;], 属性多值是返回列表tag_element.string   # 获取节点的文本内容如果使用关联选择, 且结果为生成器可以先转为列表再索引定位元素后在调用上面的获取元素方法, 如:list(p.parents)[0].attrs[&#39;name&#39;]\n\n练习示例123456789101112131415161718192021222324252627282930313233343536373839404142434445html &#x3D; &#39;&#39;&#39;    &lt;div class&#x3D;&quot;panel&quot;&gt;        &lt;div class&#x3D;&quot;panel-heading&quot;&gt;            &lt;h4&gt;BeautifulSoup练习&lt;&#x2F;h4&gt;        &lt;&#x2F;div&gt;        &lt;div class&#x3D;&quot;panel-body&quot;&gt;            &lt;ul class&#x3D;&quot;list&quot; id&#x3D;&quot;list-1&quot;&gt;                &lt;li class&#x3D;&quot;element&quot;&gt;第一个li标签&lt;&#x2F;li&gt;                &lt;li class&#x3D;&quot;element&quot;&gt;第二个li标签&lt;&#x2F;li&gt;                &lt;li class&#x3D;&quot;element&quot;&gt;第三个li标签&lt;&#x2F;li&gt;            &lt;&#x2F;ul&gt;            &lt;ul class&#x3D;&quot;list list-small&quot;&gt;                &lt;li class&#x3D;&quot;element&quot;&gt;one&lt;&#x2F;li&gt;                &lt;li class&#x3D;&quot;element&quot;&gt;two&lt;&#x2F;li&gt;            &lt;&#x2F;ul&gt;            &lt;li class&#x3D;&quot;element&quot;&gt;测试多层级选择器&lt;&#x2F;li&gt;        &lt;&#x2F;div&gt;    &lt;&#x2F;div&gt;&#39;&#39;&#39;from bs4 import BeautifulSoupsoup &#x3D; BeautifulSoup(html, &#39;lxml&#39;)# 1.根据节点名定位节点, 获取其文本h4 &#x3D; soup.select(&#39;h4&#39;)   # 标签选择器print(h4[0].get_text())# 2.根据class属性定位节点panel &#x3D; soup.select(&#39;.panel-heading&#39;)print(panel)# 3.根据id属性定位节点ul &#x3D; soup.select(&#39;#list-1&#39;)print(ul)# 4.嵌套选择ul_list &#x3D; soup.select(&#39;ul&#39;)for ul in ul_list:    li &#x3D; ul.select(&#39;li&#39;)    print(li)    # 5.单层级选择器与多层级选择器li_list_single &#x3D; soup.select(&quot;.panel-body &gt; ul &gt; li&quot;)li_list_multi &#x3D; soup.select(&quot;.panel-body li&quot;)\n\n爬取整部三国演义销售, 写入txt文件:123456789101112131415161718192021import requestsfrom bs4 import BeautifulSoupurl &#x3D; &#39;http:&#x2F;&#x2F;www.shicimingju.com&#x2F;book&#x2F;sanguoyanyi.html&#39;headers &#x3D; &#123;    &#39;User-Agent&#39;:&#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;75.0.3770.142 Safari&#x2F;537.36&#39;&#125;res &#x3D; requests.get(url&#x3D;url, headers&#x3D;headers)soup &#x3D; BeautifulSoup(res.text, &#39;lxml&#39;)a_list &#x3D; soup.select(&quot;.book-mulu ul li a&quot;)for item in a_list:    name &#x3D; item.string    href &#x3D; item[&quot;href&quot;]    # print(href)    full_url &#x3D; &#39;http:&#x2F;&#x2F;www.shicimingju.com&#39; + href    detail_page &#x3D; requests.get(url&#x3D;full_url, headers&#x3D;headers).text    soup_detail &#x3D; BeautifulSoup(detail_page, &#39;lxml&#39;)    div &#x3D; soup_detail.select(&quot;.chapter_content&quot;)[0]    print(type(div.get_text()))    with open(&#39;%s.txt&#39; % name, &#39;w&#39;, encoding&#x3D;&quot;utf-8&quot;) as f:        f.write(div.get_text())\n\n3.xpathxpath解析库的介绍数据解析的过程中使用过正则表达式, 但正则表达式想要进准匹配难度较高, 一旦正则表达式书写错误, 匹配的数据也会出错.网页由三部分组成: HTML, Css, JavaScript, HTML页面标签存在层级关系, 即DOM树, 在获取目标数据时可以根据网页层次关系定位标签, 在获取标签的文本或属性.\n\nxpath解析库解析数据原理:\n根据网页DOM树定位节点标签\n获取节点标签的正文文本或属性值\n\n\nxpath安装1pip install lxml\n\nrequests模块爬取糗事百科热门的标题:1234567891011121314import requestsfrom lxml import etreeurl &#x3D; &#39;https:&#x2F;&#x2F;www.qiushibaike.com&#x2F;&#39;headers &#x3D; &#123;    &quot;User-Agent&quot;:&#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;75.0.3770.142 Safari&#x2F;537.36&#39;&#125;res &#x3D; requests.get(url&#x3D;url, headers&#x3D;headers)tree &#x3D; etree.HTML(res.text)title_lst &#x3D; tree.xpath(&#39;&#x2F;&#x2F;ul&#x2F;li&#x2F;div&#x2F;a&#x2F;text()&#39;)for item in title_lst:    print(item)\n\nxpath使用步骤123456from lxml import etreetree &#x3D; etree.HTML(res.text)tree &#x3D; etree.parse(res.html, etree.HTMLParse())  # 示例如下, 了解内容tag_or_attr &#x3D; tree.xpath(&#39;xpath表达式&#39;)\n\nxpath语法:12345678910111213141516171819201.常用规则:      1.  nodename:\t\t  节点名定位      2.  &#x2F;&#x2F;:\t\t\t  从当前节点选取子孙节点      3.  &#x2F;:\t\t\t  从当前节点选取直接子节点      4.  nodename[@attribute&#x3D;&quot;...&quot;]  根据属性定位标签      5.  @attributename:  获取属性       6.  text():\t\t   获取文本   2.属性匹配两种情况: 多属性匹配 &amp;  单属性多值匹配      2.1 单属性多值匹配    示例: tree.xpath(&#39;&#x2F;&#x2F;div[contains(@class, &quot;dc&quot;)]&#x2F;text()&#39;)    2.2 多属性匹配    示例: tree.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;item&quot; and @name&#x3D;&quot;test&quot;]&#x2F;text()&#39;) \t    3.按序选择:    3.1 索引定位: 从1开始    3.2 last()函数    3.3 position()函数\n\n获取响应数据内容:res.text        获取HTML文本\nres.content        获取二进制流\nres.json()        获取json数据\n\n解析示例: 示例解析的是本地文件本地文件12345678910111213141516171819202122232425262728293031&lt;!DOCTYPE html&gt;&lt;html lang&#x3D;&quot;en&quot;&gt;&lt;head&gt;    &lt;meta charset&#x3D;&quot;UTF-8&quot;&gt;    &lt;title&gt;Xpath练习文件&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;&lt;body&gt;&lt;div id&#x3D;&quot;007&quot;&gt;    &quot;我是div标签的文字内容, 和下面的p标签还有div标签是同级的哦&quot;    &lt;p&gt;这是p标签内的文字内容&lt;&#x2F;p&gt;    &lt;div&gt;这是p标签同级的div标签&lt;&#x2F;div&gt;&lt;&#x2F;div&gt;&lt;div class&#x3D;&quot;divtag&quot;&gt;    &lt;ul&gt;        &lt;li&gt;第1个li标签&lt;&#x2F;li&gt;        &lt;li&gt;第2个li标签&lt;&#x2F;li&gt;        &lt;li&gt;第3个li标签&lt;&#x2F;li&gt;        &lt;li&gt;第4个li标签&lt;&#x2F;li&gt;        &lt;li&gt;第5个li标签&lt;&#x2F;li&gt;    &lt;&#x2F;ul&gt;    &lt;a href&#x3D;&quot;https:&#x2F;&#x2F;www.baidu.com&quot;&gt;这是百度的跳转连接&lt;&#x2F;a&gt;&lt;&#x2F;div&gt;&lt;div class&#x3D;&quot;c1&quot; name&#x3D;&quot;laoda&quot;&gt;老大在此&lt;&#x2F;div&gt;&lt;div class&#x3D;&quot;c1 c3&quot; name&#x3D;&quot;laoer&quot;&gt;老二任性, class有两个值&lt;&#x2F;div&gt;&lt;div class&#x3D;&quot;c1&quot; name&#x3D;&quot;laosan&quot;&gt;我是老三&lt;&#x2F;div&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt;\n解析本地文件1234567891011121314151617181920212223242526272829303132333435363738394041from lxml import etreetree &#x3D; etree.parse(&#39;.&#x2F;x.html&#39;, etree.HTMLParser())# 1.根据节点名, 即nodename定位title标签, 获取标签内文字title_text &#x3D; tree.xpath(&#39;&#x2F;&#x2F;title&#x2F;text()&#39;)print(title_text)# 2.根据节点属性定位: 定位id为007的div标签div_007 &#x3D; tree.xpath(&#39;&#x2F;&#x2F;div[@id&#x3D;&quot;007&quot;]&#39;)print(div_007)# 3.示例直接子节点与子孙节点:&#x2F;, &#x2F;&#x2F;div_007_one &#x3D; tree.xpath(&#39;&#x2F;&#x2F;div[@id&#x3D;&quot;007&quot;]&#x2F;text()&#39;)print(div_007_one)div_007_two &#x3D; tree.xpath(&#39;&#x2F;&#x2F;div[@id&#x3D;&quot;007&quot;]&#x2F;&#x2F;text()&#39;)print(div_007_two)# 4.获取a标签的href属性a_href &#x3D; tree.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;divtag&quot;]&#x2F;a&#x2F;@href&#39;)print(a_href)# 4.多属性定位: 根据class属性和name属性定位div标签div_two_attr &#x3D; tree.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;c1&quot; and @name&#x3D;&quot;laoda&quot;]&#x2F;text()&#39;)print(div_two_attr)# 5.属性多值定位: 定位所有class中有c1的div标签div_c1 &#x3D; tree.xpath(&#39;&#x2F;&#x2F;div[contains(@class, &quot;c1&quot;)]&#39;)# 6.按序定位li_first &#x3D; tree.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;divtag&quot;]&#x2F;ul&#x2F;li[1]&#x2F;text()&#39;)  # 定位第一个li标签, 获取其文本print(li_first)li_last &#x3D; tree.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;divtag&quot;]&#x2F;ul&#x2F;li[last()]&#x2F;text()&#39;)  # 定位最后一个li标签print(li_last)li_daotwo &#x3D; tree.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;divtag&quot;]&#x2F;ul&#x2F;li[last()-1]&#x2F;text()&#39;)  # 定位倒数第二个li标签print(li_daotwo)li_qianthree &#x3D; tree.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;divtag&quot;]&#x2F;ul&#x2F;li[position()&lt;4]&#x2F;text()&#39;)  # 定位前三个li标签print(li_qianthree)","categories":["爬虫"],"tags":["python"]},{"title":"requests模块初级用法&高级用法","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/07/17/requests模块初级用法&高级用法/","content":"1.requests模块初级用法requests库的安装1pip install requests\n\nget请求:\n不携带参数的get请求\n不携带参数的get请求 + headers\n携带参数的get请求 + headers\n\npost请求:\n构建参数的post请求\n\n响应数据的获取与属性(1).响应数据的获取:\n        res.text: 文本数据\n        res.json(): json数据\n        res.content: 流\n(2).响应的其他属性:\n        res.status_code: 获取响应状态码\n        res.headers: 响应头\n        res.cookie: cookie信息\n\nrequests模块的get请求1.不携带参数的get请求: 爬取搜狗主页12345678import requestsurl &#x3D; &#39;https:&#x2F;&#x2F;www.sogou.com&#x2F;&#39;res &#x3D; requests.get(url&#x3D;url)print(res)print(res.text)with open(&#39;sougou.html&#39;, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:    f.write(res.text)\n\n2.不携带参数的get请求  +  headers: 爬取知乎的发现页12345678import requestsheaders &#x3D; &#123;    &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;75.0.3770.142 Safari&#x2F;537.36&#39;&#125;url &#x3D; &#39;https:&#x2F;&#x2F;www.zhihu.com&#x2F;explore&#39;res &#x3D; requests.get(url&#x3D;url, headers&#x3D;headers)with open(&#39;zhihu.html&#39;, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:    f.write(res.text)\n\n3.携带参数的get请求  +  headers: 知乎的发现栏中搜索Python123456789101112131415import requestsheaders &#x3D; &#123;    &#39;User-Agent&#39;:&#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;75.0.3770.142 Safari&#x2F;537.36&#39;&#125;url&#x3D; &#39;https:&#x2F;&#x2F;www.zhihu.com&#x2F;search?&#39;params &#x3D; &#123;    &#39;type&#39;:&#39;content&#39;,    &#39;q&#39;:&#39;python&#39;&#125;res &#x3D; requests.get(url&#x3D;url, headers&#x3D;headers, params&#x3D;params)print(res)print(res.text)with open(&#39;python.html&#39;, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:    f.write(res.text)\n\n4.requests模块的post请求:以post方式请求httpbin.org/post时会返回提交的请求信息1234567891011import requestsheaders &#x3D; &#123;     &#39;User-Agent&#39;:&#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;75.0.3770.142 Safari&#x2F;537.36&#39;&#125;url &#x3D; &#39;http:&#x2F;&#x2F;httpbin.org&#x2F;post&#39;data &#x3D; &#123;    &#39;name&#39;: &#39;spiderman&#39;,    &#39;age&#39;: 8&#125;res &#x3D; requests.post(url&#x3D;url, headers&#x3D;headers, data&#x3D;data)print(res.text)\n以post方式请求百度翻译返回请求信息123456789101112131415161718import requests#  确定url,发送请求，响应数据url &#x3D; &#39;https:&#x2F;&#x2F;fanyi.baidu.com&#x2F;sug&#39;while 1:    kw &#x3D; input(&#39;请输入需要查询的单词：&#39;)    if kw &#x3D;&#x3D; &#39;q&#39;:        break    params &#x3D; &#123;        &#39;kw&#39;:kw    &#125;    headers &#x3D; &#123;        &#39;User-agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.106 Safari&#x2F;537.36&#39;    &#125;    res &#x3D; requests.post(url &#x3D; url,headers &#x3D; headers,params &#x3D; params).json()    # print(res)    kdata &#x3D; res[&#39;data&#39;][0][&#39;v&#39;]    print(kdata)\n\njson形式与流形式的响应数据示例1.json形式响应数据示例: bilibili的Python视频教程, 目录列表1234567891011import requestsheaders &#x3D; &#123;     &#39;User-Agent&#39;:&#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;75.0.3770.142 Safari&#x2F;537.36&#39;&#125;url &#x3D; &#39;https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;web-interface&#x2F;view?aid&#x3D;14184325&amp;cid&#x3D;23153678&#39;res &#x3D; requests.get(url&#x3D;url, headers&#x3D;headers)print(res)print(res.status_code)print(res.headers)print(&#39;~~~~~~&#39;)print(res.json())\n\n2.流形式响应数据示例: 站长素材的简历模板123456789import requestsheaders &#x3D; &#123;     &#39;User-Agent&#39;:&#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;75.0.3770.142 Safari&#x2F;537.36&#39;&#125;url &#x3D; &#39;http:&#x2F;&#x2F;fjdx.sc.chinaz.net&#x2F;Files&#x2F;DownLoad&#x2F;jianli&#x2F;201907&#x2F;jianli10810.rar&#39;res &#x3D; requests.get(url&#x3D;url, headers&#x3D;headers)print(res.content)with open(&#39;janli.rar&#39;, &#39;wb&#39;) as f:    f.write(res.content)\n\n\n2.requests模块高级用法文件上传功能12345678import requests# 定义上传文件数据, 键为file, 值为文件句柄files &#x3D; &#123;&#39;file&#39;: open(&#39;favicon.ico&#39;, &#39;rb&#39;)&#125;r &#x3D; requests.post(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;post&#39;, files&#x3D;files)print(res.text)\n\ncookie处理方法\nheaders添加cookie键值对\nRequestsCookieJar对象\n\n\nheaders内添加cookie键值对处理cookie1234567891011import requestsurl &#x3D; &#39;https:&#x2F;&#x2F;www.baidu.com&#39;headers &#x3D; &#123;    &#39;User-Agent&#39;:&#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;75.0.3770.142 Safari&#x2F;537.36&#39;,    &#39;Cookies&#39;:&#39;BAIDUID&#x3D;79A570F8D90B2C45E42D40A3666ADC46:FG&#x3D;1; BIDUPSID&#x3D;79A570F8D90B2C45E42D40A3666ADC46; PSTM&#x3D;1551074009; BD_UPN&#x3D;12314753; sugstore&#x3D;0; BDORZ&#x3D;FFFB88E999055A3F8A630C64834BD6D0; yjs_js_security_passport&#x3D;10c9ca61409abe70ac5c03db796f78648e697d8f_1563711806_js; COOKIE_SESSION&#x3D;2860_2_2_7_3_5_0_0_2_4_106_0_3778_177561_116_109_1563714759_1563714752_1563714643%7C9%23177557_14_1563714643%7C7; delPer&#x3D;0; BD_HOME&#x3D;0; H_PS_PSSID&#x3D;1452_21117_29522_29521_28519_29099_28831_29221&#39;&#125;res &#x3D; requests.get(url&#x3D;url, headers&#x3D;headers)res.encoding &#x3D; &#39;utf-8&#39;with open(&#39;baidu_cookie.html&#39;, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:    f.write(res.text)\n\nRequestsCookieJar对象处理cookie: 用cookie维持百度登陆123456789101112import requestscookies &#x3D; &#39;BAIDUID&#x3D;79A570F8D90B2C45E42D40A3666ADC46:FG&#x3D;1; BIDUPSID&#x3D;79A570F8D90B2C45E42D40A3666ADC46; PSTM&#x3D;1551074009; BD_UPN&#x3D;12314753; sugstore&#x3D;0; BDORZ&#x3D;FFFB88E999055A3F8A630C64834BD6D0; yjs_js_security_passport&#x3D;10c9ca61409abe70ac5c03db796f78648e697d8f_1563711806_js; COOKIE_SESSION&#x3D;2860_2_2_7_3_5_0_0_2_4_106_0_3778_177561_116_109_1563714759_1563714752_1563714643%7C9%23177557_14_1563714643%7C7; delPer&#x3D;0; BD_HOME&#x3D;0; H_PS_PSSID&#x3D;1452_21117_29522_29521_28519_29099_28831_29221; BDUSS&#x3D;lSVnBVVkRVNFpNZ2ZJZ2ZpNFpjblFFSX5EaW9DNzBpcnNkaDZIQVdRd2Z1bHhkRVFBQUFBJCQAAAAAAAAAAAEAAABwfMtW09rQodPjMDgyMGZyZWUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB8tNV0fLTVdYX&#39;jar &#x3D; requests.cookies.RequestsCookieJar()headers &#x3D; &#123;    &#39;User-Agetn&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;75.0.3770.142 Safari&#x2F;537.36&#39;&#125;for cookie in cookies.split(&#39;;&#39;):    key, value &#x3D; cookie.split(&#39;&#x3D;&#39;, 1)    jar.set(key, value)    res &#x3D; requests.get(&#39;http:&#x2F;&#x2F;www.baidu.com&#39;, cookies&#x3D;jar, headers&#x3D;headers)print(res.text)  # 响应数据中包含用户名信息, 说明cookie生效\n\n会话维持HTTP无状态:使用requests模块中的get()和post()方法请求网页时, 每一次请求都是独立的, 没有连续请求之间的状态保持. 假象, 如果你登陆了淘宝后向查看订单, 那么如果没有状态的维持就无法实现.\n\n会话的维持: Session对象123from requests import Sessions &#x3D; Session()res &#x3D; s.get(&#39;https:&#x2F;&#x2F;www.baidu.com&#39;)\n\nSSL证书验证1.SSL证书验证requests提供了证书验证的功能. 当发起HTTP请求时, 模块会检查SSL证书. 但检查的行为可以用verify参数来控制.\n12verify &#x3D; False  # 不检查SSL证书verify &#x3D; True  # 检查SSL证书\n\n2.异常如果使用requests模块的SSL验证, 验证不通过会抛出异常, 此时可以将verify参数设置为False\n3.www.12306.cn的证书验证:会抛出异常123import requestsresponse &#x3D; requests.get(&#39;https:&#x2F;&#x2F;www.12306.cn&#39;)print(response.status_code)\n\n不抛异常, 但会出现警告123import requestsresponse &#x3D; requests.get(&#39;https:&#x2F;&#x2F;www.12306.cn&#39;, verify&#x3D;False)print(response.status_code)\n\n禁止警告12345import requestsfrom requests.packages import urllib3urllib3.disable_warnings()response &#x3D; requests.get(url&#x3D;&#39;https:&#x2F;&#x2F;www.12306.cn&#39;, verify&#x3D;False)print(response.status_code)\n\n\n代理设置代理: 代理即代理ip代理ip是指在请求的过程中使用非本机ip进行请求, 避免大数据量频繁请求的过程中出现ip封禁, 限制数据的爬取.\n\n代理ip分类:\n透明代理ip: 请求时, 服务器知道请求的真实ip, 知道请求使用了代理\n匿名代理ip: 请求时, 服务器知道请求使用了代理, 但不知道请求的真实ip\n高匿代理ip: 请求时, 服务器不知道请求使用了代理, 也不知道请求的真实ip\n\n\nrequests模块使用代理ip1234567import requestsurl &#x3D; &#39;http:&#x2F;&#x2F;www.httpbin.org&#39;proxies &#x3D; &#123;    &#39;http&#39;: &#39;http:&#x2F;&#x2F;61.183.176.122:57210&#39;&#125;res &#x3D; requests.get(url&#x3D;url, proxies&#x3D;proxies)print(res.text)\n\n\n超时设置由于网络状况的不同, 服务器配置差异以及服务器处理并发的能力不同, 有时会出现服务器的响应时间过长, 甚至无法获取响应而抛出异常. requests模块发送请求可以设置超时时间, 在超时时间内未得到响应, 便会抛出异常.一方面, 减少了请求的阻塞时间, 一方面, 可以进行异常处理, 执行相应的操作.\n12345678import requestsurl &#x3D; &#39;https:&#x2F;&#x2F;www.baidu.com&#39;headers &#x3D; &#123;    &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;75.0.3770.142 Safari&#x2F;537.36&#39;&#125;res &#x3D; requests.get(url&#x3D;url, headers&#x3D;headers, timeout&#x3D;0.001)  # 在0.001秒为得到响应, 抛出requests.exceptions.ConnectTimeout异常print(res.text)\n\n\n构建Request对象12345678910111213141516171819202122232425# 1.Prepared Request利用 Prepared Request 数据结构构件Request对象. 其构建及使用步骤如下:from requests import Request, Session# 构建Request对象url &#x3D; &#39;...&#39;data &#x3D; &#123;...&#125;params &#x3D; &#123;...&#125;headers &#x3D; &#123;...&#125;session &#x3D; Session()# 构建post请求:req_post &#x3D; Request(method&#x3D;&#39;POST&#39;, url&#x3D;url, headers&#x3D;headers, data&#x3D;data)req_obj_post &#x3D; session.prepare_request(req_post)# 构建get请求:req_get &#x3D; Request(method&#x3D;&#39;GET&#39;, url&#x3D;url, headers&#x3D;headers, params&#x3D;params)req_obj_get &#x3D; session.prepare_request(req_get)# 利用构建的请求对象, 向服务器发送请求res &#x3D; session.send(req_obj_post)res &#x3D; session.send(req_obj_get)# 应用:通过此方法, 我们可以构建一个独立的request对象, 当需要请求的url很多时, 我们可以为每一个url构建一个request对象, 将所有request对象置于队列中, 便于调度.\n\n\n构建Request对象, 请求糗事百科获取页面123456789101112from requests import Request, Sessionurl &#x3D; &#39;https:&#x2F;&#x2F;www.qiushibaike.com&#x2F;&#39;headers &#x3D; &#123;    &quot;User-Agent&quot;:&#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;75.0.3770.142 Safari&#x2F;537.36&#39;&#125;session &#x3D; Session()req_get &#x3D; Request(url&#x3D;url, headers&#x3D;headers, method&#x3D;&#39;GET&#39;)req_get_obj &#x3D; session.prepare_request(req_get)res &#x3D; session.send(req_get_obj)res.encoding &#x3D; &#39;utf-8&#39;with open(&#39;qb_reqobj.html&#39;, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:    f.write(res.text)\n\nurllib简单介绍urllib简介:\nurllib模块是Python的一个请求模块\nPython2中是urllib和urllib2相结合实现请求的发送. Python3中同一为urllib库\nurllib是Python内置的请求库, 其包含4个模块:\nrequest模块: 模拟发送请求\nerror模块: 异常处理模块\nparse模块: 工具模块, 提供关于URL的处理方法, 如拆分, 解析, 合并等\nrobotparser模块: 识别robots协议\n\n\n\n\n","categories":["爬虫"],"tags":["python"]},{"title":"爬虫概述&协议&请求","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/07/11/爬虫概述&协议&请求/","content":"1.爬虫概念爬虫, 又称网页蜘蛛或网络机器人，爬虫是 模拟人操作客户端(浏览器, APP) 向服务器发起网络请求 抓取数据的自动化程序或脚本.\n说明:\n模拟: 用爬虫程序伪装出人的行为, 避免被服务识别为爬虫程序\n客户端: 浏览器, APP都可以实现人与服务器之间的交互行为, 应用客户端从服务器获取数据\n自动化: 数据量较小时可以人工获取数据, 但往往在公司中爬取的数据量在百万条, 千万条级别的, 所以要程序自动化获取数据.\n\n\n2.爬虫语言爬虫语言：1PHP, C&#x2F;C++, Java, Python\n对比：\nPHP: 并发能力差, 对多进程和多线程支持不好, 数据量较大时爬虫效率较低\nC/C++: 语言效率高, 但学习成本高, 对程序员的技术能力要求较高,  所以目前还停留在研究层面, 市场需求量很小\nJava: Python爬虫的主要竞争对手, 由于Java语言的特点, 代码臃肿, 代码量大, 维护成本重构成本高, 开发效率低. 但目前市场上岗位需求比较旺盛.\nPython:语法简单, 学习成本较低, 对新手比较友好. Python语言良好的生态, 大量库和框架的支持是的Python爬虫目前处于爬虫圈的主导地位.\n\n\n3.爬虫分类 a. 通用爬虫\n b. 聚焦爬虫\n通用爬虫\n通用爬虫：约定哪些内容允许哪些爬虫抓取—&gt;搜索引擎\n\n实例: 百度, 搜狗, Google的搜索引擎\n\n功能: 访问网页 -&gt; 抓取数据 -&gt; 数据处理 -&gt; 提供检索服务\n\n工作流:\n\n给定一个起始URL, 存于爬取队列中\n爬虫程序从队列中取出url, 爬取数据\n解析爬取数据, 获取网页内的所有url, 放入爬取队列\n重复第二个步骤\n\n\n使搜索引擎获取网站链接:\n\n主动将url提交各搜索引擎\n在其他热门网站设置友情了解\n百度和DNS服务商合作, 收录新网站\n\n\n网站排名(SEO):\n\n根据PageRank值进行排名(流量, 点击率)\n百度竞价排名, 钱多就靠前排\n\n\n缺点:\n\n抓取的内容多数无用\n无法精确获取数据\n\n\n\n协议: 无需遵守robots协议\n\n查看方法：网站url/robots.txt, 如https://www.baidu.com/robots.txt\n\n\n聚焦爬虫\n概念：聚焦爬虫指针对某一领域根据特定要求实现的爬虫程序, 抓取需要的数据(垂直领域爬取)\n设计思路：\n确定爬取的url, 模拟浏览器请服务器发送请求:requests,urllib,aiohttp\n获取响应数据并进行数据解析:xpath,bs4,pyquery,正则\n将目标数据持久化：文件，关系型数据库，非关系型数据库\n\n\n\n1234567891011121314# 需求: 爬取百度首页, 并写入文件中, 最后用浏览器打开文件查看效果import requests# 1.确定url, 向服务器发送请求url &#x3D; &#39;https:&#x2F;&#x2F;www.baidu.com&#39;res &#x3D; resquests.get(url&#x3D;url)# 2.操作响应数据, 获取目标数据res.encoding &#x3D; &#39;utf-8&#39;# 3.将目标数据持久化到本地: 写入文件with open(&#39;baidu.html&#39;, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:    f.write(res.text)\n\n\n4.爬虫反爬机制与反反爬策略\nUA检测\nrobot协议\n动态数据加载: JS动态数据加载, Ajax数据加载\nIP封禁\n账号封禁\n验证码\n数据加密\n隐藏参数\n图片懒加载\n\n\n5.协议HTTP协议\nHTTP协议: 明文传输, 端口80\nHttp协议, 全称为Hyper Text Transfer Protocol, 即超文本传输协议.\nHTTP协议是用于从网络传输超文本数据到本地浏览器的传送协议, 它能保证高效而准确地传送超文本文档.\n目前广泛使用的是HTTP 1.1版本HTTPS协议\nHTTPS协议: 加密传输, 端口443\nHTTPS全称是Hyper Text Transfer Protocol over Secure Socket Layer, 是以安全为目标的HTTP通道. HTTPS协议实质是HTTP的安全版, 即HTTP下加入SSL层, 简称HTTPS.\nHTTPS的安全体现在SSL的加密行为, 即通过HTTPS协议传输的数据都是经过SSL加密的\nHTTPS的作用:\n建立一个信息安全的通道来保证数据传输的安全\n确认网站的真实性, 凡是使用了HTTPS的网站, 都可以通过点击浏览器地址栏的锁头标志来查看网站认证之后的真实信息, 也可以通过CA机构颁发的安全签章来查询\n\n\n\n\n6.服务器常见端口\nftp：File Transfer Protocol的缩写, 即文件传输协议. 端口：21\nssh：Secure Shell的缩写, 用于远程登录会话. 端口：22\nMySQL：关系型数据库, 端口：3306\nMongoDB：非关系型数据库, 端口：27017\nRedis：非关系型数据库, 端口：6379\n\n\n7.开发准备web端123Python3.6PycharmGoogle Chrome\nAPP123fiddler抓包工具Charles抓包工具Appium自动化测试工具\n\n8.请求过程与网页基础URL介绍HTTP请求过程请求过程:客户端, 通常指web浏览器或APP向服务器发起请求, 服务器接收到请求进行处理, 并向客户端发起响应.\n\n9.请求由客户端向服务器发出的,可以分为四部分内容:  \n\n请求方法(Request Method) \n请求网址(Request URL)\n请求头(Request Headers)\n请求体(Request Body)\n\n\n请求方法:常见有8种12345678- GET：请求页面, 并返回页面内容 - POST：用于提交表单数据或上传文件, 数据包含在请求体中  - PUT：从客户端向服务器传送的数据取代指定文档中的内容- DELETE：请求服务器删除指定的页面- HEAD：类似于GET请求，只不过返回的响应中没有具体的内容，用于获取报头- CONNECT：把服务器当作跳板，让服务器代替客户端访问其他网页- OPTIONS：允许客户端查看服务器的性能- TRACE：回显服务器收到的请求，主要用于测试或诊断\n\n重点掌握GET &amp; POST的区别:\nGET请求中的参数包含在URL里面, 数据可以在URL中看到, 而POST请求的URL不会包含这些数据, POST的数据都是通过表单形式传输的, 会包含在请求体中\nGET请求提交的数据最多只有1024字节, 而POST方式没有限制\n\n\n请求网址:请求的网址，即统一资源定位符URL，它可以唯一确定我们想请求的资源\n\n请求头:请求头:用来说明服务器要使用的附加信息. 重点: Accept, Cookie, Referer, User-Agent\n\nAccept：请求报头域，用于指定客户端可接受哪些类型的信息。 \nCookie：也常用复数形式 Cookies，这是网站为了辨别用户进行会话跟踪而存储在用户本地的数据。它的主要功能是维持当前访问会话。例如，我们输入用户名和密码成功登录某个网站后，服务器会用会话保存登录状态信息，后面我们每次刷新或请求该站点的其他页面时，会发现都是登录状态，这就是Cookies的功劳。Cookies里有信息标识了我们所对应的服务器的会话，每次浏览器在请求该站点的页面时，都会在请求头中加上Cookies并将其发送给服务器，服务器通过Cookies识别出是我们自己，并且查出当前状态是登录状态，所以返回结果就是登录之后才能看到的网页内容。  \nReferer：此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息并做相应的处理，如作来源统计、防盗链处理等。  \nUser-Agent：简称UA，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本、浏览器及版本等信息。在做爬虫时加上此信息，可以伪装为浏览器；如果不加，很可能会被识别出为爬虫  \nx-requested-with :XMLHttpRequest   # 代表ajax请求\nAccept-Language：指定客户端可接受的语言类型。\nAccept-Encoding：指定客户端可接受的内容编码\nContent-Type：也叫互联网媒体类型（Internet Media Type）或者MIME类型，在HTTP协议消息头中，它用来表示具体请求中的媒体类型信息。例如，text/html代表HTML格式，image/gif代表GIF图片，application/json代表JSON类型\n\n\n请求体：请求体一般承载的内容是POST请求中的表单数据，而对于GET请求，请求体则为空\n\n10.响应由服务端返回给客户端的, 可以分为三部分: \n\n响应状态码(response status code)\n响应头(response headers)\n响应体(response body)\n\n\n响应状态码:用于判断请求后的响应状态, 如200代表请求成功, 404代表页面页面找不到, 500代表服务器错误\n\n常见的状态码:200系列:12345678910111213141516171819200系列:\t200   成功         服务器已成功处理了请求  (**)300系列:    301    永久移动     请求的网页已永久移动到新位置，即永久重定向  (**)    302    临时移动     请求的网页暂时跳转到其他页面，即暂时重定向  (**)400系列:    400    错误请求     服务器无法解析该请求  (**)    401    未授权       请求没有进行身份验证或验证未通过    403    禁止访问     服务器拒绝此请求  (**)    404    未找到       服务器找不到请求的网页500系列    500    服务器内部错误   服务器遇到错误，无法完成请求  (**)    501    未实现       服务器不具备完成请求的功能    502    错误网关     服务器作为网关或代理，从上游服务器收到无效响应    504    网关超时     服务器作为网关或代理，但是没有及时从上游服务器收到请求    505    HTTP版本不支持   服务器不支持请求中所用的HTTP协议版本\n\n注意:状态码不能完全代表响应状态, 部分网站的状态码是自定义的, 一切以响应的数据为准\n\n响应头:响应头包含了服务器对请求的应答信息\n\nDate：标识响应产生的时间。\nContent-Encoding：指定响应内容的编码。\nServer：包含服务器的信息，比如名称、版本号等。\nContent-Type：文档类型，指定返回的数据类型是什么，如text/html代表返回HTML文档，application/x-javascript则代表返回JavaScript文件，image/jpeg则代表返回图片。\nSet-Cookie：设置Cookies。响应头中的Set-Cookie告诉浏览器需要将此内容放在Cookies中，下次请求携带Cookies请求。\nExpires：指定响应的过期时间，可以使代理服务器或浏览器将加载的内容更新到缓存中。如果再次访问时，就可以直接从缓存中加载，降低服务器负载，缩短加载时间。\n\n\n响应体:最重要的当属响应体的内容了。响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的HTML代码；请求一张图片时，它的响应体就是图片的二进制数据。我们做爬虫请求网页后，要解析的内容就是响应体.\n\n11.网页基础网页可以分为三部分, HTML, CSS, JavaScript\n\nHTML: 其全称叫作Hyper Text Markup Language，即超文本标记语言，定义了网页的骨架    \nCSS: 全称叫作Cascading Style Sheets，即层叠样式表，定义了网页的样式\nJavaScript: 简称JS，是一种脚本语言定义了网页与用户的交互行为, 如下载进度条, 提示框, 播图 \n\n\n","categories":["爬虫"],"tags":["python"]},{"title":"网络协议http与https,tcp和udp,tcp/ip","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/02/19/网络协议http与https,tcp和udp,tcp/ip/","content":"HTTP和HTTPS是什么？我们使用浏览器访问一个网站页面，在浏览器的地址栏中我们会看到一串URL，如图网站的URL会分为两部分：通信协议和域名地址。\n域名地址都很好理解，不同的域名地址表示网站中不同的页面，而通信协议，简单来说就是浏览器和服务器之间沟通的语言。网站中的通信协议一般就是HTTP协议和HTTPS协议。\nHTTP协议\nHTTP协议是一种使用明文数据传输的网络协议。一直以来HTTP协议都是最主流的网页协议，但是互联网发展到今天，HTTP协议的明文传输会让用户存在一个非常大的安全隐患。试想一下，假如你在一个HTTP协议的网站上面购物，你需要在页面上输入你的银行卡号和密码，然后你把数据提交到服务器实现购买。假如这个适合，你的传输数据被第三者给截获了，由于HTTP明文数据传输的原因，你的银行卡号和密码，将会被这个截获人所得到。现在你还敢在一个HTTP的网站上面购物吗？你还会在一个HTTP的网站上面留下你的个人信息吗？HTTPS协议\nHTTPS协议可以理解为HTTP协议的升级，就是在HTTP的基础上增加了数据加密。在数据进行传输之前，对数据进行加密，然后再发送到服务器。这样，就算数据被第三者所截获，但是由于数据是加密的，所以你的个人信息让然是安全的。这就是HTTP和HTTPS的最大区别。\n其实如果你足够细心，你会发现现在很多大型互联网网站，如百度、淘宝、腾讯很早就已经把HTTP换成HTTPS了。\nHTTP和HTTPS的其他不同\n数据加密传输，是HTTP和HTTPS之间的本质性区别，其实除了这个之外，HTTPS网站和HTTP网站还有其他地方不同。\n当你使用Chrome浏览器访问一个HTTP网站的时候，你会发现浏览器会对该HTTP网站显示“不安全”的安全警告，提示用户当前所访问的网站可能会存在风险。而假如你访问的是一个HTTPS网站时，情况却是完全不一样。你会发现浏览器的地址栏会变成绿色，企业名称会展示在地址栏中，地址栏上面还会出现一把“安全锁”的图标。这些都会给与用户很大的视觉上的安全体验。以下是EV证书在不同浏览器中的展现。\n除了浏览器视觉上不同以外，HTTPS网站和HTTP网站还有一个很重要的区别，就是对搜索排名的提升，这也是很多站长所关注的地方。\n百度和谷歌两大搜索引擎都已经明确表示，HTTPS网站将会作为搜索排名的一个重要权重指标。也就是说HTTPS网站比起HTTP网站在搜索排名中更有优势。\nHTTPS网站相比起HTTP网站拥有着多种的优势，HTTP明显已经不能适应当今这个互联网时代，可以预见到HTTP在不久的将来将会全面被HTTPS所取代。\nTCP和UDP的区别1234(1)同步：端口下如果同是来了两个客户端请求，第一个连接得到响应，与服务端建立通讯，而第二个请求就会被一直阻塞直到第一个请求完成操作，各请求之间排队，顺序执行。　(2)异步呢，就是同时来两个或者多个请求，服务端就同时响应多个客户端，同时给他们连接。各个客户端与服务器的通讯是并行的，一个客户端不必等另一个客户端完成操作。通常用这两个方法来接收一个客户端请求。　(3)阻塞 调用是指调用结果返回之前，当前线程会被挂起。比如：Console.ReadLine(),如果你不向控制台输入数据，这个方法就会一直等待，知道你输入数据后才会向下执行。这个时候当前线程被挂起来了，让CPU去做其他事情。　(4)非阻塞和阻塞的概念相对应，指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。\nsyn攻击123456在三次握手过程中，服务器发送SYN-ACK之后，收到客户端的ACK之前的TCP连接称为半连接(half-open connect).此时服务器处于Syn_RECV状态.当收到ACK后，服务器转入ESTABLISHED状态.Syn攻击就是 攻击客户端 在短时间内伪造大量不存在的IP地址，向服务器不断地发送syn包，服务器回复确认包，并等待客户的确认，由于源地址是不存在的，服务器需要不断的重发直 至超时，这些伪造的SYN包将长时间占用未连接队列，正常的SYN请求被丢弃，目标系统运行缓慢，严重者引起网络堵塞甚至系统瘫痪。Syn攻击是一个典型的DDOS攻击。检测SYN攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源IP地址是随机的，基本上可以断定这是一次SYN攻击.在Linux下可以如下命令检测是否被Syn攻击netstat -n -p TCP | grep SYN_RECV一般较新的TCP&#x2F;IP协议栈都对这一过程进行修正来防范Syn攻击，修改tcp协议实现。主要方法有SynAttackProtect保护机制、SYN cookies技术、增加最大半连接和缩短超时时间等.但是不能完全防范syn攻击。\nTCP和UDP是OSI模型中的运输层中的协议。TCP提供可靠的通信传输，而UDP则常被用于广播提供面向无连接的通信服务\n握手“我想给你发数据，可以吗？” （请提供序列号作为起始数据段）SYN：同步序列编号（Synchronize Sequence Numbers）“可以，你什么时候发？” （已提供序列号）SYN+ACK应答“我现在就发，你接着吧！”  ACK消息响应\n挥手 （A为主动关闭方，可以是SERVICE也可以是CLIENT）SERVER：传输好了，我要关了   1 ) 当主机A完成数据传输后,发送FIN,提出停止TCP连接的请求，进入FIN_WAIT1状态CLIENT：我看一下                       2 ) 主机B收到FIN后，将发送ACK，进入CLOSE_WAIT状态CLIENT：OK ，可以关                 3 ) 由B 端再提出反方向的关闭请求,将发送FIN，并进入LAST_ACK状态SERVER： 嗯，我关了                 4 ) 主机A对主机B的请求进行确认收到FIN,将发送ACK,双方向的关闭结束.进入TIME_WAIT状态，经过2MSL时间后关闭\nUDP（User Data Protocol，用户数据报协议）我们经常使用“ping”命令来测试两台主机之间TCP/IP通信是否正常，其实“ping”命令的原理就是向对方主机发送UDP数据包，然后对方主机确认收到数据包，如果数据包是否到达的消息及时反馈回来，那么网络就是通的。UDP应用场景： 1.面向数据报方式  2.网络数据大多为短消息   3.拥有大量Client  4.对数据安全性无特殊要求  5.网络负担非常重，但对响应速度要求高\n小结TCP与UDP的区别：    1.基于连接与无连接；    2.对系统资源的要求（TCP较多，UDP少）；    3.UDP程序结构较简单；    4.流模式与数据报模式 ；    5.TCP保证数据正确性，UDP可能丢包，TCP保证数据顺序，UDP不保证。\n123456789101112131415161718192021222324252627282930313233TCP: TCP编程的服务器端一般步骤是： TCP包头的最小长度，为20字节。　　1、创建一个socket，用函数socket()； 　　2、设置socket属性，用函数setsockopt(); * 可选 　　3、绑定IP地址、端口等信息到socket上，用函数bind(); 　　4、开启监听，用函数listen()； 　　5、接收客户端上来的连接，用函数accept()； 　　6、收发数据，用函数send()和recv()，或者read()和write(); 　　7、关闭网络连接； 　　8、关闭监听； TCP编程的客户端一般步骤是： 　　1、创建一个socket，用函数socket()； 　　2、设置socket属性，用函数setsockopt();* 可选 　　3、绑定IP地址、端口等信息到socket上，用函数bind();* 可选 　　4、设置要连接的对方的IP地址和端口等属性； 　　5、连接服务器，用函数connect()； 　　6、收发数据，用函数send()和recv()，或者read()和write(); 　　7、关闭网络连接；UDP:与之对应的UDP编程步骤要简单许多，分别如下： UDP编程的服务器端一般步骤是： 　　1、创建一个socket，用函数socket()； 　　2、设置socket属性，用函数setsockopt();* 可选 　　3、绑定IP地址、端口等信息到socket上，用函数bind(); 　　4、循环接收数据，用函数recvfrom(); 　　5、关闭网络连接； UDP编程的客户端一般步骤是： 　　1、创建一个socket，用函数socket()； 　　2、设置socket属性，用函数setsockopt();* 可选 　　3、绑定IP地址、端口等信息到socket上，用函数bind();* 可选 　　4、设置对方的IP地址和端口等属性; 　　5、发送数据，用函数sendto(); 　　6、关闭网络连接；\n\n为什么会有TCP/IP协议在世界上各地，各种各样的电脑运行着各自不同的操作系统为大家服务，这些电脑在表达同一种信息的时候所使用的方法是千差万别。就好像圣经中上帝打乱了各地人的口音，让他们无法合作一样。计算机使用者意识到，计算机只是单兵作战并不会发挥太大的作用。只有把它们联合起来，电脑才会发挥出它最大的潜力。于是人们就想方设法的用电线把电脑连接到了一起。\n但是简单的连到一起是远远不够的，就好像语言不同的两个人互相见了面，完全不能交流信息。因而他们需要定义一些共通的东西来进行交流，TCP/IP就是为此而生。TCP/IP不是一个协议，而是一个协议族的统称。里面包括了IP协议，IMCP协议，TCP协议，以及我们更加熟悉的http、ftp、pop3协议等等。电脑有了这些，就好像学会了外语一样，就可以和其他的计算机终端做自由的交流了。\nTCP/IP模型应用层:向用户提供一组常用的应用程序，比如电子邮件、文件传输访问、远程登录等。远程登录TELNET使用TELNET协议提供在网络其它主机上注册的接口。TELNET会话提供了基于字符的虚拟终端。文件传输访问FTP使用FTP协议来提供网络内机器间的文件拷贝功能。\n传输层:提供应用程序间的通信。其功能包括：一、格式化信息流；二、提供可靠传输。为实现后者，传输层协议规定接收端必须发回确认，并且假如分组丢失，必须重新发送。\n网络层 ：负责相邻计算机之间的通信。其功能包括三方面。一、处理来自传输层的分组发送请求，收到请求后，将分组装入IP数据报，填充报头，选择去往信宿机的路径，然后将数据报发往适当的网络接口。\n二、处理输入数据报：首先检查其合法性，然后进行寻径–假如该数据报已到达信宿机，则去掉报头，将剩下部分交给适当的传输协议；假如该数据报尚未到达信宿，则转发该数据报。\n三、处理路径、流控、拥塞等问题。\n网络接口层：这是TCP/IP软件的最低层，负责接收IP数据报并通过网络发送之，或者从网络上接收物理帧，抽出IP数据报，交给IP层。\nIPIP 用于计算机之间的通信。\nIP 是无连接的通信协议。它不会占用两个正在通信的计算机之间的通信线路。这样，IP 就降低了对网络线路的需求。每条线可以同时满足许多不同的计算机之间的通信需要。\n通过 IP，消息（或者其他数据）被分割为小的独立的包，并通过因特网在计算机之间传送。\nIP 负责将每个包路由至它的目的地。\nIP地址每个计算机必须有一个 IP 地址才能够连入因特网。\n每个 IP 包必须有一个地址才能够发送到另一台计算机。\n网络上每一个节点都必须有一个独立的Internet地址（也叫做IP地址）。现在，通常使用的IP地址是一个32bit的数字，也就是我们常说的IPv4标准，这32bit的数字分成四组，也就是常见的255.255.255.255的样式。IPv4标准上，地址被分为五类，我们常用的是B类地址。具体的分类请参考其他文档。需要注意的是IP地址是网络号+主机号的组合，这非常重要。\nCP/IP 使用 32 个比特来编址。一个计算机字节是 8 比特。所以 TCP/IP 使用了 4 个字节。一个计算机字节可以包含 256 个不同的值：00000000、00000001、00000010、00000011、00000100、00000101、00000110、00000111、00001000 … 直到 11111111。现在，你知道了为什么 TCP/IP 地址是介于 0 到 255 之间的 4 个数字。\nTCP 使用固定的连接TCP 用于应用程序之间的通信。\n当应用程序希望通过 TCP 与另一个应用程序通信时，它会发送一个通信请求。这个请求必须被送到一个确切的地址。在双方“握手”之后，TCP 将在两个应用程序之间建立一个全双工 (full-duplex) 的通信。\n这个全双工的通信将占用两个计算机之间的通信线路，直到它被一方或双方关闭为止。\nUDP 和 TCP 很相似，但是更简单，同时可靠性低于 TCP。\nIP 路由器当一个 IP 包从一台计算机被发送，它会到达一个 IP 路由器。\nIP 路由器负责将这个包路由至它的目的地，直接地或者通过其他的路由器。\n在一个相同的通信中，一个包所经由的路径可能会和其他的包不同。而路由器负责根据通信量、网络中的错误或者其他参数来进行正确地寻址。\n域名12 个阿拉伯数字很难记忆。使用一个名称更容易。\n用于 TCP/IP 地址的名字被称为域名。www.baidu.com就是一个域名。\n当你键入一个像https://www.baidu.com/这样的域名，域名会被一种 DNS 程序翻译为数字。\n在全世界，数量庞大的 DNS 服务器被连入因特网。DNS 服务器负责将域名翻译为 TCP/IP 地址，同时负责使用新的域名信息更新彼此的系统。\n当一个新的域名连同其 TCP/IP 地址一同注册后，全世界的 DNS 服务器都会对此信息进行更新。\nTCP/IPTCP/IP 意味着 TCP 和 IP 在一起协同工作。\nTCP 负责应用软件（比如你的浏览器）和网络软件之间的通信。\nIP 负责计算机之间的通信。\nTCP 负责将数据分割并装入 IP 包，然后在它们到达的时候重新组合它们。\nIP 负责将包发送至接受者。\n","categories":[],"tags":["python"]},{"title":"python常见算法面试题","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/02/16/python常见算法面试题/","content":"为什么输出6，6，6，6123def num():    return [lambda x: i*x for i in range(4)]   # 这里使用的是lambda函数 print([m(2) for m in num()])     # 输出: [6, 6, 6, 6]\n思路这题涉及到了闭包延时绑定，当循环执行完了之后才会执行传参，循环四次，每一次循环完 i=3 然后再和x相乘 所以结果是6，6，6，6。 如果把 [ lambda x: ix for i in range(4) ] 改成 （ lambda x: ix for i in range(4) ）这样就变成了一个生成器 自动实现迭代器协议，一边循环一边计算的机制， 这样结果就是 0，2，4，6.\n两数之和给定 nums = [2, 7, 11, 15], target = 9因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1]\n思路：这里可以使用字典来解题，通过enumerate方法遍历获取数据的下标包括对应值，然后以key，value形式把该数据的下标和对应值存入字典，然后再出通过enumerate方法遍历数据，每一次获取数据就从字典拿出一个值，用目标值减去从字典拿出的这个值得到一个结果值，如果结果值存在字典当中，那么返回两个数的下标，如果为None，说明字典中没有这个值。\n123456789101112131415161718192021222324def get_index_list(nums, target):    # Todo 作为一个方法来实现，批量解决这个获取索引的问题    &quot;&quot;&quot;    :params nums：传的参数的列表    :params target: 目标值    :return: 返回索引    &quot;&quot;&quot;    dic = &#123;&#125;    for a, b in enumerate(nums):  # a是下标 b是对应值        dic[b] = a  # 对应值存入字典    for i, b in enumerate(nums):        j = dic.get(target-b)  # 从字典中拿出对应值 用目标值减去对应值        if j is not None:  # 判断如果减去的对应值不为空，则返回下标            return [i, j]if __name__ == &quot;__main__&quot;:    print(get_index_list([2, 7, 11, 15],9))\n数组中重复的数字示例：\n123输入:[2,3,1,0,2,5,3]输出: 2 或 3\n\n思路：这道题想到的是，使用列表中的count方法，定义一个空列表，遍历数据然后进行判断，如果数据值出现个数大于或等于2，说明该数据是重复的，然后把重复的筛取出来之后存入空列表，再进行返回输出。\n实现代码：1234567891011121314151617def get_number(nums):    &quot;&quot;&quot;    :params nums：传的参数的数组    :return: 返回重复数字    &quot;&quot;&quot;    nub = []    for i in nums:        if nums.count(i) &gt;= 2:            if str(i) not in nub:                nub.append(str(i))    print(&#x27;或&#x27;.join(nub))if __name__ == &quot;__main__&quot;:    get_number([2, 3, 1, 0, 2, 5, 3])\n\n队列实现一个栈思路：使用一个队列，实现栈的一些基本操作，栈（后进先出）的特性。\n实现代码：12345678910111213141516171819202122232425262728293031323334353637383940# 队列实现一个栈 （栈：后进先出）class Stack(object):    def __init__(self):        # 定义一个队列        self.lst = []    def is_None(self):        # 判断栈是否为空 返回 ture false        return self.lst == []    def push(self, i):        # 加入元素        self.lst.append(i)    def pop(self):        # 出栈        return self.lst.pop()    def stack_top(self):        # 返回栈顶元素        return self.lst[-1]    def size(self):        # 栈的大小        return len(self.lst)if __name__ == &quot;__main__&quot;:    stack = Stack()    print(stack.is_None())    stack.push(1)    stack.push(2)    stack.push(3)    print(stack.lst)    print(stack.pop())    print(stack.stack_top())    print(stack.size())\n\n\n回文数判断一个整数是否是回文数。回文数是指正序（从左向右）和倒序（从右向左）读都是一样的整数。\n示例1：\n12输入: 121输出: true\n\n示例2：\n123输入: -121输出: false解释: 从左向右，为 -121 。 从右向左读，为121- 。 因此它不是一个回文数\n\n示例3：\n123输入: 10输出: false解释: 从右向左读，为 01 。 因此它不是一个回文数\n\n思路：这题可以使用字符串 双指针的方法 将数据转化为字符串 首先定义好第一个元素下标和最后一个元素下标，while循环 只要条件不满足 一直循环 循环判断第一个和最后一个元素是否相等 不相等返回false 相等继续循环，如果循环条件满足之后都相等，返回 false\n实现代码：1234567891011121314151617181920212223def palindromic_number(x):    &quot;&quot;&quot;    :params x：传的参数的列表    :return: 返回Ture False    &quot;&quot;&quot;    lst = list(str(x))    print(lst)    L, R = 0, len(lst)-1    while L &lt;= R:        print(L, R)        if lst[L] != lst[R]:            return False        L += 1        R -= 1    return Trueif __name__ == &quot;__main__&quot;:    print(palindromic_number(1231))\n\n\n分别用生成器和迭代器生成斐波那契数列示例：\n1输出: 1 1 2 3 5 8 13\n\n1234567891011121314151617181920212223# 使用迭代器生成斐波那契数列class Fibonacii(object):    def __init__(self,all_num):        self.all_num = all_num        self.cur_idx = 0        self.a = 0          self.b = 1        def __iter__(self):        return self        def __next__(self):        if self.cur_idx &gt;= self.all_num:            raise StopIteration        ret = self.a                self.a, self.b = self.b, self.a + self.b        self.cur_idx += 1        return ret fibo = Fibonacii(10)for i in fibo:    print(i)\n\n12345678910111213# 使用生成器生成斐波那契数列def fibnacii(count):    num = 0    a, b = 0, 1    while num &lt; count:        yield a        a, b = b, a+b        num += 1fi = fibnacii(10)for i in fi:    print(i)\n\n反转字符数组思路：直接使用反转\n实现代码：123456789# 反转字符串def reverseString(s):    s[0::] = s[::-1]    print(s)if __name__ == &quot;__main__&quot;:    reverseString([&#x27;b&#x27;, &#x27;&#x27;, &#x27;a&#x27;, &#x27;r&#x27;])\n\n字符串反转12345a=&quot;Let&#x27;s take LeetCode contest&quot;def b():    return &#x27; &#x27;.join(a.split(&#x27; &#x27;)[::-1])[::-1]if __name__ == &#x27;__main__&#x27;:    print(b())\n\n球12345678910111213def a():    slit=[]    s=100    for c in range(0,10):        if c==0:            slit.append(s)        else:            slit.append(s*2)        s=s/2    print(sum(slit))    print(s)if __name__ == &#x27;__main__&#x27;:    a()\n冒泡排序12345678910#定义一个函数传入个变量def bubble_sort(li):    for i in range(len(li)-1):        for j in range(len(li)-i-1):            if li[j] &gt; li[j+1]:                li[j],li[j+1]=li[j+1],li[j]li = [1,5,2,6,3,7,4,8,9,0]bubble_sort(li)print(li)\n","categories":[],"tags":["python"]},{"title":"完整的HTTP请求全过程","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/02/08/完整的HTTP请求全过程/","content":"一、 HTTP请求和响应步骤以上完整表示了HTTP请求和响应的7个步骤，下面从TCP/IP协议模型的角度来理解HTTP请求和响应如何传递的。\n二、TCP/IP协议TCP/IP协议模型（Transmission Control Protocol/Internet Protocol），包含了一系列构成互联网基础的网络协议，是Internet的核心协议，通过20多年的发展已日渐成熟，并被广泛应用于局域网和广域网中，目前已成为事实上的国际标准。TCP/IP协议簇是一组不同层次上的多个协议的组合，通常被认为是一个四层协议系统，与OSI的七层模型相对应。\nHTTP协议就是基于TCP/IP协议模型来传输信息的。(1). 链路层\n也称作数据链路层或网络接口层（在第一个图中为网络接口层和硬件层），通常包括操作系统中的设备驱动程序和计算机中对应的网络接口卡。它们一起处理与电缆（或其他任何传输媒介）的物理接口细节。ARP（地址解析协议）和RARP（逆地址解析协议）是某些网络接口（如以太网和令牌环网）使用的特殊协议，用来转换IP层和网络接口层使用的地址。\n(2). 网络层\n也称作互联网层（在第一个图中为网际层），处理分组在网络中的活动，例如分组的选路。在TCP/IP协议族中，网络层协议包括IP协议（网际协议），ICMP协议（Internet互联网控制报文协议），以及IGMP协议（Internet组管理协议）。\nIP是一种网络层协议，提供的是一种不可靠的服务，它只是尽可能快地把分组从源结点送到目的结点，但是并不提供任何可靠性保证。同时被TCP和UDP使用。TCP和UDP的每组数据都通过端系统和每个中间路由器中的IP层在互联网中进行传输。\nICMP是IP协议的附属协议。IP层用它来与其他主机或路由器交换错误报文和其他重要信息。\nIGMP是Internet组管理协议。它用来把一个UDP数据报多播到多个主机。\n(3). 传输层\n主要为两台主机上的应用程序提供端到端的通信。在TCP/IP协议族中，有两个互不相同的传输协议：TCP（传输控制协议）和UDP（用户数据报协议）。\nTCP为两台主机提供高可靠性的数据通信。它所做的工作包括把应用程序交给它的数据分成合适的小块交给下面的网络层，确认接收到的分组，设置发送最后确认分组的超时时钟等。由于运输层提供了高可靠性的端到端的通信，因此应用层可以忽略所有这些细节。为了提供可靠的服务，TCP采用了超时重传、发送和接收端到端的确认分组等机制。\nUDP则为应用层提供一种非常简单的服务。它只是把称作数据报的分组从一台主机发送到另一台主机，但并不保证该数据报能到达另一端。一个数据报是指从发送方传输到接收方的一个信息单元（例如，发送方指定的一定字节数的信息）。UDP协议任何必需的可靠性必须由应用层来提供。(4). 应用层\n应用层决定了向用户提供应用服务时通信的活动。TCP/IP 协议族内预存了各类通用的应用服务。包括 HTTP，FTP（File Transfer Protocol，文件传输协议），DNS（Domain Name System，域名系统）服务。当应用程序用TCP传送数据时，数据被送入协议栈中，然后逐个通过每一层直到被当作一串比特流送入网络。其中每一层对收到的数据都要增加一些首部信息（有时还要增加尾部信息），该过程如图所示。当目的主机收到一个以太网数据帧时，数据就开始从协议栈中由底向上升，同时去掉各层协议加上的报文首部。每层协议盒都要去检查报文首部中的协议标识，以确定接收数据的上层协议。这个过程称作分用（Demultiplexing）。协议是通过目的端口号、源I P地址和源端口号进行解包的。\n通过以上步骤我们从TCP/IP模型的角度来理解了一次HTTP请求与响应的过程。\n下面这张图更清楚明白：下面具体来看如何进行一步步操作的。\n三、TCP三次握手TCP是面向连接的，无论哪一方向另一方发送数据之前，都必须先在双方之间建立一条连接。在TCP/IP协议中，TCP协议提供可靠的连接服务，连接是通过三次握手进行初始化的。三次握手的目的是同步连接双方的序列号和确认号并交换 TCP窗口大小信息。 第一次握手：建立连接。客户端发送连接请求报文段，将SYN位置为1，Sequence Number为x；然后，客户端进入SYN_SEND状态，等待服务器的确认；\n第二次握手：服务器收到SYN报文段。服务器收到客户端的SYN报文段，需要对这个SYN报文段进行确认，设置Acknowledgment Number为x+1(Sequence Number+1)；同时，自己自己还要发送SYN请求信息，将SYN位置为1，Sequence Number为y；服务器端将上述所有信息放到一个报文段（即SYN+ACK报文段）中，一并发送给客户端，此时服务器进入SYN_RECV状态；\n第三次握手：客户端收到服务器的SYN+ACK报文段。然后将Acknowledgment Number设置为y+1，向服务器发送ACK报文段，这个报文段发送完毕以后，客户端和服务器端都进入ESTABLISHED状态，完成TCP三次握手。\n为什么要三次握手\n为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误。\n具体例子：“已失效的连接请求报文段”的产生在这样一种情况下：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。”\n四、HTTP协议Http是什么？\n通俗来讲，他就是计算机通过网络进行通信的规则，是一个基于请求与响应，无状态的，应用层的协议，常基于TCP/IP协议传输数据。目前任何终端（手机，笔记本电脑。。）之间进行任何一种通信都必须按照Http协议进行，否则无法连接。\n四个基于：\n请求与响应：客户端发送请求，服务器端响应数据\n无状态的：协议对于事务处理没有记忆能力，客户端第一次与服务器建立连接发送请求时需要进行一系列的安全认证匹配等，因此增加页面等待时间，当客户端向服务器端发送请求，服务器端响应完毕后，两者断开连接，也不保存连接状态，一刀两断！恩断义绝！从此路人！下一次客户端向同样的服务器发送请求时，由于他们之前已经遗忘了彼此，所以需要重新建立连接。\n应用层：Http是属于应用层的协议，配合TCP/IP使用。\nTCP/IP：Http使用TCP作为它的支撑运输协议。HTTP客户机发起一个与服务器的TCP连接，一旦连接建立，浏览器（客户机）和服务器进程就可以通过套接字接口访问TCP。\n针对无状态的一些解决策略：\n有时需要对用户之前的HTTP通信状态进行保存，比如执行一次登陆操作，在30分钟内所有的请求都不需要再次登陆。于是引入了Cookie技术。\nHTTP/1.1想出了持久连接（HTTP keep-alive）方法。其特点是，只要任意一端没有明确提出断开连接，则保持TCP连接状态，在请求首部字段中的Connection: keep-alive即为表明使用了持久连接。等等还有很多。。。。。。\n下面开始讲解重头戏：HTTP请求报文，响应报文，对应于上述步骤的2，3，4，5，6。\nHTTP报文是面向文本的，报文中的每一个字段都是一些ASCII码串，各个字段的长度是不确定的。HTTP有两类报文：请求报文和响应报文。\n五、HTTP请求报文一个HTTP请求报文由请求行（request line）、请求头部（header）、空行和请求数据4个部分组成，下图给出了请求报文的一般格式。1.请求行\n请求行分为三个部分：请求方法、请求地址和协议版本\n请求方法\nHTTP/1.1 定义的请求方法有8种：GET、POST、PUT、DELETE、PATCH、HEAD、OPTIONS、TRACE。\n最常的两种GET和POST，如果是RESTful接口的话一般会用到GET、POST、DELETE、PUT。\n请求地址\nURL:统一资源定位符，是一种自愿位置的抽象唯一识别方法。\n组成：&lt;协议&gt;：//&lt;主机&gt;：&lt;端口&gt;/&lt;路径&gt;\n端口和路径有时可以省略（HTTP默认端口号是80）\n如下例：有时会带参数，GET请求\n协议版本\n协议版本的格式为：HTTP/主版本号.次版本号，常用的有HTTP/1.0和HTTP/1.1\n2.请求头部\n请求头部为请求报文添加了一些附加信息，由“名/值”对组成，每行一对，名和值之间使用冒号分隔。\n常见请求头如下：请求头部的最后会有一个空行，表示请求头部结束，接下来为请求数据，这一行非常重要，必不可少。\n3.请求数据\n可选部分，比如GET请求就没有请求数据。\n下面是一个POST方法的请求报文：\n123456789101112POST 　/index.php　HTTP/1.1 　　 请求行Host: localhostUser-Agent: Mozilla/5.0 (Windows NT 5.1; rv:10.0.2) Gecko/20100101 Firefox/10.0.2　　请求头Accept: text/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8Accept-Language: zh-cn,zh;q=0.5Accept-Encoding: gzip, deflateConnection: keep-aliveReferer: http://localhost/Content-Length：25Content-Type：application/x-www-form-urlencoded　　空行username=aa&amp;password=1234　　请求数据\n\n六、HTTP响应报文HTTP响应报文主要由状态行、响应头部、空行以及响应数据组成。\n1.状态行\n由3部分组成，分别为：协议版本，状态码，状态码描述。\n其中协议版本与请求报文一致，状态码描述是对状态码的简单描述，所以这里就只介绍状态码。\n状态码\n状态代码为3位数字。1xx：指示信息–表示请求已接收，继续处理。2xx：成功–表示请求已被成功接收、理解、接受。3xx：重定向–要完成请求必须进行更进一步的操作。4xx：客户端错误–请求有语法错误或请求无法实现。5xx：服务器端错误–服务器未能实现合法的请求。\n下面列举几个常见的：\n\n2.响应头部\n与请求头部类似，为响应报文添加了一些附加信息\n常见响应头部如下： 3.响应数据\n用于存放需要返回给客户端的数据信息。\n下面是一个响应报文的实例：\n12345678910111213141516171819202122HTTP/1.1 200 OK　　状态行Date: Sun, 17 Mar 2013 08:12:54 GMT　　响应头部Server: Apache/2.2.8 (Win32) PHP/5.2.5X-Powered-By: PHP/5.2.5Set-Cookie: PHPSESSID=c0huq7pdkmm5gg6osoe3mgjmm3; path=/Expires: Thu, 19 Nov 1981 08:52:00 GMTCache-Control: no-store, no-cache, must-revalidate, post-check=0, pre-check=0Pragma: no-cacheContent-Length: 4393Keep-Alive: timeout=5, max=100Connection: Keep-AliveContent-Type: text/html; charset=utf-8　　空行&lt;html&gt;　　响应数据&lt;head&gt;&lt;title&gt;HTTP响应示例&lt;title&gt;&lt;/head&gt;&lt;body&gt;Hello HTTP!&lt;/body&gt;&lt;/html&gt;\n关于请求头部和响应头部的知识点很多，这里只是简单介绍。\n通过以上步骤，数据已经传递完毕，HTTP/1.1会维持持久连接，但持续一段时间总会有关闭连接的时候，这时候据需要断开TCP连接。\n七、TCP四次挥手当客户端和服务器通过三次握手建立了TCP连接以后，当数据传送完毕，肯定是要断开TCP连接的啊。那对于TCP的断开连接，这里就有了神秘的“四次分手”。第一次分手：主机1（可以使客户端，也可以是服务器端），设置Sequence Number，向主机2发送一个FIN报文段；此时，主机1进入FIN_WAIT_1状态；这表示主机1没有数据要发送给主机2了；\n第二次分手：主机2收到了主机1发送的FIN报文段，向主机1回一个ACK报文段，Acknowledgment Number为Sequence Number加1；主机1进入FIN_WAIT_2状态；主机2告诉主机1，我“同意”你的关闭请求；\n第三次分手：主机2向主机1发送FIN报文段，请求关闭连接，同时主机2进入LAST_ACK状态；\n第四次分手：主机1收到主机2发送的FIN报文段，向主机2发送ACK报文段，然后主机1进入TIME_WAIT状态；主机2收到主机1的ACK报文段以后，就关闭连接；此时，主机1等待2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，主机1也可以关闭连接了。\n为什么要四次分手\nTCP协议是一种面向连接的、可靠的、基于字节流的运输层通信协议。TCP是全双工模式，这就意味着，当主机1发出FIN报文段时，只是表示主机1已经没有数据要发送了，主机1告诉主机2，它的数据已经全部发送完毕了；但是，这个时候主机1还是可以接受来自主机2的数据；当主机2返回ACK报文段时，表示它已经知道主机1没有数据发送了，但是主机2还是可以发送数据到主机1的；当主机2也发送了FIN报文段时，这个时候就表示主机2也没有数据要发送了，就会告诉主机1，我也没有数据要发送了，之后彼此就会愉快的中断这次TCP连接。\n通过以上步骤便完成了HTTP的请求和响应，进行了数据传递，这其中涉及到需要知识点，都进行了逐一了解。\n","categories":[],"tags":["python"]},{"title":"UWSGI和WSGI和nginx","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/01/29/UWSGI和WSGI和nginx/","content":"什么是UWSGI和WSGIuwsgi协议uwsgi（小写！）协议是uWSGI服务器使用的本机协议。\n它是一个二进制协议，可以携带任何类型的数据。uwsgi数据包的前4个字节描述了该数据包包含的数据的类型。\n每个uwsgi请求都会以uwsgi格式生成响应。\n甚至Web服务器处理程序也遵守此规则，因为HTTP响应是有效的uwsgi数据包（请参阅modifier1= 72）。\n该协议主要通过TCP起作用，但是主进程可以绑定到UDP单播/多播，用于嵌入式SNMP服务器或群集管理/消息请求。\nSCTP支持正在开发中。\n什么是 WSGI先说一下CGI，（通用网关接口， Common Gateway Interface/CGI），定义客户端与Web服务器的交流方式的一个程序。例如正常情况下客户端发来一个请求，根据HTTP协议Web服务器将请求内容解析出来，进过计算后，再将加us安出来的内容封装好，例如服务器返回一个HTML页面，并且根据HTTP协议构建返回内容的响应格式。涉及到TCP连接、HTTP原始请求和相应格式的这些，都由一个软件来完成，这时，以上的工作需要一个程序来完成，而这个程序便是CGI。\n那什么是WSGI呢？维基上的解释为，Web服务器网关接口(Python Web Server Gateway Interface，WSGI)**，是为Python语言定义的Web服务器和Web应用程序或框架之间的一种简单而通用的接口。从语义上理解，貌似WSGI就是Python为了解决Web服务器端与客户端**之间的通信问题而产生的，并且WSGI是基于现存的CGI标准而设计的，同样是一种程序（或者Web组件的接口规范？）。\nWSGI区分为两部分：一种为“服务器”或“网关”，另一种为“应用程序”或“应用框架”。 所谓的WSGI中间件同时实现了API的两方，即在WSGI服务器和WSGI应用之间起调解作用：从WSGI服务器的角度来说，中间件扮演应用程序，而从应用程序的角度来说，中间件扮演服务器。中间件具有的功能：\n\n重写环境变量后，根据目标URL，将请求消息路由到不同的应用对象。\n允许在一个进程中同时运行多个应用程序或应用框架\n负载均衡和远程处理，通过在网络上转发请求和相应消息。\n进行内容后处理，例如应用XSLT样式表。（以上 from 维基）\n\n看了这么多，总结一下，其实可以说WSGI就是基于Python的以CGI为标准做一些扩展。\n什么是ASGI异步网关协议接口，一个介于网络协议服务和Python应用之间的标准接口，能够处理多种通用的协议类型，包括HTTP，HTTP2和WebSocket。 然而目前的常用的WSGI主要是针对HTTP风格的请求响应模型做的设计，并且越来越多的不遵循这种模式的协议逐渐成为Web变成的标准之一，例如WebSocket。 ASGI尝试保持在一个简单的应用接口的前提下，提供允许数据能够在任意的时候、被任意应用进程发送和接受的抽象。并且同样描述了一个新的，兼容HTTP请求响应以及WebSocket数据帧的序列格式。允许这些协议能通过网络或本地socket进行传输，以及让不同的协议被分配到不同的进程中。\nWSGI和ASGI的区别在哪以上，WSGI是基于HTTP协议模式的，不支持WebSocket，而ASGI的诞生则是为了解决Python常用的WSGI不支持当前Web开发中的一些新的协议标准。同时，ASGI对于WSGI原有的模式的支持和WebSocket的扩展，即ASGI是WSGI的扩展。\nnginx的正向代理和反向代理nginx概述nginx是一款自由的、开源的、高性能的HTTP服务器和反向代理服务器；同时也是一个IMAP、POP3、SMTP代理服务器；nginx可以作为一个HTTP服务器进行网站的发布处理，另外nginx可以作为反向代理进行负载均衡的实现。\n正向代理在如今的网络环境下，我们如果由于技术需要去访问国外的某些网站，此时你会发现位于国外的某些网站我们通过浏览器是没有办法访问的，此时大家都可能会用一个操作FQ进行访问，FQ的方式主要是找到一个可以访问国外网站的代理服务器，我们将请求发送给代理服务器，代理服务器去访问国外的网站，然后将访问到的数据传递给我们。这就是正向代理。正向代理最大的特点是客户端非常明确要访问的服务器地址；服务器只清楚请求来自哪个代理服务器，而不清楚来自哪个具体的客户端；正向代理模式屏蔽或者隐藏了真实客户端信息。\n反向代理比如某宝网站，每天同时连接到网站的访问人数已经爆表，单个服务器远远不能满足人民日益增长的购买欲望了，此时就出现了一个大家耳熟能详的名词：分布式部署；也就是通过部署多台服务器来解决访问人数限制的问题；某宝网站中大部分功能也是直接使用nginx进行反向代理实现的，并且通过封装nginx和其他的组件之后起了一个高大上的名字：Tengine\n","categories":[],"tags":["python"]},{"title":"flask中使用原生sql语句","url":"https://github.com/xuMr6/xumr6.github.io.git/2018/01/16/flask中使用原生sql语句/","content":"话不多说直接上代码\n123456789101112131415161718192021config配置SQLALCHEMY_DATABASE_URI = &#x27;mysql+pymysql://root:6666@127.0.0.1:3306/xu_falask&#x27;SQLALCHEMY_TRACK_MODIFICATIONS = False导包from flask import Flaskapp = Flask(__name__)db = SQLAlchemy(app)# 原生sql插入db.session.execute(        &#x27;insert into goods(name,price,user_name,stact,num,creact_time)  value(&quot;%s&quot;,%d,&quot;%s&quot;,%d,%d,&quot;%s&quot;)&#x27; % (            name, int(price), user_name, 1, int(num), now_time.strftime(&quot;%Y-%m-%d %X&quot;)))    db.session.commit()# 原生sql语句查询goods = db.session.execute(&#x27;select * from goods where stact=1 and user_name=&quot;%s&quot;&#x27; % name).fetchall()# 原生sql语句修改db.session.execute(&#x27;update goods set stact=0 where name=&quot;%s&quot;&#x27; % name)    db.session.commit()","categories":[],"tags":["python"]},{"title":"redis最完整的解释","url":"https://github.com/xuMr6/xumr6.github.io.git/2017/11/19/redis最完整的解释/","content":"简介\nRedis （全称：Remote Dictionary Server 远程字典服务）是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。它是一个运行在内存中的数据结构存储系统，它可以用作数据库、缓存和消息中间件。\n我们平时在项目中设计数据访问的时候往往都是采用直接访问数据库，采用数据库连接池来实现，但是如果我们的项目访问量过大或者访问过于频繁，将会对我们的数据库带来很大的压力。为了解决这个问题从而redis数据库脱颖而出，redis数据库出现时是以非关系数据库的光环展示在广大程序猿的面前的，后来redis的迭代版本支持了缓存数据、登录session状态（分布式session共享）等。所以又被作为内存缓存的形式应用到大型企业级项目中。\nRedis是什么？主流的理解有以下三种\n1.key value store.是一个以key-value形式存储的数据库，定位直指MySQL，用来作为唯一的存储系统。\n2.memory cache.是一个把数据存储在内存中的高速缓存，用来在应用和数据库间提供缓冲，替代memcachd。\n3.data structrue server.把它支持对复杂数据结构的高速操作作为卖点，提供某些特殊业务场景的计算和展现需求。比如排行榜应用，Top 10之类的。\nRedis特点Redis 是完全开源免费的，遵守BSD协议，是一个高性能的key-value数据库。\nRedis 与其他 key - value 缓存产品有以下三个特点：\nRedis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。\nRedis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。\nRedis支持数据的备份，即master-slave模式的数据备份。\nRedis 优势性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s 。这主要归功于这些数据都存在于内存中，并且是单线程。\n丰富的数据类型 – Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。\n原子性 – Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行。\n丰富的特性 – Redis还支持 publish/subscribe（发布/订阅）、通知,、key 过期、Lua 脚本、事务、Pipeline（管道，即当指令到达一定数量后，客户端才会执行）等等特性。\n兼容性强 – 支持多种编程语言。支持Java、PHP、Golang、Python、Ruby、Lua、Nodejs等。\n高可用和分布式 – Redis-Sentinel（v2.8）支持高可用，Redis-Cluster（v3.0）支持分布式\nRedis使用场景1、缓存——热数据\n这是 Redis 使用最多的场景。Redis 能够替代 Memcached，让你的缓存从只能存储数据变得能够更新数据，因此你不再需要每次都重新生成数据。\n2、计数器\n诸如统计点击数、转发数、评论数等应用。由于单线程，可以避免并发问题，保证不会出错，而且100%毫秒级性能！爽。 命令：INCRBY 当然爽完了，别忘记持久化，毕竟是redis只是存了内存！\n3、消息队列\n相当于消息系统，运行稳定并且快速，支持模式匹配，能够实时订阅与取消频道，和ActiveMQ，RocketMQ等工具类似，但是个人觉得简单用一下还行，如果对于数据一致性要求高的话还是用RocketMQ等专业系统。 由于redis把数据添加到队列是返回添加元素在队列的第几位，所以可以做判断用户是第几个访问这种业务 队列不仅可以把并发请求变成串行，并且还可以做队列或者栈使用\n4、位操作（大数据处理）\n用于数据量上亿的场景下，例如几亿用户系统的签到，去重登录次数统计，某用户是否在线状态等等。 腾讯10亿用户，要几毫秒内查询到某个用户是否在线，你能怎么做？千万别说给每个用户建立一个key，然后挨个记（你可以算一下需要的内存会很恐怖，而且这种类似的需求很多，腾讯光这个得多花多少钱。。）这里要用到位操作——使用setbit、getbit、bitcount命令。\n原理是： redis内构建一个足够长的数组，每个数组元素只能是0和1两个值，然后这个数组的下标index用来表示我们上面例子里面的用户id（必须是数字哈），那么很显然，这个几亿长的大数组就能通过下标和元素值（0和1）来构建一个记忆系统，上面我说的几个场景也就能够实现。用到的命令是：setbit、getbit、bitcount。\n5、分布式锁与单线程机制\n验证前端的重复请求（可以自由扩展类似情况），可以通过redis进行过滤：每次请求将request Ip、参数、接口等hash作为key存储redis（幂等性请求），设置多长时间有效期，然后下次请求过来的时候先在redis中检索有没有这个key，进而验证是不是一定时间内过来的重复提交 秒杀系统，基于redis是单线程特征，防止出现数据库“爆破” 全局增量ID生成，类似“秒杀”\n6、最新列表\n例如新闻列表页面最新的新闻列表，如果总数量很大的情况下，尽量不要使用select a from A limit 10这种low货，尝试redis的 LPUSH命令构建List，一个个顺序都塞进去就可以啦。不过万一内存清掉了咋办？也简单，查询不到存储key的话，用mysql查询并且初始化一个List到redis中就好了。\n7、排行榜\n谁得分高谁排名往上。命令：ZADD（有续集，sorted set）\n（附）Redis命令目录Key（键）DELDUMPEXISTSEXPIREEXPIREATKEYSMIGRATEMOVEOBJECTPERSISTPEXPIREPEXPIREATPTTLRANDOMKEYRENAMERENAMENXRESTORESORTTTLTYPESCAN\nString（字符串）APPENDBITCOUNTBITOPBITFIELDDECRDECRBYGETGETBITGETRANGEGETSETINCRINCRBYINCRBYFLOATMGETMSETMSETNXPSETEXSETSETBITSETEXSETNXSETRANGESTRLEN\nHash（哈希表）HDELHEXISTSHGETHGETALLHINCRBYHINCRBYFLOATHKEYSHLENHMGETHMSETHSETHSETNXHVALSHSCANHSTRLEN\nList（列表）BLPOPBRPOPBRPOPLPUSHLINDEXLINSERTLLENLPOPLPUSHLPUSHXLRANGELREMLSETLTRIMRPOPRPOPLPUSHRPUSHRPUSHX\nSet（集合）SADDSCARDSDIFFSDIFFSTORESINTERSINTERSTORESISMEMBERSMEMBERSSMOVESPOPSRANDMEMBERSREMSUNIONSUNIONSTORESSCAN\nSortedSet（有序集合）ZADDZCARDZCOUNTZINCRBYZRANGEZRANGEBYSCOREZRANKZREMZREMRANGEBYRANKZREMRANGEBYSCOREZREVRANGEZREVRANGEBYSCOREZREVRANKZSCOREZUNIONSTOREZINTERSTOREZSCANZRANGEBYLEXZLEXCOUNTZREMRANGEBYLEX\nHyperLogLogPFADDPFCOUNTPFMERGE\nGEO（地理位置）GEOADDGEOPOSGEODISTGEORADIUSGEORADIUSBYMEMBERGEOHASH\nPub/Sub（发布/订阅）PSUBSCRIBEPUBLISHPUBSUBPUNSUBSCRIBESUBSCRIBEUNSUBSCRIBE\nTransaction（事务）DISCARDEXECMULTIUNWATCHWATCH\nScript（脚本）EVALEVALSHASCRIPT EXISTSSCRIPT FLUSHSCRIPT KILLSCRIPT LOAD\nConnection（连接）AUTHECHOPINGQUITSELECT\nServer（服务器）BGREWRITEAOFBGSAVECLIENT GETNAMECLIENT KILLCLIENT LISTCLIENT SETNAMECONFIG GETCONFIG RESETSTATCONFIG REWRITECONFIG SETDBSIZEDEBUG OBJECTDEBUG SEGFAULTFLUSHALLFLUSHDBINFOLASTSAVEMONITORPSYNCSAVESHUTDOWNSLAVEOFSLOWLOGSYNCTIME\nredis五大数据类型Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。\nredis五大数据类型实现原理目录1、对象的类型与编码\n2、字符串对象\n3、列表对象\n4、哈希对象\n5、集合对象\n6、有序集合对象\n7、五大数据类型的应用场景\n8、内存回收和内存共享\n9、对象的空转时长\n　　在Redis中，并没有直接使用这些数据结构来实现键值对数据库，而是基于这些数据结构创建了一个对象系统，这些对象系统也就是前面说的五大数据类型，每一种数据类型都至少用到了一种数据结构。通过这五种不同类型的对象，Redis可以在执行命令之前，根据对象的类型判断一个对象是否可以执行给定的命令，而且可以针对不同的场景，为对象设置多种不同的数据结构，从而优化对象在不同场景下的使用效率。\n1、对象的类型与编码　　Redis使用前面说的五大数据类型来表示键和值，每次在Redis数据库中创建一个键值对时，至少会创建两个对象，一个是键对象，一个是值对象，而Redis中的每个对象都是由 redisObject 结构来表示：\n1234567891011121314151617181920212223typedef struct redisObject&#123;     &#x2F;&#x2F;类型     unsigned type:4;     &#x2F;&#x2F;编码     unsigned encoding:4;     &#x2F;&#x2F;指向底层数据结构的指针     void *ptr;     &#x2F;&#x2F;引用计数     int refcount;     &#x2F;&#x2F;记录最后一次被程序访问的时间     unsigned lru:22;&#125;robj\n①、type属性\n对象的type属性记录了对象的类型，这个类型就是前面讲的五大数据类型：可以通过如下命令来判断对象类型：1     | type——– | —–注意：在Redis中，键总是一个字符串对象，而值可以是字符串、列表、集合等对象，所以我们通常说的键为字符串键，表示的是这个键对应的值为字符串对象，我们说一个键为集合键时，表示的是这个键对应的值为集合对象。②、encoding 属性和 *prt 指针　对象的 prt 指针指向对象底层的数据结构，而数据结构由 encoding 属性来决定。　而每种类型的对象都至少使用了两种不同的编码：　可以通过如下命令查看值对象的编码：　1     | OBJECT ENCODING    key——– | —–　比如 string 类型：（可以是 embstr编码的简单字符串或者是 int 整数值实现）　2、字符串对象　　字符串是Redis最基本的数据类型，不仅所有key都是字符串类型，其它几种数据类型构成的元素也是字符串。注意字符串的长度不能超过512M。\n　　①、编码\n　　字符串对象的编码可以是int，raw或者embstr。\n　　1、int 编码：保存的是可以用 long 类型表示的整数值。\n　　2、raw 编码：保存长度大于44字节的字符串（redis3.2版本之前是39字节，之后是44字节）。\n　　3、embstr 编码：保存长度小于44字节的字符串（redis3.2版本之前是39字节，之后是44字节）。　　由上可以看出，int 编码是用来保存整数值，raw编码是用来保存长字符串，而embstr是用来保存短字符串。其实 embstr 编码是专门用来保存短字符串的一种优化编码，raw 和 embstr 的区别：　embstr与raw都使用redisObject和sds保存数据，区别在于，embstr的使用只分配一次内存空间（因此redisObject和sds是连续的），而raw需要分配两次内存空间（分别为redisObject和sds分配空间）。因此与raw相比，embstr的好处在于创建时少分配一次空间，删除时少释放一次空间，以及对象的所有数据连在一起，寻找方便。而embstr的坏处也很明显，如果字符串的长度增加需要重新分配内存时，整个redisObject和sds都需要重新分配空间，因此redis中的embstr实现为只读。\n　　ps：Redis中对于浮点数类型也是作为字符串保存的，在需要的时候再将其转换成浮点数类型。\n　　②、编码的转换\n　　当 int 编码保存的值不再是整数，或大小超过了long的范围时，自动转化为raw。\n　　对于 embstr 编码，由于 Redis 没有对其编写任何的修改程序（embstr 是只读的），在对embstr对象进行修改时，都会先转化为raw再进行修改，因此，只要是修改embstr对象，修改后的对象一定是raw的，无论是否达到了44个字节。　　3、列表对象　　list 列表，它是简单的字符串列表，按照插入顺序排序，你可以添加一个元素到列表的头部（左边）或者尾部（右边），它的底层实际上是个链表结构。\n　　①、编码\n　　列表对象的编码可以是 ziplist(压缩列表) 和 linkedlist(双端链表)。 关于链表和压缩列表的特性可以看我前面的这篇博客。\n　　比如我们执行以下命令，创建一个 key = ‘numbers’，value = ‘1 three 5’ 的三个值的列表。　1     | rpush numbers 1 “three” 5———|———–　ziplist 编码表示如下：　②、编码转换\n　　当同时满足下面两个条件时，使用ziplist（压缩列表）编码：\n　　1、列表保存元素个数小于512个\n　　2、每个元素长度小于64字节\n　　不能满足这两个条件的时候使用 linkedlist 编码。\n　　上面两个条件可以在redis.conf 配置文件中的 list-max-ziplist-value选项和 list-max-ziplist-entries 选项进行配置。　　4、哈希对象　　哈希对象的键是一个字符串类型，值是一个键值对集合。\n　　①、编码\n　　哈希对象的编码可以是 ziplist 或者 hashtable。\n　　当使用ziplist，也就是压缩列表作为底层实现时，新增的键值对是保存到压缩列表的表尾。比如执行以下命令：　　　1     |hset profile name “Tom”———|———–2     |   hset profile age 253|hset profile career “Programmer”　　如果使用ziplist，profile 存储如下：　　当使用 hashtable 编码时，上面命令存储如下：hashtable 编码的哈希表对象底层使用字典数据结构，哈希对象中的每个键值对都使用一个字典键值对。\n　　在前面介绍压缩列表时，我们介绍过压缩列表是Redis为了节省内存而开发的，是由一系列特殊编码的连续内存块组成的顺序型数据结构，相对于字典数据结构，压缩列表用于元素个数少、元素长度小的场景。其优势在于集中存储，节省空间。\n　　②、编码转换\n　　和上面列表对象使用 ziplist 编码一样，当同时满足下面两个条件时，使用ziplist（压缩列表）编码：\n　　1、列表保存元素个数小于512个\n　　2、每个元素长度小于64字节\n　　不能满足这两个条件的时候使用 hashtable 编码。第一个条件可以通过配置文件中的 set-max-intset-entries 进行修改。\n5、集合对象 　　集合对象 set 是 string 类型（整数也会转换成string类型进行存储）的无序集合。注意集合和列表的区别：集合中的元素是无序的，因此不能通过索引来操作元素；集合中的元素不能有重复。\n　　①、编码\n　　集合对象的编码可以是 intset 或者 hashtable。\n　　intset 编码的集合对象使用整数集合作为底层实现，集合对象包含的所有元素都被保存在整数集合中。\n　　hashtable 编码的集合对象使用 字典作为底层实现，字典的每个键都是一个字符串对象，这里的每个字符串对象就是一个集合中的元素，而字典的值则全部设置为 null。这里可以类比Java集合中HashSet 集合的实现，HashSet 集合是由 HashMap 来实现的，集合中的元素就是 HashMap 的key，而 HashMap 的值都设为 null。　　　1     |     SADD numbers 1 3 5———|———–1|SADD Dfruits “apple” “banana” “cherry”—–|—　②、编码转换\n　　当集合同时满足以下两个条件时，使用 intset 编码：\n　　1、集合对象中所有元素都是整数\n　　2、集合对象所有元素数量不超过512\n　　不能满足这两个条件的就使用 hashtable 编码。第二个条件可以通过配置文件的 set-max-intset-entries 进行配置。\n6、有序集合对象　　和上面的集合对象相比，有序集合对象是有序的。与列表使用索引下标作为排序依据不同，有序集合为每个元素设置一个分数（score）作为排序依据。\n　　①、编码\n　　有序集合的编码可以是 ziplist 或者 skiplist。\n　　ziplist 编码的有序集合对象使用压缩列表作为底层实现，每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员，第二个节点保存元素的分值。并且压缩列表内的集合元素按分值从小到大的顺序进行排列，小的放置在靠近表头的位置，大的放置在靠近表尾的位置。　　1|ZADD price 8.5 apple 5.0 banana 6.0 cherry—|—-skiplist 编码的有序集合对象使用 zet 结构作为底层实现，一个 zset 结构同时包含一个字典和一个跳跃表：\n1234567891011typedef struct zset&#123;     &#x2F;&#x2F;跳跃表     zskiplist *zsl;     &#x2F;&#x2F;字典     dict *dice;&#125; zset;\n字典的键保存元素的值，字典的值则保存元素的分值；跳跃表节点的 object 属性保存元素的成员，跳跃表节点的 score 属性保存元素的分值。\n　　这两种数据结构会通过指针来共享相同元素的成员和分值，所以不会产生重复成员和分值，造成内存的浪费。\n　　说明：其实有序集合单独使用字典或跳跃表其中一种数据结构都可以实现，但是这里使用两种数据结构组合起来，原因是假如我们单独使用 字典，虽然能以 O(1) 的时间复杂度查找成员的分值，但是因为字典是以无序的方式来保存集合元素，所以每次进行范围操作的时候都要进行排序；假如我们单独使用跳跃表来实现，虽然能执行范围操作，但是查找操作有 O(1)的复杂度变为了O(logN)。因此Redis使用了两种数据结构来共同实现有序集合。\n　　②、编码转换\n　　当有序集合对象同时满足以下两个条件时，对象使用 ziplist 编码：\n　　1、保存的元素数量小于128；\n　　2、保存的所有元素长度都小于64字节。\n　　不能满足上面两个条件的使用 skiplist 编码。以上两个条件也可以通过Redis配置文件zset-max-ziplist-entries 选项和 zset-max-ziplist-value 进行修改。\n7、五大数据类型的应用场景　　对于string 数据类型，因为string 类型是二进制安全的，可以用来存放图片，视频等内容，另外由于Redis的高性能读写功能，而string类型的value也可以是数字，可以用作计数器（INCR,DECR），比如分布式环境中统计系统的在线人数，秒杀等。\n　　对于 hash 数据类型，value 存放的是键值对，比如可以做单点登录存放用户信息。\n　　对于 list 数据类型，可以实现简单的消息队列，另外可以利用lrange命令，做基于redis的分页功能\n　　对于 set 数据类型，由于底层是字典实现的，查找元素特别快，另外set 数据类型不允许重复，利用这两个特性我们可以进行全局去重，比如在用户注册模块，判断用户名是否注册；另外就是利用交集、并集、差集等操作，可以计算共同喜好，全部的喜好，自己独有的喜好等功能。\n　　对于 zset 数据类型，有序的集合，可以做范围查找，排行榜应用，取 TOP N 操作等。\n8、内存回收和内存共享\n①、内存回收\n　　前面讲 Redis 的每个对象都是由 redisObject 结构表示：\n12345678910111213141516171819202122232425typedef struct redisObject&#123;     &#x2F;&#x2F;类型     unsigned type:4;     &#x2F;&#x2F;编码     unsigned encoding:4;     &#x2F;&#x2F;指向底层数据结构的指针     void *ptr;     &#x2F;&#x2F;引用计数     int refcount;     &#x2F;&#x2F;记录最后一次被程序访问的时间     unsigned lru:22; &#125;robj\n　其中关键的 type属性，encoding 属性和 ptr 指针都介绍过了，那么 refcount 属性是干什么的呢？\n　　因为 C 语言不具备自动回收内存功能，那么该如何回收内存呢？于是 Redis自己构建了一个内存回收机制，通过在 redisObject 结构中的 refcount 属性实现。这个属性会随着对象的使用状态而不断变化：\n　　1、创建一个新对象，属性 refcount 初始化为1\n　　2、对象被一个新程序使用，属性 refcount 加 1\n　　3、对象不再被一个程序使用，属性 refcount 减 1\n　　4、当对象的引用计数值变为 0 时，对象所占用的内存就会被释放。\n　　在 Redis 中通过如下 API 来实现：　　　学过Java的应该知道，引用计数的内存回收机制其实是不被Java采用的，因为不能克服循环引用的例子（比如 A 具有 B 的引用，B 具有 C 的引用，C 具有 A 的引用，除此之外，这三个对象没有任何用处了），这时候 A B C 三个对象会一直驻留在内存中，造成内存泄露。那么 Redis 既然采用引用计数的垃圾回收机制，如何解决这个问题呢？\n　　在前面介绍 redis.conf 配置文件时，在  MEMORY MANAGEMENT 下有个 maxmemory-policy 配置：\n　　maxmemory-policy ：当内存使用达到最大值时，redis使用的清楚策略。有以下几种可以选择：\n　　　　1）volatile-lru   利用LRU算法移除设置过过期时间的key (LRU:最近使用 Least Recently Used ) \n　　　　2）allkeys-lru   利用LRU算法移除任何key \n　　　　3）volatile-random 移除设置过过期时间的随机key \n　　　　4）allkeys-random  移除随机key\n　　　　5）volatile-ttl   移除即将过期的key(minor TTL) \n　　　　6）noeviction  noeviction   不移除任何key，只是返回一个写错误 ，默认选项\n　　通过这种配置，也可以对内存进行回收。\n②、内存共享 \n　　refcount 属性除了能实现内存回收以外，还能用于内存共享。\n　　比如通过如下命令 set k1 100,创建一个键为 k1，值为100的字符串对象，接着通过如下命令 set k2 100 ，创建一个键为 k2，值为100 的字符串对象，那么 Redis 是如何做的呢？\n　　1、将数据库键的值指针指向一个现有值的对象\n　　2、将被共享的值对象引用refcount 加 1　　　注意：Redis的共享对象目前只支持整数值的字符串对象。之所以如此，实际上是对内存和CPU（时间）的平衡：共享对象虽然会降低内存消耗，但是判断两个对象是否相等却需要消耗额外的时间。对于整数值，判断操作复杂度为O(1)；对于普通字符串，判断复杂度为O(n)；而对于哈希、列表、集合和有序集合，判断的复杂度为O(n^2)。\n　　虽然共享对象只能是整数值的字符串对象，但是5种类型都可能使用共享对象（如哈希、列表等的元素可以使用）。\n9、对象的空转时长　　在 redisObject 结构中，前面介绍了 type、encoding、ptr 和 refcount 属性，最后一个 lru 属性，该属性记录了对象最后一次被命令程序访问的时间。\n　　使用 OBJECT IDLETIME 命令可以打印给定键的空转时长，通过将当前时间减去值对象的 lru 时间计算得到。　　lru 属性除了计算空转时长以外，还可以配合前面内存回收配置使用。如果Redis打开了maxmemory选项，且内存回收算法选择的是volatile-lru或allkeys—lru，那么当Redis内存占用超过maxmemory指定的值时，Redis会优先选择空转时间最长的对象进行释放。\nredis五大数据类型常用命令https://www.runoob.com/redis/redis-data-types.html\nredis持久化原理redis支持非常丰富的内存数据结构类型，redis一共支持4种持久化方式：\n1、定时快照方式（snapshot）\n2、基于语句追加文件的方式（aof）\n3、虚拟内存\n4、Diskstore方式\nredis支持小量数据落地功能，后两种方式兵不成熟，下面分别介绍下这几种持久化方式：\n定时快照方式（snapshot）：\n该持久化方式实际是在redis内部一个定时器事件，每隔固定事件去检查当前数据发生的改变次数与时间是否满足配置的持久化触发的条件，如果满足则，通过操作fork调用来创建出一个子进程，这个子进程默认会与父进程贡献相同的地址空间，这就可以通过子进程来遍历整个内存来进行存储操作，而主进程则仍然可以提供服务，当有写入时由操作系统按照内存页（page）为单位来进行copy-on-write保证父子进程之间不会互相影响。该持久化的主要缺点是定时快照只是代表一段时间内的内存映射，所以系统重启会丢失上次快照与重启之间所有的数据。\n基于语句追加方式（aof）：\naof方式实际类型似mysql的基于语句binlog方式，即每条会使redis内存数据发生改变的命令都会追加到一个log文件中，也就是说这个log文件就是redis的持久化数据。\naof的方式主要缺点是追加log文件可能导致体积过大，当系统重启恢复数据时如果是aof的方式则加载数据会非常慢，几十G的数据可能需要几小时才能加载完，当然这个耗时并不是因为磁盘文件读取速度慢，而是由于读取的所有命令都要在内存中执行一遍。另外由于每条命令都要写log，所以使用aof的方式，redis的读写性能也会有所下降。\n什么是redis哨兵模式哨兵模式是一种特殊模式，首先redis提供了哨兵的命令，哨兵是一个独立的进程，作为进程，它会独立运行。其原理是哨兵通过发送命令，等待redis服务器响应，从而监控运行的redis实例。\nSentinel（哨兵）是Redis的高可用性解决方案：由一个或多个Sentinel实例组成的Sentinel系统可以监视任意多个主服务器，以及这些主服务器属下的所有从服务器，并在被监视的主服务器进入下线状态，自动将下线主服务器属下的某个从服务器升级为新的主服务器。\n主要功能：\n1、不时的监控redis是否按照预期良好的运行；\n2、如果发生某个redis节点运行出现状况，能够通知另外一个进程(例如它的客户端)。\n3、能够进行自动切换，当一个mater节点不可用时能够选举出master的多个slave（如果有超过一个slave的话）中的一个来作为新的master，其他的slave节点会将它所追随的master地址改为被提升master的slave的新地址。\n4、哨兵为客户端提供服务发现，客户端练肌肉哨兵，哨兵提供当前master的地址然后提供服务，如果出现切换，也就是master挂了，哨兵会提供客户端一个新地址。\nredis击穿、穿透、雪崩以及解决方案1 击穿: 指的是单个key在缓存中查不到，去数据库查询，这样如果数据量不大或者并发不大的话是没有什么问题的。\n   如果数据库数据量大并且是高并发的情况下那么就可能会造成数据库压力过大而崩溃\n注意: 这里指的是单个key发生高并发!!!\n解决方案:\n1234567891011121314151617181) 通过synchronized+双重检查机制：某个key只让一个线程查询，阻塞其它线程    在同步块中，继续判断检查，保证不存在，才去查DB。 例如:     private static volaite Object lockHelp&#x3D;new Object();  public String getValue(String key)&#123;    String value&#x3D;redis.get(key,String.class);    if(value&#x3D;&#x3D;&quot;null&quot;||value&#x3D;&#x3D;null||StringUtils.isBlank(value)&#123;        synchronized(lockHelp)&#123;               value&#x3D;redis.get(key,String.class);                if(value&#x3D;&#x3D;&quot;null&quot;||value&#x3D;&#x3D;null||StringUtils.isBlank(value)&#123;                    value&#x3D;db.query(key);                     redis.set(key,value,1000);                 &#125;           &#125;          &#125;           return value;  &#125;\n缺点: 会阻塞其它线程\n\n设置value永不过期\n 这种方式可以说是最可靠的，最安全的但是占空间，内存消耗大，并且不能保持数据最新 这个需要根据具体的业务逻辑来做 \n个人觉得如果要保持数据最新不放这么试试，仅供参考：\n起个定时任务或者利用TimerTask 做定时，每个一段时间多这些值进行数据库查询更新一次缓存，当然前提时不会给数据库造成压力过大(这个很重要)\n\n使用互斥锁(mutex key)\n业界比较常用的做法，是使用mutex。简单地来说，就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用缓存工具的某些带成功操作返回值的操作（比如Redis的SETNX或者Memcache的ADD）去set一个mutex key，当操作返回成功时，再进行load db的操作并回设缓存；否则，就重试整个get缓存的方法。\n\n\nSETNX，是「SET if Not eXists」的缩写，也就是只有不存在的时候才设置，可以利用它来实现锁的效果。在redis2.6.1之前版本未实现setnx的过期时间，所以这里给出两种版本代码参考：\n1234567891011121314151617181920public String get(key) &#123;      String value &#x3D; redis.get(key);      if (value &#x3D;&#x3D; null) &#123; &#x2F;&#x2F;代表缓存值过期          &#x2F;&#x2F;设置3min的超时，防止del操作失败的时候，下次缓存过期一直不能load db          if (redis.setnx(key_mutex, 1, 3 * 60) &#x3D;&#x3D; 1) &#123;  &#x2F;&#x2F;代表设置成功               value &#x3D; db.get(key);                      redis.set(key, value, expire_secs);                      redis.del(key_mutex);                     return value;              &#125; else &#123;  &#x2F;&#x2F;这个时候代表同时候的其他线程已经load db并回设到缓存了，这时候重试获取缓存值即可                      sleep(10);                      get(key);  &#x2F;&#x2F;重试              &#125;          &#125; else &#123;              return value;                &#125;&#125;\n缺点:\n\n代码复杂度增大\n\n存在死锁的风险\n\n存在线程池阻塞的风险\n\n\n2 雪崩\n雪崩指的是多个key查询并且出现高并发，缓存中失效或者查不到，然后都去db查询，从而导致db压力突然飙升，从而崩溃。\n出现原因: 1 key同时失效 2 redis本身崩溃了\n方案:\n在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。(跟击穿的第一个方案类似，但是这样是避免不了其它key去查数据库，只能减少查询的次数)可以通过缓存reload机制，预先去更新缓存，再即将发生大并发访问前手动触发加载缓存不同的key，设置不同的过期时间，具体值可以根据业务决定，让缓存失效的时间点尽量均匀做二级缓存，或者双缓存策略。A1为原始缓存，A2为拷贝缓存，A1失效时，可以访问A2，A1缓存失效时间设置为短期，A2设置为长期。(这种方式复杂点)3 击透\n   一般是出现这种情况是因为恶意频繁查询才会对系统造成很大的问题: key缓存并且数据库不存在，所以每次查询都会查询数据库从而导致数据库崩溃。\n解决方案:\n      1) 使用布隆过滤器: 热点数据等场景(具体看使用场景)\n布隆过滤器是什么？\n布隆过滤器可以理解为一个不怎么精确的 set 结构，当你使用它的 contains 方法判断某个对象是否存在时，它可能会误判。但是布隆过滤器也不是特别不精确，只要参数设置的合理，它的精确度可以控制的相对足够精确，只会有小小的误判概率。\n当布隆过滤器说某个值存在时，这个值可能不存在；当它说不存在时，那就肯定不存在。打个比方，当它说不认识你时，肯定就不认识；当它说见过你时，可能根本就没见过面，不过因为你的脸跟它认识的人中某脸比较相似 (某些熟脸的系数组合)，所以误判以前见过你。\n缺点: 1 会存在一定的误判率\n      2  对新增加的数据无法进行布隆过滤\n\n      3 数据的key不会频繁的更改\n   google 的 gauva 中有布隆过滤的实现\n BloomFilter的关键在于hash算法的设定和bit数组的大小确定，通过权衡得到一个错误概率可以接受的结果。\n\n 我们设置的容错率越小那么过滤函数也就多，分配的空间也就越大(存放bits)，那么误判率也就越小。\n2 将击透的key缓存起来，但是时间不能太长，下次进来是直接返回不存在，但是这种情况无法过滤掉动态的key，就是说每次请求进来都是不同额key，这样还是会造成这个问题\nredis事务Redis事务的概念：\n　　Redis 事务的本质是一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。\n　　总结说：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。　　\nRedis事务没有隔离级别的概念：\n　　批量操作在发送 EXEC 命令前被放入队列缓存，并不会被实际执行，也就不存在事务内的查询要看到事务里的更新，事务外查询不能看到。Redis不保证原子性：\n　　Redis中，单条命令是原子性执行的，但事务不保证原子性，且没有回滚。事务中任意命令执行失败，其余的命令仍会被执行。\nRedis事务的三个阶段：\n开始事务命令入队执行事务Redis事务相关命令：\n　　watch key1 key2 … : 监视一或多个key,如果在事务执行之前，被监视的key被其他命令改动，则事务被打断 （ 类似乐观锁 ）\n　　multi : 标记一个事务块的开始（ queued ）\n　　exec : 执行所有事务块的命令 （ 一旦执行exec后，之前加的监控锁都会被取消掉 ）　\n　　discard : 取消事务，放弃事务块中的所有命令\n　　unwatch : 取消watch对所有key的监控\nRedis事务使用案例：\n（1）正常执行（2）放弃事务\n（3）若在事务队列中存在命令性错误（类似于java编译性错误），则执行EXEC命令时，所有命令都不会执行\n（4）若在事务队列中存在语法性错误（类似于java的1/0的运行时异常），则执行EXEC命令时，其他正确命令会被执行，错误命令抛出异常。\n（5）使用watch\n案例一：使用watch检测balance，事务期间balance数据未变动，事务执行成功案例二：使用watch检测balance，在开启事务后（标注1处），在新窗口执行标注2中的操作，更改balance的值，模拟其他客户端在事务执行期间更改watch监控的数据，然后再执行标注1后命令，执行EXEC后，事务未成功执行。一但执行 EXEC 开启事务的执行后，无论事务使用执行成功， WARCH 对变量的监控都将被取消。\n故当事务执行失败后，需重新执行WATCH命令对变量进行监控，并开启新的事务进行操作。\n总结：　　watch指令类似于乐观锁，在事务提交时，如果watch监控的多个KEY中任何KEY的值已经被其他客户端更改，则使用EXEC执行事务时，事务队列将不会被执行，同时返回Nullmulti-bulk应答以通知调用者事务执行失败。\nredis乐观锁乐观锁（又名乐观并发控制，Optimistic Concurrency Control，缩写“OCC”），是一种并发控制的方法。它假设多用户并发的事务在处理时不会彼此互相影响，各事务能够在不产生锁的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。\n与乐观所相对的，就是悲观锁（又名悲观并发控制，Pessimistic Concurrency Control，缩写“PCC”），它可以阻止一个事务以影响其他用户的方式来修改数据。如果一个事务执行的操作都某行数据应用了锁，那只有当这个事务把锁释放，其他事务才能够执行与该锁冲突的操作。\n通俗的说，就是悲观锁就是“先取锁在访问”，因为悲观锁会“悲观”地认为访问会产生冲突，因此这种保守的策略虽然在数据处理的安全行上提供了保障，但是在效率方面会让数据库产生极大的开销，而且还有可能出现死锁的情况。\n在Redis中WATCH命令的实现是基于乐观锁，即，假设访问不会产生冲突，但是在提交数据之前会先检查该事务该事物读取数据后，其他事务是否修改数据，如果其他事务修改了数据，像MySQL提供了回滚操作，而Redis不支持回滚，因为antirez认为这与Redis简单高效的设计主旨不相符，并且Redis事务执行时错误在开发环境时是可以避免的。\n乐观锁控制的事务一般包括三个阶段：\n读取：当执行完MULTI命令后，客户端进入事务模式，客户端接下来输入的命令会读入到事务队列中，入队过程中出错会设置CLIENT_DIRTY_EXEC标识。校验：如果数据库有键被修改，那么会检测被修改的键是否是被WATCH命令监视的命令，如果是则会设置对应的标识（CLIENT_DIRTY_CAS），并且在命令执行前会检测这两个标识，如果检测到该标识，则会取消事务的执行。写入：如果没有设置以上两种标识，那么会执行事务的命令，而Redis是单进程模型，因此可以避免执行事务命令时其他请求可能修改数据库键的可能。Redis的乐观锁不是通常实现乐观锁的一般方法：检测版本号，而是在执行完一个写命令后，会进行检查，检查是否是被WATCH监视的键。 \n悲观锁 悲观锁（Pessimistic Lock），顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，当其他线程想要访问数据时，都需要阻塞挂起。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁、表锁，读锁，写锁等，都是在操作之前先上锁。\n分布式锁分布式锁，是一种思想，它的实现方式有很多。比如，我们将沙滩当做分布式锁的组件，那么它看起来应该是这样的：\n.加锁在沙滩上踩一脚，留下自己的脚印，就对应了加锁操作。其他进程或者线程，看到沙滩上已经有脚印，证明锁已被别人持有，则等待。\n解锁把脚印从沙滩上抹去，就是解锁的过程。\n锁超时为了避免死锁，我们可以设置一阵风，在单位时间后刮起，将脚印自动抹去。\n分布式锁的实现有很多，比如基于数据库、memcached、Redis、系统文件、zookeeper等。它们的核心的理念跟上面的过程大致相同。\nHash冲突1、 什么是hash表\n根据设定的哈希函数H(key)和处理冲突的方法将一组关键字映像到一个有限的连续的地址集（区间）上，并以关键字在地址集中的“像”作为记录在表中的存储位置，这种表便称为哈希表，这一映像过程称为哈希造表或者散列，所得的存储位置称哈希地址或散列地址。\n2、 hash冲突\n对应不同的关键字可能获得相同的hash地址，即 key1≠key2，但是f(key1)=f(key2)。这种现象就是冲突，而且这种冲突只能尽可能的减少，不能完全避免。因为哈希函数是从关键字集合和地址集合的映像，通常关键字集合比较大，而地址集合的元素仅为哈希表中的地址值。\n3、 处理冲突的方法。\n通常用的冲突处理方法有以下几种：\n(1)、开放定址法其中H(key)为hash函数，m为哈希表的长度， [公式] 为增量序列，其取值有三种防止：a、线性探测再散列： b、二次探测再散列：c、随机探测再散列，eg：H(key) = key MOD 7，哈希表中已经存在关键字：11，12，15，在插入\n关键字18的时候，通过哈希函数的到的地址为4，产生冲突，使用线性探测得到的下一个地址为5，依然冲突，继续探测得到地址6，为空，冲突处理结束，使用二次探测解决冲突第一个地址为5，冲突，再次探测，地址为3，为空，冲突处理结束。\n线性探测：二次探测：随机探测：从上述可以看出使用线性探测的时候，当i，i+1,位置上都已填有记录的时候，下一个哈希地址为，i，i+1i+2的记录都将填入i+2的地址，者会造成第一个哈希地址不同的记录争夺同一个后继哈希地址的现象称为“二次聚集”，即在处理相同哈希值冲突的时候造成了不同哈希值的冲突，但是在另一个方面可以看出，只要未满使用线性探测总能找到空位置。\n（2）、在哈希法[公式] 都是不同的哈希函数，对产生地址冲突的关键字再次进行哈希计算，获取另一个哈希地址，知道不在产生冲突，这种方法不易产生“二次聚集”，但是增加的计算的时间。\n（3）、链地址法\n每个哈希地址对应的一个线性表，将地址相同的记录按序写入链表，这种处理方法如果收到哈希共计，出现大量的哈希冲突，会导致查询的时间复杂度增长，甚至退化为O(n)，为了提高查询效率我们可以使用跳表或者红黑树等结构替换线性表。\neg: H(key) = key MOD 7，已存在的记录为，0，4，7，9，11，12（4）、建立公共溢出区顾名思义，在创建哈希表时，同时创建另一个表，将所有发生哈希冲突的记录都存储到溢出表。\n","categories":[],"tags":["python"]},{"title":"celery使用+定义时间任务","url":"https://github.com/xuMr6/xumr6.github.io.git/2017/10/30/celery使用+定义时间任务/","content":"首先说明我用的django框架,那么为什么要使用celery框架,django3.0以下版本都是同步处理请求,假设我现在有一万个任务,我的django框架是承受不了的,所以就用到了clery异步.\n\n\n首先在settings配置12345678# 异步任务代理CELERY_BROKER_URL = &#x27;redis://127.0.0.1:6379/&#x27;# 任务结果CELERY_RESULT_BACKEND = &#x27;redis://127.0.0.1:6379/&#x27;# 保存格式CELERY_RESULT_SERIALIZER = &#x27;json&#x27;\n在项目下创建一个celery.py     在里面写\n123456789101112131415from __future__ import absolute_import, unicode_literalsimport osfrom celery import Celery# 设置环境变量os.environ.setdefault(&#x27;DJANGO_SETTINGS_MODULE&#x27;, &#x27;liuyue_good.settings&#x27;)# 注册Celery的appapp = Celery(&#x27;liuyue_good&#x27;)# 绑定配置文件app.config_from_object(&#x27;django.conf:settings&#x27;, namespace=&#x27;CELERY&#x27;)# 自动发现各个app下的tasks.py文件app.autodiscover_tasks()\n在__init中写\n123456789101112131415from __future__ import absolute_import, unicode_literalsimport osfrom celery import Celery# 设置环境变量os.environ.setdefault(&#x27;DJANGO_SETTINGS_MODULE&#x27;, &#x27;liuyue_good.settings&#x27;)# 注册Celery的appapp = Celery(&#x27;liuyue_good&#x27;)# 绑定配置文件app.config_from_object(&#x27;django.conf:settings&#x27;, namespace=&#x27;CELERY&#x27;)# 自动发现各个app下的tasks.py文件app.autodiscover_tasks()\n在__init中写\n12345678910from __future__ import absolute_import, unicode_literalsfrom .celery import app as celery_app# 导包import pymysql# 初始化pymysql.install_as_MySQLdb()__all__ = [&#x27;celery_app&#x27;]\n在views中定义函数调用celery任务123456789from factory_test import SimpleFactoryfrom myapp import tasksfrom django.http import JsonResponse# 调用celery任务def celery_test(request):    tasks.aync_test.delay()    SimpleFactory.ThirdLogin(&quot;gitee&quot;)    return JsonResponse(&#123;&#x27;task_id&#x27;: &#x27;Hello python&#x27;&#125;)\n最后启动celery12# celery协程启动命令celery worker -A liuyue_good -l info -P eventlet\n问题来了  怎么用celery设置一个定时任务呢????别急这就来了在settings里配置\n1234567CELERY_BEAT_SCHEDULE = &#123;    # 定义定时任务    &#x27;celery_work&#x27;: &#123;        &#x27;task&#x27;: &#x27;myapp.tasks.aync_test&#x27;,        &#x27;schedule&#x27;: timedelta(seconds=30)    &#125;&#125;\n\n启动定时任务也要有的12# 还要启动一个定时任务的服务celery -A liuyue_good beat -l info\n这样一个完整的celery就完成了   是不是感觉有点麻烦  别急嘛  还有简单一点的在根目录下创建一个文件夹\n里面配置celery1234567from celery import Celeryapp = Celery(&#x27;tasks&#x27;, broker=&#x27;redis://localhost&#x27;, backend=&#x27;redis://localhost&#x27;)@app.task(name=&#x27;myapp.tasks.mail&#x27;)def mail(mailaddr):    print(mailaddr)\n怎么样是不是超简单别急还没完  需要启动celery1celery -A tasks worker --pool=solo -l info\n到这里就结束了","categories":[],"tags":["python"]},{"title":"wrap_socket() got an unexpected keyword argument ‘_context‘","url":"https://github.com/xuMr6/xumr6.github.io.git/2017/10/21/wrap_socket() got an unexpected keyword argument ‘_context‘/","content":"python3.8使用celery报错wrap_socket() got an unexpected keyword argument ‘_context’原启动方法为:起执行任务的服务\n1elery worker -A celery_task -l info -P eventlet\n起提交任务的服务\n1celery beat -A celery_task -l info\n\n改变服务器启动方法不要用eventlet，加个参数\n1celery worker -A celery_task --loglevel=info --pool=solo\n\n注意:celery_task是文件名,注意修改\n解决一切疑难杂症,值得一试\n","categories":[],"tags":["python"]},{"title":"orm最全宝典","url":"https://github.com/xuMr6/xumr6.github.io.git/2017/10/16/orm最全宝典/","content":"一、基本查询\n1.查询编号为1的图书【两种方式】12BookInfo.objects.filter(id=1)  # 得到的是查询集BookInfo.objects.get(id=1)  # 有且只有一个id=1的数据,才能正确\n2.查询所有图书的数量1BookInfo.objects.filter().count()\n3.查询书名包含雪山的图书【开头，结尾】12BookInfo.objects.filter(btitle__startswith=&#x27;雪山&#x27;)BookInfo.objects.filter(btitle__endswith=&#x27;雪山&#x27;)\n4.查询书籍名不为空的书籍12345book = BookInfo.objects.get(id=6)book.btitle = Nonebook.save()book = BookInfo.objects.filter(img__isnull=False)print(book)\n5.查询书籍编号为1，3，5的图书123books = BookInfo.objects.filter(id__in=[1, 3, 5])books = BookInfo.objects.filter(id__range=[1, 3])print(books)\n6.查询编号大于3的图书12books = BookInfo.objects.filter(id__gt=3)print(books)\n7.查询编号大于等于3的图书12books = BookInfo.objects.filter(id__gte=3)print(books)\n8.查询编号不等于3的图书12books = BookInfo.objects.exclude(id=3)print(books)\n9.查询1980年发表的图书12books = BookInfo.objects.filter(bpub_date__year=&#x27;1980&#x27;)print(books)\n10.查询1980年1月1号之后发表的图书12books = BookInfo.objects.filter(bpub_date__gt=&#x27;1990-01-01&#x27;)print(books)\n\n\n\n\n二、关联过滤查询 根据关联的模型的属性过滤\n\n19.查询书籍编号为1，中的所有英雄123456789heros = HeroInfo.objects.filter(hbook_id=1)print(heros)book = BookInfo.objects.get(id=1)heros = HeroInfo.objects.filter(hbook=book)print(heros)- 20.查询书籍名称为雪山飞狐的，所有的英雄heros = HeroInfo.objects.filter(hbook__btitle=&#x27;雪山飞狐&#x27;)print(heros)\n21.查询英雄编号为1，所属的图书12book = HeroInfo.objects.get(id=1).hbookprint(book.btitle)\n22.查询英雄中有乔峰的书籍12books = BookInfo.objects.filter(heroinfo__hname=&#x27;乔峰&#x27;)print(books)\n23.查询英雄名字中包含‘郭’的书籍12books = BookInfo.objects.filter(heroinfo__hname__contains=&#x27;不&#x27;).distinct()print(books)\n24.查询书籍阅读量大于30的书籍中的所有英雄12heros = HeroInfo.objects.filter(hbook__bread__gt=30)print(heros)\n25.取1号图书中的所有英雄12book = BookInfo.objects.get(id=1)print(book.heros.all())\n三、FQ查询\n11.查询阅读量大于等于评论量的图书123456books = BookInfo.objects.all()book_list = []for book in books:    if book.bread&gt;=book.bcomment:        book_list.append(book)BookInfo.objects.filter(bread__gte=F(&#x27;bcomment&#x27;))\n12.查询阅读量大于等于2倍评论量的图书1BookInfo.objects.filter(bread__gte=F(&#x27;bcomment&#x27;)*2)\n13.查询阅读量大于20，并且编号小于3的图书12BookInfo.objects.filter(bread__gt=20, id__lt=3)BookInfo.objects.filter(bread__gt=20).filter(id__lt=3)\n14.查询阅读量大于20，或编号小于3的图书,  &amp;  |  ~12books = BookInfo.objects.filter(Q(bread__gt=20) | Q(id__lt=3))print(books)\n15.查询编号不等于3的图书，使用Q对象实现12books = BookInfo.objects.filter(~Q(id=3))print(books)\n\n\n\n四、聚合查询\n16.查询所有阅读量的总和\n12bread_sum = BookInfo.objects.all().aggregate(Sum(&#x27;bread&#x27;))print(bread_sum.get(&#x27;bread__sum&#x27;))\n17.查询最大的评论量\n123456789# 平均数bread_avg = BookInfo.objects.all().aggregate(Avg(&#x27;bread&#x27;))print(bread_avg)# 最小数bread_min = BookInfo.objects.all().aggregate(Min(&#x27;bread&#x27;))print(bread_min)# 最大数bread_max = BookInfo.objects.all().aggregate(Max(&#x27;bread&#x27;))print(bread_max)\n\n18.查询所有图书，按照阅读量排序，升序【降序】\n12print(BookInfo.objects.all().order_by(&#x27;bread&#x27;)[:3])print(BookInfo.objects.all().order_by(&#x27;-bread&#x27;))\n\n复杂的查询语句books = BookInfo.objects.filter(Q(id__gt=3) | Q(bread__gte=20)).order_by(‘bcomment’)[:3]print(books.query)\n\n\n","categories":[],"tags":["python"]}]