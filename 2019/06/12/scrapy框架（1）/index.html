<!DOCTYPE html>
<html>
<meta  lang="en" >
<head>
  <meta charset="UTF-8">
  <meta name="viewport"
    content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color" content="#fff" id="theme-color">
  <link rel="icon" href="/img/1.jpg">
  <title>Mr xu</title>
  
  
  <meta property="og:title" content="scrapy框架（1）">
  
  
  <meta property="og:url" content="https://github.com/xuMr6/xumr6.github.io.git/2019/06/12/scrapy%E6%A1%86%E6%9E%B6%EF%BC%881%EF%BC%89/index.html">
  
  
  <meta property="og:img" content="/img/1.jpg">
  
  
  
  <meta property="og:type" content="article">
  <meta property="og:article:published_time" content="2019-06-12">
  <meta property="og:article:modified_time" content="2020-09-04">
  <meta property="og:article:author" content="Mr xu">
  
  
  <meta property="og:article:tag" content="python">
  
  
  
  
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
    var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
      }
    };
    setDarkmode();
  </script>
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
  </script>
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_f7g5jnuftcf.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  <link rel="preload" href="//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js" as="script">
  
  
  <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
  
  
  
  
<link rel="stylesheet" href="/css/main.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_f7g5jnuftcf.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">

  
  
  
<meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="XuMr" type="application/atom+xml">
</head>


<body>
  <div class="wrapper">
    
    <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
      <img class="navbar-logo-img" src="/img/1.jpg">
      
      <span class="navbar-logo-dsc">Mr xu</span>
    </span>
  </div>
  <div class="navbar-menu">
    
    <a href="/" class="navbar-menu-item">
    
    首页
    
    </a>
    
    <a href="/archives" class="navbar-menu-item">
    
    时间
    
    </a>
    
    <a href="/tags" class="navbar-menu-item">
    
    标签
    
    </a>
    
    <a href="/categories" class="navbar-menu-item">
    
    分类
    
    </a>
    
    <a class="navbar-menu-item darknavbar" id="dark"><i class="iconfont icon-weather"></i></a>
    <a class="navbar-menu-item searchnavbar" id="search"><i class="iconfont icon-search" style="font-size: 1.2rem; font-weight: 400;"></i></a>
  </div>
</nav>
    
    <div id="local-search" style="display: none;">
      <input class="navbar-menu-item" id="search-input" placeholder="请输入搜索内容...">
      <div id="search-content"></div>
    </div>
    
    <div class="section-wrap">
      <div class="container">
        <div class="columns">
          <main class="main-column">
<article class="card card-content">
  <header>
    <h1 class="post-title">
      scrapy框架（1）
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2019-06-12T04:32:11.000Z" style="display: flex; align-items: center;">
      <i class="iconfont icon-calendar" style="margin-right: 2px;"></i>
      <span>2019-06-12</span>
    </time>
    
    <span class="dot"></span>
    
    <a href="/categories/爬虫/" class="post-meta-link">爬虫</a>
    
    
    
    <span class="dot"></span>
    <span>2k words</span>
    
  </div>
  
  <div class="post-meta post-show-meta" style="margin-top: -10px;">
    <div style="display: flex; align-items: center;">
      <i class="iconfont icon-biaoqian" style="margin-right: 2px; font-size: 1.15rem;"></i>
      
      
        <a href="/tags/python/" class="post-meta-link">python</a>
      
    </div>
  </div>
  
  </header>
  <div id="section" class="post-content">
    <h2 id="1-scrapy安装与环境依赖"><a href="#1-scrapy安装与环境依赖" class="headerlink" title="1.scrapy安装与环境依赖"></a>1.scrapy安装与环境依赖</h2><p>在安装scrapy前需要安装好相应的依赖库, 再安装scrapy, 具体安装步骤如下:</p>
<h4 id="1-安装lxml库"><a href="#1-安装lxml库" class="headerlink" title="(1).安装lxml库"></a>(1).安装lxml库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install lxml</span><br></pre></td></tr></table></figure>
<h4 id="2-安装wheel"><a href="#2-安装wheel" class="headerlink" title="(2).安装wheel"></a>(2).安装wheel</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install wheel</span><br></pre></td></tr></table></figure>
<h4 id="3-安装twisted"><a href="#3-安装twisted" class="headerlink" title="(3).安装twisted"></a>(3).安装twisted</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install twisted文件路径</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(twisted需下载后本地安装,下载地址:http:&#x2F;&#x2F;www.lfd.uci.edu&#x2F;~gohlke&#x2F;pythonlibs&#x2F;#twisted)</span><br><span class="line">    (版本选择如下图,版本后面有解释,请根据自己实际选择)</span><br></pre></td></tr></table></figure>
<p><img src="/hugoblog/twisted.png" alt="twisted"></p>
<h4 id="4-安装pywin32"><a href="#4-安装pywin32" class="headerlink" title="(4).安装pywin32"></a>(4).安装pywin32</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pywin32</span><br></pre></td></tr></table></figure>
<h4 id="5-安装scrapy"><a href="#5-安装scrapy" class="headerlink" title="(5).安装scrapy"></a>(5).安装scrapy</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br></pre></td></tr></table></figure>
<h4 id="6-成功验证"><a href="#6-成功验证" class="headerlink" title="(6).成功验证"></a>(6).成功验证</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在cmd命令行输入scrapy,显示Scrapy1.6.0-no active project,证明安装成功 </span><br></pre></td></tr></table></figure>
<hr>
<h2 id="2-创建项目"><a href="#2-创建项目" class="headerlink" title="2.创建项目"></a>2.创建项目</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.手动创建一个目录test</span><br><span class="line">2.在test文件夹下创建爬虫项目为spiderpro: scrapy startproject spiderpro</span><br><span class="line">3.进入项目文件夹: cd spiderpro</span><br><span class="line">4.创建爬虫文件: scrapy genspider 爬虫名 域名</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="3-项目目录介绍"><a href="#3-项目目录介绍" class="headerlink" title="3.项目目录介绍"></a>3.项目目录介绍</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">spiderpro</span><br><span class="line">　　spiderpro # 项目目录</span><br><span class="line">　　　　__init__</span><br><span class="line">　　　　spiders:爬虫文件目录</span><br><span class="line">　　　　　　__init__</span><br><span class="line">　　　　　　tests.py:爬虫文件</span><br><span class="line">　　　　items.py:定义爬取数据持久化的数据结构</span><br><span class="line">　　　　middlewares.py:定义中间件</span><br><span class="line">　　　　pipelines.py:管道,持久化存储相关</span><br><span class="line">　　　　settings.py:配置文件</span><br><span class="line">　　venv:虚拟环境目录</span><br><span class="line">　 scrapy.cfg: scrapy项目配置文件</span><br></pre></td></tr></table></figure>
<h3 id="说明"><a href="#说明" class="headerlink" title="说明:"></a>说明:</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(1).spiders:其内包含一个个Spider的实现, 每个Spider是一个单独的文件</span><br><span class="line">　　(2).items.py:它定义了Item数据结构, 爬取到的数据存储为哪些字段</span><br><span class="line">　　(3).pipelines.py:它定义Item Pipeline的实现</span><br><span class="line">　　(4).settings.py:项目的全局配置</span><br><span class="line">　　(5).middlewares.py:定义中间件, 包括爬虫中间件和下载中间件</span><br><span class="line">　　(6).scrapy.cfg:它是scrapy项目的配置文件, 其内定义了项目的配置路径, 部署相关的信息等</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="4-scrapy框架介绍-5大核心组件与数据流向"><a href="#4-scrapy框架介绍-5大核心组件与数据流向" class="headerlink" title="4.scrapy框架介绍: 5大核心组件与数据流向"></a>4.scrapy框架介绍: 5大核心组件与数据流向</h2><p><img src="/hugoblog/scrapy.png" alt="scrapy"></p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构:"></a>架构:</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Scrapy Engine: 这是引擎，负责Spiders、ItemPipeline,Downloader、Scheduler中间的通讯，信号、数据传递等等!</span><br><span class="line"></span><br><span class="line">　　Scheduler(调度器): 它负责接受引擎发送过来的requests请求，并按照一定的方式进行整理排列，入队、并等待Scrapy Engine(引擎)来请求时，交给引擎。</span><br><span class="line"></span><br><span class="line">　　Downloader（下载器)：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spiders来处理，</span><br><span class="line"></span><br><span class="line">　　Spiders：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)，</span><br><span class="line"></span><br><span class="line">　　Item Pipeline：它负责处理Spiders中获取到的Item，并进行处理，比如去重，持久化存储（存数据库，写入文件，总之就是保存数据用的）</span><br><span class="line"></span><br><span class="line">　　Downloader Middlewares(下载中间件)：你可以当作是一个可以自定义扩展下载功能的组件</span><br><span class="line"></span><br><span class="line">　　Spider Middlewares(Spider中间件)：你可以理解为是一个可以自定扩展和操作引擎和Spiders中间‘通信‘的功能组件（比如进入Spiders的Responses;和从Spiders出去的Requests）</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="工作流"><a href="#工作流" class="headerlink" title="工作流:"></a>工作流:</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1.spider将请求发送给引擎, 引擎将request发送给调度器进行请求调度</span><br><span class="line"></span><br><span class="line">　　2.调度器把接下来要请求的request发送给引擎, 引擎传递给下载器, 中间会途径下载中间件</span><br><span class="line"></span><br><span class="line">　　3.下载携带request访问服务器, 并将爬取内容response返回给引擎, 引擎将response返回给spider</span><br><span class="line"></span><br><span class="line">　　4.spider将response传递给自己的parse进行数据解析处理及构建item一系列的工作, 最后将item返回给引擎, 引擎传递个pipeline</span><br><span class="line"></span><br><span class="line">　　5.pipe获取到item后进行数据持久化</span><br><span class="line"></span><br><span class="line">　　6.以上过程不断循环直至爬虫程序终止</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="5-使用scrapy框架爬取糗百"><a href="#5-使用scrapy框架爬取糗百" class="headerlink" title="5.使用scrapy框架爬取糗百"></a>5.使用scrapy框架爬取糗百</h2><h4 id="需求-爬取糗事百科热门板块-每一条的标题-好笑-评论条数及作者信息-解析爬取的信息数据-定制item数据存储结构-最终将数据存储于MongoDB数据库中"><a href="#需求-爬取糗事百科热门板块-每一条的标题-好笑-评论条数及作者信息-解析爬取的信息数据-定制item数据存储结构-最终将数据存储于MongoDB数据库中" class="headerlink" title="需求: 爬取糗事百科热门板块,每一条的标题,好笑,评论条数及作者信息,解析爬取的信息数据,定制item数据存储结构,最终将数据存储于MongoDB数据库中."></a>需求: 爬取糗事百科热门板块,每一条的标题,好笑,评论条数及作者信息,解析爬取的信息数据,定制item数据存储结构,最终将数据存储于MongoDB数据库中.</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 创建项目:</span><br><span class="line">scrapy startproject qsbk # 创建项目</span><br><span class="line">cd qsbk # 切换到项目目录</span><br><span class="line">scrapy genspider qsbk_hot www.qiushibaike.com # 创建爬虫文件, qsbk_hot为爬虫名, www...com为爬取范围</span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># item文件定义数据存储的字段:</span><br><span class="line">import scrapy</span><br><span class="line">class QsbkItem(scrapy.Item):</span><br><span class="line">    title &#x3D; scrapy.Field()  # 标题</span><br><span class="line">    lau &#x3D; scrapy.Field()  # 好笑数</span><br><span class="line">    comment &#x3D; scrapy.Field()  # 评论数</span><br><span class="line">    auth &#x3D; scrapy.Field()  # 作者</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># spider文件中定义解析数据的方法</span><br><span class="line">class QsbkHotSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D;&#39;qsbk_hot&#39;</span><br><span class="line">    # allowed_domains &#x3D; [&#39;www.qiushibaike.com&#39;] # 无用, 可注释掉</span><br><span class="line">    start_urls &#x3D;[&#39;http:&#x2F;&#x2F;www.qiushibaike.com&#x2F;&#39;]</span><br><span class="line"></span><br><span class="line">    # 思路:一条热点数据在前端中对应一个li标签, 将一页中的所有li标签取出, 再进一步操作</span><br><span class="line">    def parse(self, response):</span><br><span class="line"></span><br><span class="line">        li_list &#x3D; response.selector.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;recommend-article&quot;]&#x2F;ul&#x2F;li&#39;)</span><br><span class="line"></span><br><span class="line">        # 循环li标签组成的列表, 先实例化item, 再取需要的字段, 并该item对象的相应属性赋值</span><br><span class="line">        for li in li_list:</span><br><span class="line"></span><br><span class="line">            # 实例化item对象</span><br><span class="line">            item &#x3D;QsbkItem()</span><br><span class="line"></span><br><span class="line">            # 解析获取title(标题), lau(好笑数), comment(评论数), auth(作者)等信息</span><br><span class="line">            title &#x3D; li.xpath(&#39;.&#x2F;div[@class&#x3D;&quot;recmd-right&quot;]&#x2F;a&#x2F;text()&#39;).extract_first()</span><br><span class="line">            lau &#x3D; li.xpath(&#39;.&#x2F;div[@class&#x3D;&quot;recmd-right&quot;]&#x2F;div[@class&#x3D;&quot;recmd-detail clearfix&quot;]&#x2F;div&#x2F;span[1]&#x2F;text()&#39;).extract_first()</span><br><span class="line">            comment &#x3D; li.xpath(&#39;.&#x2F;div[@class&#x3D;&quot;recmd-right&quot;]&#x2F;div[@class&#x3D;&quot;recmd-detail clearfix&quot;]&#x2F;div&#x2F;span[4]&#x2F;text()&#39;).extract_first()</span><br><span class="line">            auth &#x3D; li.xpath(&#39;.&#x2F;div[@class&#x3D;&quot;recmd-right&quot;]&#x2F;div[@class&#x3D;&quot;recmd-detail clearfix&quot;]&#x2F;a&#x2F;span&#x2F;text()&#39;).extract_first()</span><br><span class="line"></span><br><span class="line">            # 因为部分热点数据还没有评论和好笑数, 所以需对数据进行处理</span><br><span class="line">            if not lau:</span><br><span class="line">                lau &#x3D;None</span><br><span class="line">            if not comment:</span><br><span class="line">                comment &#x3D;None</span><br><span class="line"></span><br><span class="line">            # 将字段的值存储在item的属性中</span><br><span class="line">            item[&quot;title&quot;]&#x3D; title</span><br><span class="line">            item[&quot;lau&quot;]&#x3D; lau</span><br><span class="line">            item[&quot;comment&quot;]&#x3D; comment</span><br><span class="line">            item[&quot;auth&quot;]&#x3D; auth</span><br><span class="line"></span><br><span class="line">            # 返回item, 框架会自动将item传送至pipeline中的指定类</span><br><span class="line">            yield item</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 在pipeline中定义管道类进行数据的存储</span><br><span class="line">import pymongo</span><br><span class="line"></span><br><span class="line">classQsbkPipeline(object):</span><br><span class="line">　　# 连接MongoDB数据库</span><br><span class="line">	conn &#x3D; pymongo.MongoClient(&quot;localhost&quot;, 27017)</span><br><span class="line">	db &#x3D; conn.qiubai  #(数据库名)</span><br><span class="line">	table &#x3D; db.qb_hot  #(表名)</span><br><span class="line"></span><br><span class="line">　　def process_item(self, item, spider):</span><br><span class="line">　　　　# 向数据库中出入数据</span><br><span class="line">　　　　self.table.insert(dict(item))</span><br><span class="line"></span><br><span class="line">　　　　# 此处return item是为了下一个管道类能够接收到item进行存储</span><br><span class="line">　　　　return item</span><br><span class="line"></span><br><span class="line">　　def close_spider(self):</span><br><span class="line">　　　　# 关闭数据库连接</span><br><span class="line">　　　　self.conn.close()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 此示例中配置文件中的配置的项, 注意是不是全部的配置, 是针对该项目增加或修改的配置项</span><br><span class="line"></span><br><span class="line"># 忽略robots协议</span><br><span class="line">ROBOTSTXT_OBEY &#x3D;False</span><br><span class="line"></span><br><span class="line"># UA伪装</span><br><span class="line">USER_AGENT &#x3D; &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;76.0.3809.87 Safari&#x2F;537.36&#39;</span><br><span class="line"></span><br><span class="line"># 管道类的注册配置</span><br><span class="line">ITEM_PIPELINES &#x3D;&#123;</span><br><span class="line">&#39;qsbk.pipelines.QsbkPipeline&#39;:300,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="6-scrapy爬取校花网人名与图片下载链接"><a href="#6-scrapy爬取校花网人名与图片下载链接" class="headerlink" title="6.scrapy爬取校花网人名与图片下载链接"></a>6.scrapy爬取校花网人名与图片下载链接</h2><h4 id="需求-爬取校花网大学校花的默认的第一页的所有图片src和人名-并通过管道存入mongodb数据库"><a href="#需求-爬取校花网大学校花的默认的第一页的所有图片src和人名-并通过管道存入mongodb数据库" class="headerlink" title="需求: 爬取校花网大学校花的默认的第一页的所有图片src和人名, 并通过管道存入mongodb数据库"></a>需求: 爬取校花网大学校花的默认的第一页的所有图片src和人名, 并通过管道存入mongodb数据库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 创建项目:</span><br><span class="line">scrapy startproject xiaohuaspider # 创建项目</span><br><span class="line">cd xiaohuaspider # 切换到项目目录</span><br><span class="line">scrapy genspider hua www.baidu.com # 创建爬虫文件, hua为爬虫名, www.baidu.com为爬取范围</span><br></pre></td></tr></table></figure>

<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 创建item类, 用于存储解析出的数据</span><br><span class="line">import scrapy</span><br><span class="line">class XiaohuaspiderItem(scrapy.Item):</span><br><span class="line">    name &#x3D; scrapy.Field()</span><br><span class="line">    src &#x3D; scrapy.Field()</span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># spider中定义爬取的行为与解析数据的操作</span><br><span class="line">import scrapy</span><br><span class="line">from ..items import XiaohuaspiderItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class HuaSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &#39;hua&#39;</span><br><span class="line">    # allowed_domains &#x3D; [&#39;www.baidu.com&#39;]</span><br><span class="line">    start_urls &#x3D; [&#39;http:&#x2F;&#x2F;www.xiaohuar.com&#x2F;hua&#x2F;&#39;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        div_list &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;img&quot;]&#39;)</span><br><span class="line">        for div in div_list:</span><br><span class="line">            item &#x3D; XiaohuaspiderItem()</span><br><span class="line">            name &#x3D; div.xpath(&#39;.&#x2F;&#x2F;span&#x2F;text()&#39;).extract_first()</span><br><span class="line">            src &#x3D; div.xpath(&#39;.&#x2F;a&#x2F;img&#x2F;@src&#39;).extract_first()</span><br><span class="line">            item[&quot;name&quot;] &#x3D; name</span><br><span class="line">            item[&quot;src&quot;] &#x3D; src</span><br><span class="line">            yield item</span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># itemPipeline编码, 持久化数据到本地</span><br><span class="line">import pymongo</span><br><span class="line"></span><br><span class="line">class XiaohuaspiderPipeline(object):</span><br><span class="line">    conn &#x3D; pymongo.MongoClient(&#39;localhost&#39;, 27017)</span><br><span class="line">    db &#x3D; conn.xiaohua</span><br><span class="line">    table &#x3D; db.hua</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        self.table.insert(dict(item))</span><br><span class="line">        return item</span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.conn.close()</span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 配置项:</span><br><span class="line"># UA伪装:</span><br><span class="line">USER_AGENT &#x3D; &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;76.0.3809.87 Safari&#x2F;537.36&#39;</span><br><span class="line"></span><br><span class="line"># 忽略robots协议:</span><br><span class="line">ROBOTSTXT_OBEY &#x3D; False</span><br><span class="line"></span><br><span class="line"># 开启管道类</span><br><span class="line">ITEM_PIPELINES &#x3D; &#123;</span><br><span class="line">   &#39;xiaohuaspider.pipelines.XiaohuaspiderPipeline&#39;: 300,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>

  </div>
  <div>
  
  <div class="post-note note-warning copyright" style="margin-top: 42px">
    <p><span style="font-weight: bold;">作者：</span><a target="_blank" rel="nofollow noopener noreferrer" href="https://github.com/xuMr6/xumr6.github.io.git/about">Mr xu</a></p>
    <p><span style="font-weight: bold;">文章链接：</span><a target="_blank" rel="nofollow noopener noreferrer" href="https://github.com/xuMr6/xumr6.github.io.git/2019/06/12/scrapy%E6%A1%86%E6%9E%B6%EF%BC%881%EF%BC%89/">https://github.com/xuMr6/xumr6.github.io.git/2019/06/12/scrapy%E6%A1%86%E6%9E%B6%EF%BC%881%EF%BC%89/</a></p>
    <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，转载请注明出处！</p>
  </div>
  
  </div>
</article>
<div class="nav">
  
  <div class="nav-item-prev">
    <a href="/2019/06/22/Scrapy框架（2）/" class="nav-link">
      <i class="iconfont icon-left nav-prev-icon"></i>
      <div>
        <div class="nav-label">Prev</div>
        
        <div class="nav-title">Scrapy框架（2） </div>
        
      </div>
    </a>
  </div>
  
  
  <div class="nav-item-next">
    <a href="/2019/05/29/Scrapy &amp; Django项目/" class="nav-link">
      <div>
        <div class="nav-label">Next</div>
        
        <div class="nav-title">Scrapy &amp; Django项目 </div>
        
      </div>
      <i class="iconfont icon-right nav-next-icon"></i>
    </a>
  </div>
  
</div>

<div class="card card-content toc-card" id="mobiletoc">
  <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-scrapy%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E4%BE%9D%E8%B5%96"><span class="toc-text">1.scrapy安装与环境依赖</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AE%89%E8%A3%85lxml%E5%BA%93"><span class="toc-text">(1).安装lxml库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%AE%89%E8%A3%85wheel"><span class="toc-text">(2).安装wheel</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%AE%89%E8%A3%85twisted"><span class="toc-text">(3).安装twisted</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%AE%89%E8%A3%85pywin32"><span class="toc-text">(4).安装pywin32</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E5%AE%89%E8%A3%85scrapy"><span class="toc-text">(5).安装scrapy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E6%88%90%E5%8A%9F%E9%AA%8C%E8%AF%81"><span class="toc-text">(6).成功验证</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE"><span class="toc-text">2.创建项目</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E9%A1%B9%E7%9B%AE%E7%9B%AE%E5%BD%95%E4%BB%8B%E7%BB%8D"><span class="toc-text">3.项目目录介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%B4%E6%98%8E"><span class="toc-text">说明:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-scrapy%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D-5%E5%A4%A7%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B5%81%E5%90%91"><span class="toc-text">4.scrapy框架介绍: 5大核心组件与数据流向</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84"><span class="toc-text">架构:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E4%BD%9C%E6%B5%81"><span class="toc-text">工作流:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E4%BD%BF%E7%94%A8scrapy%E6%A1%86%E6%9E%B6%E7%88%AC%E5%8F%96%E7%B3%97%E7%99%BE"><span class="toc-text">5.使用scrapy框架爬取糗百</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9C%80%E6%B1%82-%E7%88%AC%E5%8F%96%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91%E7%83%AD%E9%97%A8%E6%9D%BF%E5%9D%97-%E6%AF%8F%E4%B8%80%E6%9D%A1%E7%9A%84%E6%A0%87%E9%A2%98-%E5%A5%BD%E7%AC%91-%E8%AF%84%E8%AE%BA%E6%9D%A1%E6%95%B0%E5%8F%8A%E4%BD%9C%E8%80%85%E4%BF%A1%E6%81%AF-%E8%A7%A3%E6%9E%90%E7%88%AC%E5%8F%96%E7%9A%84%E4%BF%A1%E6%81%AF%E6%95%B0%E6%8D%AE-%E5%AE%9A%E5%88%B6item%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84-%E6%9C%80%E7%BB%88%E5%B0%86%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%BA%8EMongoDB%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD"><span class="toc-text">需求: 爬取糗事百科热门板块,每一条的标题,好笑,评论条数及作者信息,解析爬取的信息数据,定制item数据存储结构,最终将数据存储于MongoDB数据库中.</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-scrapy%E7%88%AC%E5%8F%96%E6%A0%A1%E8%8A%B1%E7%BD%91%E4%BA%BA%E5%90%8D%E4%B8%8E%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5"><span class="toc-text">6.scrapy爬取校花网人名与图片下载链接</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9C%80%E6%B1%82-%E7%88%AC%E5%8F%96%E6%A0%A1%E8%8A%B1%E7%BD%91%E5%A4%A7%E5%AD%A6%E6%A0%A1%E8%8A%B1%E7%9A%84%E9%BB%98%E8%AE%A4%E7%9A%84%E7%AC%AC%E4%B8%80%E9%A1%B5%E7%9A%84%E6%89%80%E6%9C%89%E5%9B%BE%E7%89%87src%E5%92%8C%E4%BA%BA%E5%90%8D-%E5%B9%B6%E9%80%9A%E8%BF%87%E7%AE%A1%E9%81%93%E5%AD%98%E5%85%A5mongodb%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">需求: 爬取校花网大学校花的默认的第一页的所有图片src和人名, 并通过管道存入mongodb数据库</span></a></li></ol></li></ol></li></ol>
</div></main>
          <aside class="left-column">
            
            <div class="card card-author">
              
<img src="/img/1.jpg" class="author-img">

<p class="author-name">Mr xu</p>
<p class="author-description"></p>
<div class="author-message">
  <a class="author-posts-count" href="/archives">
    <span>113</span>
    <span>Posts</span>
  </a>
  <a class="author-categories-count" href="/categories">
    <span>18</span>
    <span>Categories</span>
  </a>
  <a class="author-tags-count" href="/tags">
    <span>2</span>
    <span>Tags</span>
  </a>
</div>

            </div>
            
            <div class="sticky-tablet">
  
  
  <article class="display-when-two-columns spacer">
    <div class="card card-content toc-card">
      <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-scrapy%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E4%BE%9D%E8%B5%96"><span class="toc-text">1.scrapy安装与环境依赖</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AE%89%E8%A3%85lxml%E5%BA%93"><span class="toc-text">(1).安装lxml库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%AE%89%E8%A3%85wheel"><span class="toc-text">(2).安装wheel</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%AE%89%E8%A3%85twisted"><span class="toc-text">(3).安装twisted</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%AE%89%E8%A3%85pywin32"><span class="toc-text">(4).安装pywin32</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E5%AE%89%E8%A3%85scrapy"><span class="toc-text">(5).安装scrapy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E6%88%90%E5%8A%9F%E9%AA%8C%E8%AF%81"><span class="toc-text">(6).成功验证</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE"><span class="toc-text">2.创建项目</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E9%A1%B9%E7%9B%AE%E7%9B%AE%E5%BD%95%E4%BB%8B%E7%BB%8D"><span class="toc-text">3.项目目录介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%B4%E6%98%8E"><span class="toc-text">说明:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-scrapy%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D-5%E5%A4%A7%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B5%81%E5%90%91"><span class="toc-text">4.scrapy框架介绍: 5大核心组件与数据流向</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84"><span class="toc-text">架构:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E4%BD%9C%E6%B5%81"><span class="toc-text">工作流:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E4%BD%BF%E7%94%A8scrapy%E6%A1%86%E6%9E%B6%E7%88%AC%E5%8F%96%E7%B3%97%E7%99%BE"><span class="toc-text">5.使用scrapy框架爬取糗百</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9C%80%E6%B1%82-%E7%88%AC%E5%8F%96%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91%E7%83%AD%E9%97%A8%E6%9D%BF%E5%9D%97-%E6%AF%8F%E4%B8%80%E6%9D%A1%E7%9A%84%E6%A0%87%E9%A2%98-%E5%A5%BD%E7%AC%91-%E8%AF%84%E8%AE%BA%E6%9D%A1%E6%95%B0%E5%8F%8A%E4%BD%9C%E8%80%85%E4%BF%A1%E6%81%AF-%E8%A7%A3%E6%9E%90%E7%88%AC%E5%8F%96%E7%9A%84%E4%BF%A1%E6%81%AF%E6%95%B0%E6%8D%AE-%E5%AE%9A%E5%88%B6item%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84-%E6%9C%80%E7%BB%88%E5%B0%86%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%BA%8EMongoDB%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD"><span class="toc-text">需求: 爬取糗事百科热门板块,每一条的标题,好笑,评论条数及作者信息,解析爬取的信息数据,定制item数据存储结构,最终将数据存储于MongoDB数据库中.</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-scrapy%E7%88%AC%E5%8F%96%E6%A0%A1%E8%8A%B1%E7%BD%91%E4%BA%BA%E5%90%8D%E4%B8%8E%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5"><span class="toc-text">6.scrapy爬取校花网人名与图片下载链接</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9C%80%E6%B1%82-%E7%88%AC%E5%8F%96%E6%A0%A1%E8%8A%B1%E7%BD%91%E5%A4%A7%E5%AD%A6%E6%A0%A1%E8%8A%B1%E7%9A%84%E9%BB%98%E8%AE%A4%E7%9A%84%E7%AC%AC%E4%B8%80%E9%A1%B5%E7%9A%84%E6%89%80%E6%9C%89%E5%9B%BE%E7%89%87src%E5%92%8C%E4%BA%BA%E5%90%8D-%E5%B9%B6%E9%80%9A%E8%BF%87%E7%AE%A1%E9%81%93%E5%AD%98%E5%85%A5mongodb%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">需求: 爬取校花网大学校花的默认的第一页的所有图片src和人名, 并通过管道存入mongodb数据库</span></a></li></ol></li></ol></li></ol>
    </div>
  </article>
  
  
  <article class="card card-content categories-widget">
    <div class="categories-card">
  <div class="categories-header"><i class="iconfont icon-fenlei" style="padding-right: 2px;"></i>Categories</div>
  <div class="categories-list">
    
      <a href="/categories/celery">
        <div class="categories-list-item">
          celery
          <span class="categories-list-item-badge">3</span>
        </div>
      </a>
    
      <a href="/categories/docker">
        <div class="categories-list-item">
          docker
          <span class="categories-list-item-badge">4</span>
        </div>
      </a>
    
      <a href="/categories/djangobook">
        <div class="categories-list-item">
          djangobook
          <span class="categories-list-item-badge">22</span>
        </div>
      </a>
    
      <a href="/categories/django">
        <div class="categories-list-item">
          django
          <span class="categories-list-item-badge">22</span>
        </div>
      </a>
    
      <a href="/categories/ES检索">
        <div class="categories-list-item">
          ES检索
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
      <a href="/categories/jwt">
        <div class="categories-list-item">
          jwt
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
      <a href="/categories/linux">
        <div class="categories-list-item">
          linux
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/categories/python">
        <div class="categories-list-item">
          python
          <span class="categories-list-item-badge">13</span>
        </div>
      </a>
    
      <a href="/categories/mysql">
        <div class="categories-list-item">
          mysql
          <span class="categories-list-item-badge">6</span>
        </div>
      </a>
    
      <a href="/categories/nginx">
        <div class="categories-list-item">
          nginx
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
      <a href="/categories/支付宝扫码">
        <div class="categories-list-item">
          支付宝扫码
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/categories/redis">
        <div class="categories-list-item">
          redis
          <span class="categories-list-item-badge">3</span>
        </div>
      </a>
    
      <a href="/categories/爬虫">
        <div class="categories-list-item">
          爬虫
          <span class="categories-list-item-badge">9</span>
        </div>
      </a>
    
      <a href="/categories/vue">
        <div class="categories-list-item">
          vue
          <span class="categories-list-item-badge">8</span>
        </div>
      </a>
    
      <a href="/categories/git">
        <div class="categories-list-item">
          git
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/categories/mongodb">
        <div class="categories-list-item">
          mongodb
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/categories/编程">
        <div class="categories-list-item">
          编程
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
      <a href="/categories/算法">
        <div class="categories-list-item">
          算法
          <span class="categories-list-item-badge">3</span>
        </div>
      </a>
    
  </div>
</div>
  </article>
  
  <article class="card card-content tags-widget">
    <div class="tags-card">
  <div class="tags-header"><i class="iconfont icon-biaoqian" style="padding-right: 2px;"></i>hot tags</div>
  <div class="tags-list">
    
    <a href="\tags\python" title="python"><div class="tags-list-item">python</div></a>
    
    <a href="\tags\Linux" title="Linux"><div class="tags-list-item">Linux</div></a>
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
          <aside class="right-column">
            <div class="sticky-widescreen">
  
  
  <article class="card card-content toc-card">
    <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-scrapy%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E4%BE%9D%E8%B5%96"><span class="toc-text">1.scrapy安装与环境依赖</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AE%89%E8%A3%85lxml%E5%BA%93"><span class="toc-text">(1).安装lxml库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%AE%89%E8%A3%85wheel"><span class="toc-text">(2).安装wheel</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%AE%89%E8%A3%85twisted"><span class="toc-text">(3).安装twisted</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%AE%89%E8%A3%85pywin32"><span class="toc-text">(4).安装pywin32</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E5%AE%89%E8%A3%85scrapy"><span class="toc-text">(5).安装scrapy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E6%88%90%E5%8A%9F%E9%AA%8C%E8%AF%81"><span class="toc-text">(6).成功验证</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE"><span class="toc-text">2.创建项目</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E9%A1%B9%E7%9B%AE%E7%9B%AE%E5%BD%95%E4%BB%8B%E7%BB%8D"><span class="toc-text">3.项目目录介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%B4%E6%98%8E"><span class="toc-text">说明:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-scrapy%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D-5%E5%A4%A7%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B5%81%E5%90%91"><span class="toc-text">4.scrapy框架介绍: 5大核心组件与数据流向</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84"><span class="toc-text">架构:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E4%BD%9C%E6%B5%81"><span class="toc-text">工作流:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E4%BD%BF%E7%94%A8scrapy%E6%A1%86%E6%9E%B6%E7%88%AC%E5%8F%96%E7%B3%97%E7%99%BE"><span class="toc-text">5.使用scrapy框架爬取糗百</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9C%80%E6%B1%82-%E7%88%AC%E5%8F%96%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91%E7%83%AD%E9%97%A8%E6%9D%BF%E5%9D%97-%E6%AF%8F%E4%B8%80%E6%9D%A1%E7%9A%84%E6%A0%87%E9%A2%98-%E5%A5%BD%E7%AC%91-%E8%AF%84%E8%AE%BA%E6%9D%A1%E6%95%B0%E5%8F%8A%E4%BD%9C%E8%80%85%E4%BF%A1%E6%81%AF-%E8%A7%A3%E6%9E%90%E7%88%AC%E5%8F%96%E7%9A%84%E4%BF%A1%E6%81%AF%E6%95%B0%E6%8D%AE-%E5%AE%9A%E5%88%B6item%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84-%E6%9C%80%E7%BB%88%E5%B0%86%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%BA%8EMongoDB%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD"><span class="toc-text">需求: 爬取糗事百科热门板块,每一条的标题,好笑,评论条数及作者信息,解析爬取的信息数据,定制item数据存储结构,最终将数据存储于MongoDB数据库中.</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-scrapy%E7%88%AC%E5%8F%96%E6%A0%A1%E8%8A%B1%E7%BD%91%E4%BA%BA%E5%90%8D%E4%B8%8E%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5"><span class="toc-text">6.scrapy爬取校花网人名与图片下载链接</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9C%80%E6%B1%82-%E7%88%AC%E5%8F%96%E6%A0%A1%E8%8A%B1%E7%BD%91%E5%A4%A7%E5%AD%A6%E6%A0%A1%E8%8A%B1%E7%9A%84%E9%BB%98%E8%AE%A4%E7%9A%84%E7%AC%AC%E4%B8%80%E9%A1%B5%E7%9A%84%E6%89%80%E6%9C%89%E5%9B%BE%E7%89%87src%E5%92%8C%E4%BA%BA%E5%90%8D-%E5%B9%B6%E9%80%9A%E8%BF%87%E7%AE%A1%E9%81%93%E5%AD%98%E5%85%A5mongodb%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">需求: 爬取校花网大学校花的默认的第一页的所有图片src和人名, 并通过管道存入mongodb数据库</span></a></li></ol></li></ol></li></ol>
  </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header"><i class="iconfont icon-wenzhang_huaban" style="padding-right: 2px;"></i>Recent Posts</div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-09-06</div>
        <a href="/2020/09/06/Python基础知识点大全/"><div class="recent-posts-item-content">Python基础知识点大全</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-08-29</div>
        <a href="/2020/08/29/Django + uWSGI + Nginx 的生产环境部署，及WSGI &amp; uwsgi &amp; uWSGI 的作用/"><div class="recent-posts-item-content">Django + uWSGI + Nginx 的生产环境部署，及WSGI &amp; uwsgi &amp; uWSGI 的作用</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-08-27</div>
        <a href="/2020/08/27/Docker 的基本常用命令/"><div class="recent-posts-item-content">Docker 的基本常用命令</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-08-25</div>
        <a href="/2020/08/25/Docker 镜像 &amp; 容器和镜像的联系 读写层/"><div class="recent-posts-item-content">Docker 镜像 &amp; 容器和镜像的联系 读写层</div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
        </div>
      </div>
    </div>
  </div>
  
  <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>Copyright ©
          
          2020 -
          
          2020
        </span>
        &nbsp;
        <a href="/" class="footer-link">Mr xu </a>
      </div>
    </div>

    
    <div class="footer-dsc">
      
      Powered by
      <a href="https://hexo.io/" class="footer-link" target="_blank" rel="nofollow noopener noreferrer">&nbsp;Hexo </a>
      
      
      <span>&nbsp;|&nbsp;</span>
      
      
      Theme -
      <a href="https://github.com/theme-kaze" class="footer-link" target="_blank"
        rel="nofollow noopener noreferrer">&nbsp;Kaze</a>
      
    </div>
    
    
    
    
</footer>
  <a role="button" id="scrollbutton" class="basebutton" >
  <i class="iconfont icon-arrowleft button-icon"></i>
</a>
<a role="button" id="menubutton" class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a role="button" id="popbutton" class="basebutton">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a role="button" id="darkbutton" class="basebutton darkwidget">
  <i class="iconfont icon-weather button-icon"></i>
</a>
<a role="button" id="searchbutton" class="basebutton searchwidget">
  <i class="iconfont icon-search button-icon"></i>
</a>

  
  
  
  <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img');
    var i;
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a');
      wrapper.setAttribute('data-fslightbox', 'gallery');
      wrapper.setAttribute('href', img[i].getAttribute('data-src'));
      wrapper.style.cssText = 'width: 100%; display: flex; justify-content: center;';
      img[i].before(wrapper);
      wrapper.append(img[i]);
    }
    refreshFsLightbox();
  }
</script>
<script>loadScript("//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js", addImgLayout)</script>
  
  
  
<script src="/js/main.js"></script>

  
  
  <script>
    var addLazyload = function () {
      var observer = lozad('.lozad', {
        load: function (el) {
          el.srcset = el.getAttribute('data-src');
        },
        loaded: function (el) {
          el.classList.add('loaded');
        }
      });
      observer.observe();
    }
  </script>
  <script>loadScript("/js/lib/lozad.min.js", addLazyload)</script>
  
  
</body>

</html>